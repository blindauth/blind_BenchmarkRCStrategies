{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sim_model_archs import RCArch\n",
    "from sim_model_archs import RegArch\n",
    "from sim_model_archs import SiameseArch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_models import train_model \n",
    "from train_models import prepare_sequences\n",
    "from train_models import save_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETERS = {\n",
    "    'filters': 20,\n",
    "    'kernel_size': 21,\n",
    "    'input_length':200,\n",
    "    'pool_size': 20, \n",
    "    'strides': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RC_WRAPPER = RCArch(**PARAMETERS)\n",
    "REG_WRAPPER = RegArch(**PARAMETERS)\n",
    "SIAMESE_WRAPPER = SiameseArch(**PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!densityMotifSimulation.py --seed 1234 --motifNames ELF1_known2 --max-motifs 3 --min-motifs 1 --mean-motifs 2 --seqLength 200  --rc-prob 0.5 --numSeqs 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!densityMotifSimulation.py --seed 1234 --motifNames GATA_known6 --max-motifs 3 --min-motifs 1 --mean-motifs 2 --seqLength 200 --numSeqs 10000 --rc-prob 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !densityMotifSimulation.py --seed 1234 --motifNames RXRA_known1 --max-motifs 3 --min-motifs 1 --mean-motifs 2 --seqLength 200 --numSeqs 10000 --rc-prob 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_train_mutate, y_test = prepare_sequences(seq_len = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_200, x_val_200, x_test_200, y_train_200, y_train_200_mutate, y_val_200, y_test_200 = prepare_sequences(seq_len = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_1000, x_val_1000, x_test_1000, y_train_1000, y_train_1000_mutate, y_val_1000, y_test_1000 = prepare_sequences(seq_len = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_seeds = [1535, 9999, 4652, 6806, 7626, 2143, 3578, 1780, 6117, 9295]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install Bio\n",
    "from Bio import SeqIO\n",
    "fasta_sequences = SeqIO.parse(open(\"simdata.fasta\"),'fasta')\n",
    "x_curr = []\n",
    "for fasta in fasta_sequences:\n",
    "    name, sequence = fasta.id, str(fasta.seq)\n",
    "    x_curr.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCACAGCAAATGGAACTGTTGACCTTTGACCTCTTTTACGGTACGGTATTGAAACATGTTTGCCAGCTATATTGCTGCAAAGCAAACTAGGAGAGTAGTAGGCAATTAGCTTTGAGGCCCCGGTCTGTTTCATTTTGCGTCTAAAGAACAGACAATGAAGGGGTCAAAGGTCAAGCTAAACAATTTTGTTATTTATGTAT',\n",
       " 'ACCTGATTTAGACGAAGACACGGACATGATAAGGAATTGCGACACGTTAATTCTTAATTGTGTAATACCTTTAAAGTGAACCGATAAAGGCCATGTTTTAGTTGATAGCTCAATGTTCAGACGCCTTGGGGTCAGATAAGTAATACCAAGATGACTAGAACCGTGCCTAGTAGCTGACCGTCATGGTATTCCTAATACAG',\n",
       " 'TGATTACACTGGGAGTATTGGGAGAAATGATGTTCATCATTTGTTTGTATGAGACAATGATTATAAAGTTTCTACGATCCACTTCCGGGTTCGTCCAAACATAGACTGAATTAAAATACATTAGAACATTGCTCAAGCGAAAACTCAATAGCTTGACTGTTTGAACCTAGTACCAGCTTTAATTAACCCGGAAGTGTATC',\n",
       " 'CCTATTTTGGCGTTTATTAACATTTCACCTGCATTTCATAATTTAAGGGTCAGTTCCTAAAGCAACACATCCCTCTGATTACAGCGGGTATGTTGACCTTTGACCTCACGTATACCTCCCCCTTATTCTTTATGATCGCTTACTGCGGAATTCCAGACGCATATAAGGTTCAATCACCATACTCGTACTACTTCAGAAAA',\n",
       " 'TACCTCATAAGGCTTCCTGTCTCAACCTAACGGGAAGTAACAATAACACTGTTACCTAGTACTCTTAGGAGGCAGATACGCTGACCTTTGACCCTAAGACTACCACCCGAACTGTGAGGCACATATAAAAGATCCCTACGTATATCGTAATACCTTTTAGTACACGCCAATTATTAAATAACGGTACGTAGTAACCGTTC',\n",
       " 'ACGTAACGGGTACCTTGTCATTCCAAATACGCGCATGTGCACTAAATTCGTACCATACATTTATGGCTGCCGCTTTACTGTTTTCTGAGTTGTCACCTTCCGCGTTATACTAGAAAACCAGGGATGTTGTGGTCTCGGGTAAGTGCGTTTTCTTATCTGGAATAGGGTTTATGCAAGATTTTAGCTATTCCTCTATTATT',\n",
       " 'TTGCCATGGCGTGTTCGTTCACACGGCTACGCTTGAAAGGTCCGATACCATAACCGATGTCCGTAAGTATGAATAGAAATGATTTCAGCTTCAGAACGGATCTTCTATGTTTAATTAATGATCTAGACTGGATGTGCGTATCGTAGGGTCAGTCTTTGTGAAATCCTCTGAGGTCAAAGTTCATACGTTAGTCTTAAGAA',\n",
       " 'GGTACGTCCCCAATTCTTTTAATACTTCCGGGATCTATCGTTAGAAATCCCTAATTCTATGGAACCCGGAAGTGAACGCTTTGTACTATACATAGATGCAGATAAATGAAGCTATTTCCGGTAGCTTAGGTTGCAAGTTTGTAAAACGAACATTCGCGAGGGAAATTATACGGAACGTAAGTTGGATTTAGTCGCTAAGA',\n",
       " 'TCTCCTACTTAGCATGTGTCAACCTATGTTCCGCTCTTACAATGGTTTTGCTATGTGACCTTTGACCCCAGTATGGTGATTTTATTTCAAAAAAGATCTTCATGGAATTATAGTGAAAAATACTCGCTAGACGATAATGAACAAGGGCCCTGTCGGGTGACCTTTGACCCCTGCATTATCATTACATACTTGAAGTGAGA',\n",
       " 'GTGTACTAAATGCAAAGCACTAACATGTTCTGACGCGAGAAACCGGATGTAAAGTTTAGCTATTAGGTCCTAAATTGCTGCGGTAATGGCATATTTAACTCCAACCACGCCATAATCATAGCTTGGGTTATTCGATCGCTCAGTCCTGGTGAATGTACTTCCGGGTCGAACAAGTGGTTAGATATTTGTTTACGCATCAT',\n",
       " 'GACGTAGACATTATACAAAACCCGGAAGTTACAAGAACGAGACTGCAATCTAGGTTGTTGCATGCATTTCTATACTTCCGGGTTATACGCTCCGTGAATCGTTCCATACACTTTATTGCGATGCGCACATTCACATTTCGATCCACTAGTCAGAGGTATAGAAAACCGGAAGTAATTAGAATTTAGTCACCAAATGGTAG',\n",
       " 'AGGAAAGGTACGTAGAAACTCAGGAACGCCCCAAAACGTGAAAACTGCGACGCCTGGGGTCAAAGGTCATCTTATTCCTTGAACTAACAAGTTGAGACATTATGAAGAATATATCATGTTTGCGGCAGCTACTTGCTCGACCTTTGACCCCGTGACTACGATATGCCAGTATGACCTTTGACCCGTGATCTTAGTTAATT',\n",
       " 'GAGTTCTCAATCTAAATTGCATGAGCTAGTTCTGTCTCGTGGTCCTTGCCATGGAACAGCTTTCGAGCTGTAATCGATATGCAAAACAGCTATCTGAATAATTATCCCCACTCACATATCTCACCTTCTATTGCGACTCCTGTCATTTCTAATCTTATTTATGTTTTGTAAGATACTTCAATCAGAGCCCGAAGTGGATA',\n",
       " 'CGATTGTTCTAACATCAGATAAAGCATGTGTGTAATTCTTAAACTCGAGTACGGCTCATGTGACTAATAGGGATATTTGGCGTGCCCAGCGGAGTACAATGCTGCCTTCAAACCTCGGTCCTTCATGTAGTAGCCTGATAAGAAATGTGTTCTCCTATCAGAACGATGCTTCTTAGCTAACTGTAGAGTGGCCGTGATTT',\n",
       " 'TAATCGGCTATCCGATGGCTGGTTGCATTTGTTATGGGAGTAGTAAACTCCCTCCATACCATATCTGCTACGAAGGTAGAAATAATTCGGATTACACTCTCTCTCAGAACCTTCCAAGTTCCTTCTCTGCTGAGGCCTCTATCAGGTATAGTTTGGTTGCTATTCTGCTTTTGTGTCACTGGCTGTCTTAAATGCTCACC',\n",
       " 'GTCAAATTTAGCGGAAGCTTATAATGATCTTGGCAGATAATCCAGGTTCGACTAGGGAGCATGGGGTATACTCTGTTATTGGTACAGTGTTACAACTCTGGTTTACTTGGATCAGAAGAAACATTTAGACCTACCCTGATATTTCTATTTAACCCGGAAGTGATATAATAAAACCTAAAATCTAACAGTAGGTTAACTCG',\n",
       " 'TTTGCCTATAATGGTGTTATTCAACATATTCAAACCCGGAAGTGCAAAGTATAGTTTTTAACACCATAAACATTATATCTTCAGATGATCTCATCCGTCTAGTTGACAAATATTAAGATAACAGTGCCTCAATTTTAGTTTTATTGGCGGTCGGTACTCTTCGGTGGAGTACTGACATCAAATCAGAGAATACGGGTACA',\n",
       " 'TTCAGTTAAAAATTGATATTGTAATGCCAGGTAAATTGTAAGACGTAGCATTTTAATCATTACGTGTTTCTATGCGTAATAATTAAAATCCTTTGAGTTCTTATAACCTCACTGCAGGCGAAGCTATTTTAACAAATAAGTGCTCAAAATGCGGATGTGTATTCATCTCGAAGAAATATTTAAACAAACAGCGGAGGGTT',\n",
       " 'TTTTAAGACAATTCATGACAATTAGATAGTAAGAAGACGGGTACACGAGCCCGGAAGTAATGTATTACCTCGTCGTATTTCCAACGTTCGTACATAACCCAGTAAACAATACCTTTATGGGGTGTTAAAAGTGTCGTAAATGTCTGGGTTTTATTTGGTCAATAAACCCGGAAGTCTGCTTGAGACTATATGGATCCGCC',\n",
       " 'TTTGATGTCTAAATATAAAATATGGTATAGCTTAAGACTGCTCGTCTATAACCAATAGAACAGATAAGAAGTTTTTACTGGATGTTGGTCTCTGGGTACATTGTGTTCCGTCAGGAAATTCCAGAGAACGAAAGATGCGAACTGGGGCGGTTAAGCTAAGCTGATAGAGAGGGAGTTGAGCCCATGTGCAGATCCTGCAA',\n",
       " 'TTCGGTGACCTTTGACCTTTGGCGTAAGAGATTCTTATAGATTGTAGAAGAGCTGAATTTCCAAGGCGTGGACGATCATACTTCACAATCGATCTGTCCTAGAAATGAGTGACCTTTGACCTTAACGGCTGTTACTTGACCTTTCACCCCTGCTGATCCCGTCCATTACTTGTCGATTAGAAATGATTGTACTCTAATAT',\n",
       " 'CCGATAAAACTGCCTACCCACTCCACTCTTGAGAAGACTTGATACTTAAGCAACTTTTTTCGTGTTTTAATACCCTGTCATTTCCCACCATGTTCGTGCGCCTAGTGTGTGTGTATGGCGCCACGTTACGCAACCGGATTCCCCACCAGGAAGTGATGCGTGAACTTTGATCTCACTTCCGGGTGTCACGTCGAACCGTT',\n",
       " 'TTGAAGTATGTTGTGCTGGGTTTAACAGATAACGGATTTAAGCGTTATGATATGAAGCGGCGGCTCTCGTATTGCTATGAACTAACTGTCTATCAGGACCGAGTCCGGATTTCCTATCGGACAGTGTGCTAGGATTTTTCTCTCGAGCTTGTATTCGCTCCGTATATCGGAACAACTACGCTTCACTTTTCACAGCTTTG',\n",
       " 'GATAACGAACTAAAAATCTTTATAGTGGTTATAAGATTGGAGGCAGAATCGCCGACGTTACAATCCATGACAACCCTAATAGATACTGCTCGGTTTAACACCAGATCGCTAATGTTAAAGACCGACTGCGATCTCTTTGGGCACTTATCATGAGGTATGATAGTCCTTCAAGCCCCTATCAAATCAGCTACTAGAGAAAA',\n",
       " 'GTCTAGTGTGTCAGCTATAGTATTTATATTCAGATAGGCAGCTCGATGACTCACGTATGCATCGAAGTTATTCCGTTCAAGGAACACACTCCGTTGTTCCATGAAATATATCGGAACTCCTTATCAGTAAAACCATCGTCGTCAGTAACTTATTGATTCAGACCTGTAACATCCAGGCACTGGAGGGCTTGCTGAAGTGG',\n",
       " 'CTCAGACTTCCGTGAATAGCTCGTTATTAATCTGAGCAATCTCCTACCCACGGTGGGTAGTCTGTACTAAAGTATATGCTAACGAACCAAAATTCTAGCTTCATTGCGTGCGAGGACTTAAAGGCTGGGCGTTGATGATGAGTACAGTAATTTTTTCCAACAGGGAGCAGATCGATTTCGATCTGTTCCGAGAACGGACG',\n",
       " 'TACCATATGGCATATGTCTGATGAGTAGCACAAGTGTGACACCCAGCACTCTTACCAAGCCTTATCTAACGGCTAGCGATATTCGCTACCCCTATAATACTTTACATGATAAGTCGGTAATAGAATTGACAACAAATTAGTTTCCGGCTCATGAATGCCCCCATCCCCTGACTGGTTGGGTTTCCCAGAGATCAGATTAT',\n",
       " 'TTACTTGAAACCCGGAAGTTGTCCCGGAAGTCGTTAGTTCTAGGACATTCAGGGACTCGAACAATACGGACCCGCAGGTCCGACAACTGGACCCTTCTACTTCCGCGTTTGCCGACTTATCCAGCGGCCTAGTTCGGGATAATTAAATTGAGCCCTCTTTAATATCAATTGAAAATAAAACTATGTTGTTATGACAATTT',\n",
       " 'CACTAAAGTATTTGTAAAATAGATGAAGTACTAGCATCGAGGTTAAAACGTGCCCTATCAGCTGAAAGAGACACTCACATTAGTTTATGGTCATCTACAGCATAAACAAGTTATGTTGAATAAATAAAGCATTTTAACATAGCAGAATCATCATGTCTAATATCATAAATTGGTGGAGTAATATATGGTGATAAGGACTG',\n",
       " 'TTAGTACAATTACCTATCAAACTTTTACTAATTGGCTATCCGTAATCCAATCGCGTATAGAAAGAATTATCGCACTATGTTACTCCTAAGTAACATTATGCAAATATTACGGTCGCATCCCTTTATGTAAAGTTAAGTGCTTATCTTGCATAGAAGATAAACGCGTAACCTTGGTTGTCAAGTCAAGAACTGTAGAGAGG',\n",
       " 'TAACTGTACTCCATACGTCTCATCCAAGGAATTTTCATGTTAACAGTGTGCAAGGGAGCTAGATTAGGTAGAATCTGGATAAAATACCTCTCCCGCCGGCCCCACTCCTATGAATGGAAGGGTACAGATTTCCACGCACTCTCATCCTGATAAGTCTGGAGGTCATAAGCGCGGTGTCAAGGTACTTCCTGGTTGTTTGT',\n",
       " 'CCACCACCCATGGATCTGAGTTAAATACATACCGGCATGATTTAGACCCCGGAATAATAAGTCCAAGTCACTTTATTCATGCTCCACTCCTCACGAGTTATCTGGTTTATATAACAGGATAGATTTATCTCGTTAAGACCGAACAGTGGCATAACTGATAATGCGTAGTGAAAAGTTGATCCACTCCAGCCCAAGGACTG',\n",
       " 'AGAGCGAACCCGCACATTGAATGAAGTGTCCATTAAGAGCTTTGGTCCAAAAGGTGACGCACAATGCGATTCTGAAACAAGGGGGGAATAACCTCAACCGGAAGTATGGACTCTACAATTTCACGGTACTTCCGGTTACTAGTCCAATGTATAGGCGATATGAGACCTATGGATTCCGTCTAAGAAACGGAGCGCTGTCA',\n",
       " 'GTTCTAGTTATGTTATTAATTACTTTTTGCTTATACGGAGGATATTAAACCTATAATGTTATGCCTAGATAAGAGAGCTTTCAATAGTGTATGCCCAGCACGAAAATGATTTTGTAAAGTGTCCAACAACCATGGGGTTTATCCTATGACCTTTGACCTTAGAGGTCTAACAAATTTTACTGCATGCCCCATTTGATTGA',\n",
       " 'TACGGCATGAGGATTTGCAGATAGACCGTCCAGTATGACTATATCATAGGGCTGAGGTTTGTTTTATACCACAGGACGATTATATATCACCAAGCGACAATCAAGTGTATTCCCATTTTATTGCGTTAATCAATTCCAACTAAAAGATATATAAAATGTTAGGTTGTCATCACGTCATTACTATACAATAGAAATATGAA',\n",
       " 'ATGCCTGAATAATTGATAGAGTTTTCCCTAACCTTCCGGGCTATTATTTCCGTGGGCTATCAAATTTTCAAACGGGGCGAATCGATATCCTCTCATAAAGAGACGAGTATATCCAGATAGGAACGATATCACACTAACATACGCATTCATCCCATCTAGGTGGAAACCATTGTATATACGGTATATATCATTTCCTACCT',\n",
       " 'TTGACCCAATATGAAGCTATATACGAAGGTGAGATTAGTGAGTATGACTGTTAATTTAGCCCTGATCCCTGAAAGTTCAGTTATAAATAATAGTATAGTAATAACTTGCTGAAACAATATCCCGGAAGTCAAATGGAGCGGTCCGCTCTTATTGTCGGGACTCAGGATTTCCATATTTGTAACTAACTACCACGATGGGC',\n",
       " 'ACTAATGGACCGCCAGACTTTGCACACCCTATAATACCTAGAACTGGTGATTCCTTAACAATATCGTAACCTCAAGGTCGAAGCTAGTACTGAATTAGATAGGTCTTTTTATTCGCACGTATATAGCGGGCCGCAAGCTTTTAGTAAATCTAGCAAGCTACTTAGGAAACTATGACCTACCTTTATTAGCAGTTTTCGAC',\n",
       " 'AACCGCATCTAACATCACCTGAAAACAACCTTATTCACGACAACGATTTCTCCCGTAGAGTCTCCTAAACTGAGATAAGCGGAAATCATGTACCGCAGCAAATGGCATTTGCCTATGACTTCCGGGTAGCATGTTTCTCATACAACTAACGGAGTTATAAAAAAGTCATACAAACTGCTGAGTAATAGCAACCTTATAGG',\n",
       " 'GCTAACTAGTTCCAACGGCAACTCTCAGCAATCACTCAGAAAGAACTAGTTCTGATGAAGCCATTCTGGACTTCCGGTTTCAAAGTCCAGGAAGTAAGCTCGTTCGGTCTCTATCATTTCCTCAGCATAGGTTCGACAATGTGGCACGACCTGGGACGCGGTCGGATTTAATTAAGTCCGTATCTTTGAGAGCTGAGTTG',\n",
       " 'GAGAGACTCATACGTTTTGATAAGAACAATCTGTTCTGACAATATCTATATGTACAAGTCTAGCTAACTGTAATCTGGACTTGTAAAGAAAGGCGAGTCAAGTAAGAAATCGAACACTAAGAGATAGTAAGAGATAAGAATCAGTTGTCAATTCATGTGTATAGTTATTGGGCAGTCGAATCTGCAATAAGAGCATGCGA',\n",
       " 'AAACAGGAGGGATGGGACTCTATAACTAGTCAGTTATTTGTAGTTCTAGCGGAGATAGATGCTGATGTTTCGTACTTCCAAGCAAGCAACATAAGTAGGCCTTATCTGCATCTGCAAGTAAATCCTGTCGACGATCGGCGACTTGTACACTTAGCATAAAAAACTCACCTTATCATACTCACCCAATCTCAGCTCCCGGT',\n",
       " 'ACACCTTCAATGGACACGATCTCAAGTAACTATGATAAACACTTCCGGGTGTAGGTTTAAGAAGTTATGTGGCTCCCGTTGACGCTTCCCGACTTTATTTGGCGCTTAATTGCCTGGGACGTGTGTGTGCAAGATACCATAGTTACCTTCTGCTACCGCAATTCGAGTCTCTCTCGAGATTCGAAACAAGTTAGCGTTCC',\n",
       " 'ACACCGTTTTCAGCGAGGCTTATAGCTATCTATCATCCCCACTTTATATAGTGACTATCGCTCTGATCTTCCGTAGACTATCCGTATTTCGACTTACGTTGTTTATTCAAGTGTACCGGTTTTTCCTGTGTAGGTGGTAGTGCCGGTTCTAGAGTATGTTCAAGTCATCTCTTCTTCTATCTGACAATAATATATCTGCC',\n",
       " 'ATAAGTCACACCCATTTCATTCACTTCCGGGATATGATCACAATATGATTGCGCGAATAGATGCCCTTCCCTTACTCTATTATGTATAATCCGTTCATAAATTGTGTTTAGCCTAAACCCGGAAGTAGTCTCAAAGTCCGAGAAAAACAAACAAAAGCTCGAGAATAAGATAGATCTAGAGAGCCAGATTACAATATCGG',\n",
       " 'TATTGTAACGGTCTAGCTGTTACGTGAAGTTGTTTCGGGGTGGTAATGATAAGGCGATATTGACGACTTGCCAACAACTAAGTGATTCATGCTGATTTCTTTGGTTCAACATATAACTTCACTTAGGGGGTGGGTGGTATAAGATAAGTGCGCCATGTTCTCGCCCCAGTACAATATCGCCATGGAACTTCCATGTGCAA',\n",
       " 'GCGCGTTTCTGTTCCTTGTTGACGCTGCAGCGGGGAAACCTGGATACCTGGGGAGGCTAACCCGGAAGTGAAGCTTACGACAAAAACCCTATCGATATTAGGCCATTAATAACCGGGTATTACTTCGCGGAAGTCTGATTCTTAAAGGACTATCTCAGGGGATGGCCTATGGAACCTGCAACAAGAAGGTTGGAAAGCCA',\n",
       " 'AAATTTTTGTACCTGACTAGAGAAGCCGGTGACCTTTGACCCCTCCACATAGCCTTCAACGTGGTGATTTTCAGGTAGAACTTTAAACCGGTGACCTTTGACCCCCGAGTGTAAGGGATAGATATATTAACATATCGTCATCAGACTTCGAGGATTTGGCATTAAGTATGGTAGTTGTATTATGTGGGACTTATGGCTCT',\n",
       " 'CACGCGGGGTCAAAGGTCATTAATGACGCTGAGCACAAGGTTTAAATGGTCCTTCGTAAATCGGACGCCGTAGATTTTCTCTTCAGACACAACTAGATATGACGCTGACCTTTGACCCCTTAAATGAGATTCCTGAAGCTAGAAATTATCAGACTTACGGAGAATTCAATGCATTCTTCTGAAAGGTCAAAGGTCATAGG',\n",
       " 'CGGTATTTCTTACATGAATATGCGAATATGTTGAGAAGTTAATGACCTCTAGGATATAAAAGCCGTGAGCGGATCGGTAACTCTTTCTATTGCTAAAGCGTGTGACCCTTGACCCCAATTACATAGAACCCGTGTTTGTAAAGCTATGAAGATGTTGTCATCAGTCCTTTACCGATAAATGACACGGTATGCTGCAAAGC',\n",
       " 'ACGCGACTATGGGATGCAACGACTTGCTGTTTAATAACATAAATTTTCGGCGGTTTTTTTCTCCCAGAAGACAGTTTGCGTAAACGTCACGACTTTTAAATTATAATTGCCCGGAATTAAAATTGAATGCATCACTTTTCTCGTGAATATTTCCGGGATTTTTGGAATCTACCAAGCCGTAGCATGACAGTTTTCTATAG',\n",
       " 'GGACAATTGTATAGACTTTTGTAAGATTACAGCCATAGTAACTAGCTCTGGTTTTGTCGACCGGATGTAGATATAGCTATATCCACGGTGCTACAGACTAATACGCATTTTTATTTCCGGGTTTAGAGTGTTATGTCGTAAAAACAAGAAGATAGGTAAGAGGATTTGTTAACTAGTAGTTAAAGCAACCCGGAAGTGAA',\n",
       " 'ATTATCTTCTACTCAAATTAATTGTCGTTGCATTTTCGACATACGGAAATTTACAATACGTAACCCCTCATGTATCGTACACGGAAGTCTTGTCCTATCAGGCCGTCAAGACTCAAATTGTATGTGGTCTTATTTAATAGATATAATGTGCTACTTCCGGGTAATCCCATTGATGCCGGCACTGTAAATTAGGAACTATA',\n",
       " 'TGAGTGCAAACCTTGATGAAGAACCCGGAAGTCCATTGACATATTTATCTATTTCAACCAAAGTCTGGAGCATTTACATCAGTCTTAAACAGGCCCACTTTGCTGAAACTGAAGACACCTTCCATCCTTAACGGTGTGTTCGTATCGCCGCATTTGACCCATCTAGGTCGCAAAATTAGACTATTAGTGATTGCTTGGTA',\n",
       " 'GCGAGCATCGGCATATCTACGACGACCTTTGACCCCTGCTTACACATATCGCCCGTTAATATTAAATTTAGATTTCAGAATTATGTGCAATGGATTCTGATACATTTTAATATCGTCAATTTTTTCCTCTATACGGACTTGAGTAGCTACTATCTTTAGGAAATACGCATGGAATTAATTGGACGTTGCAACATAGAAGT',\n",
       " 'GTATTCCACTTCCGGGTTACTAAAGCAATTACATGAAATATTGGCCAATAAATGCTGCATCAAAACAAATTGCTTTACGAGGAGATGAACTACAAATCAACAAGAAAATATTTACTTCGATTGCTGCAGGAGAAGATAAGATAACTATTTCCATCGTGTTGTATACCTAACGCTACGACCAAAAATAATAGACGATCCGG',\n",
       " 'GTGCTGGTACATGCCTAGGTTCAACGAACAATTACTACTATACATACACTTCCGGGTCTCACCCAAGGTTATTTTAAAGAGTGTGATGAGAGCCCGTCCTTTAGAAGCCTGCTTTGTTTCATACCAATATCTGAGGAAGCTTTATAACACCGACATCCGGGTTGAAAAAAATTGCTCAGCTGATGGTTGCGTGATGAATA',\n",
       " 'AATGTGACGGTGACGAATAAAGCTGTGGACATGCACCAGGGGTCAAAGGTCATATATCTGCTACTTCAAAATCCATTGTTCAAATTGTGGGTCTCGCCATATGTGTTGATGTTTAGTATTAGTGTGATGACCTTTGACCCCAACTTCAGCAGTAGAAAATATACATACAATTATTCTAGTACGCTTAATAACTACGGCTT',\n",
       " 'CTTCTGACTGGTACACTACACCTACATATTGTAACTCTTGATATAATTGGTTCTTGGAAAATATTACATGTTTCAATCTCAACACATCCGGGTTGACATTACTTCCGGATTAAAATAATCAGTAGCCGTCCGAAACCCGGAAGTAGTTATTATACATTTTGCATCGCATTATATTTCTTAAAAACGTTTATTGGATCTAT',\n",
       " 'CCAATTTAACTCGGACTGACCTGAATGCCCTTTACGGAAGCTATATGCTTACAGAAACTCTGTGTACAGCAATGCGAACAGAGGTGAAAGGTCAACAGCGGAGATCCTATCAGGAATACCGAGGGTCAAAGGTCATGGTAAACCTATCCGTACATAATAGCTATCCTTGACCTTTGACCCCTGTTACGATTTAATTTATG',\n",
       " 'CTTAGAATGCTGGCTCACGACCTGATAGCTCGATGCTTTTTTTACAGCTCAACCGTTAGTGGGCACCTATAGAGTAAGCTACTTCCGGGTTAAATTTCACCTTAGTCGAAGATGTCTTGTTGATACCACGGCATGCCTCTGCGAACAATTAAGAGCGCTCTGCTTACGAGAGGTTCCTTCTAAGATAACTAGTGTGCACC',\n",
       " 'TTCTGTTAGAAACAATGCCTATTTGAACCGCAATAGCTATTCCCGAGGAAAACAATTGAGGGTAACAGGCGTTCAGGACATTCTATCAGAATGGAGCATACGATAAGGCGAGAAGAATTCACTGCTATACCTTCCTATAAGATAATTCATCATCAAAGAGATAGGGCCATTCATTTAAGATTAACTAAAGTAATATATCC',\n",
       " 'GATGCGCCCGGAATTCCTTCCCGGCCCCTTATCAGTACACATCCAACATAAGGTAGTCGTTGGAGACTCTGTCGTGATTAACTTTAAGTCTCTAACCCTGCTTTGGACGTACACTAATGAGCTACGCGTGTCTTTTTATGATTGCTCACATGTTACAAAGTGACGTCCTATCAGCTTGATGCGTCAAAGCACGTCGATTG',\n",
       " 'AGAGATCGGTTGATTTGTATAAAAAACCCTGCTAGTGTAAATCGCGGCTTATCTAGTTAGTTAATCAGAAAGGTAATAAACGCAGCATGTCGAATGTGCCAGTGATGTCCACGTGATAATTTACCGTTTGATCAAAATGATTTATTTTCAAATGCTGTCAATTAAAAGTCCCGTTGACAGATTCAGGGGCGTGCGGCTGT',\n",
       " 'CATCGAGCTAAATATTGATATGAATACGCGCAACGCTCAATAAATAGAAGTCGCTGTGGAAGAGGTCAAAGGTCAAGGTACTTCGGGCTATTCTCCATCATACGTCAGACGGCTAGCTGGTGACCTTTGACCTTCAAGCCCTTTTCTGATGCATAGAGGTCAAAGGTCATCTAAGCACGTAGTTAACCTCGGGTACAATT',\n",
       " 'ACAACACTCTGCTGATTAAGCCAACAGCTCAGCGGAGGTTTAGGGTGGCATCTATCTGTAAGGCTTTACAATGATTGAGATAAGCTACTCGTCGATTTTTCATAAGAATAATAGGATTAGCTCCATTGGGTGTATACGGTGCAGAAAGGAAAACAATACTACTGTCTCAGAGCAAACCAGTCCAGGTTTGTCTCATGTTA',\n",
       " 'CCAGTACGTTGGAGGATTACAAGGATAGTGATAAATACCGGACTAATCTAGACCGCCCCGTGACCTTTGACCCCTTGTACTAACTGTGAATATCTTTCAATCCTAAAACGTTGATATGTATGAAGATACATTAAGAAGACAAGCCATACGAAAAGCTCTTTATATGCGGAATTGACAACAGATAATTAATCGCATGTGAA',\n",
       " 'TCTAATGGAGTATCAATATCCTGTATGGCGGTGTAAAGTACCTAATCGCGGACCGTAGAGCTAGAATGATAGCAGTTATGATATAGAGGAGGGAGGATATACATATGTATCTATCTTGCTATAAAACGCTCCTGATAAAGCGTACAGTCTCGGCTTCTCTCCTGCCTACTCAGTGGAAACGTCTTATATATGCAGACCTT',\n",
       " 'GACAGGTTCGAATATAGACAAAATTTCGGGAGTCTTCGATGGTACTTACCTTGAATAATCTTGCTTTACCCATCACGTGACCTTTCACCCCTCTATCATCACGGAACGGTGCAGCCAGCTCGATTAAGGTCAAAGGTCAACGAATAGCGGTAAGAAAGTCGATTTCAGGTGTAAGTCTTACGATTATCGGATGATTGTCA',\n",
       " 'ATATAAGGTCAAAGGTCATGCTTCCCGTCAGATGACCTTTGACCTCTAGCCCGTGCATTCTTCTCAGGACGCGACATAGTGATGTAGCGTAGCTAAGACGAAGACTTCCCCGTCTCGGGAGTGTGTCCAGGGTGACATTTTGCGTTCGGATGTTTCAAGCTTCCCCAACTGTGGGTCACTGATATGGCCGATCTGTTTCA',\n",
       " 'TTTAACAGTGTCTAACTTAACATAGTAGTTCGAATGCCTAGCTATTCCATCTATTAGCCAGTCTCACCAGACAATAAATATTGGCAATAATCGCCTATAAGGGATTAGTCGCGGTTTGGAGAGCCTTATCTGACATTGTCTGCTCTTCCGCAGGAAAAAATGGTTGATAGAGCCAGACTTATATATTTTTGCAATTCCTC',\n",
       " 'CATTTATTATCGTAGAGAAATTGTCTAGTGCTCTGTAGTTGTAGGAGCAGACCTTTGATGTTATAACCAGCGAGCACTCGAAACATAGTTAACCACCTTCATCAGTTGACCTTTGACCTTGCCCATAAGACATATACAAGAGCTAAATATATTTTGTATATCGTTATTCAGAGTACGAGGGAGAAACAGCTGATTGCACG',\n",
       " 'ACAGTCAATCTTCATGTGAGCCTGAATACCGATGTAATAACCGGCAAGCTGACAATATCTATAGAGGTCAAAGGTCATCTAGACTAATGCTGATCTGGCACAACCTATAACTAACTCCAAAAACTTGTGTCTTACTATTGTTAGCTTGATCCTGACATCTACGGATAGGATACCTTGTATCCAGGTAATTACGCACAGCT',\n",
       " 'GTTATAATGCATCGATCCACTTCCGGGTTTGGCACTGTTCGCTATTTTATCCTAGTTAGGTTGAACACTTTTCCAACAAGTATCACGAGGAAACCCGGAAGTGTTGTTATTAATCTCAGCGTTGATAATGAATGTTGCGTTGTTTGTCGATACGGGTGGTCCTTAATTTGTTGTTACCGGACATAAGTATGTGTGTTCAT',\n",
       " 'TTAGCCCATGCCCATGAACAAACGCCTATTGTAACGTACTCCAGTATTACTTGTTTAACTTTAGGCTTCAAACGCATTGTTACTCTCCGAAAACCGTCATGGGGGATGAAAGAGTTGCGCATATCCCATATCCACTAATCTTAGTCCATTAAACCGGAAGTCGAATCGCCCGGAAGGGATTCTACATTTGTTGGCGTGCT',\n",
       " 'GTGCTGTGGATTTTGTATAGCTCTCCCTGGCTGTTATTCATACCAATATGTACTATGACGCCGACGATTATACATAGCAGAGATTAGCCGAGAGTACCCGGAAGTTTTCTAATCGGACAAGCGGCTACGGAAGCGTTTTTTTTGACGAGTGTGCAACTATGCTCATCGAACCCGGAAGTGCCTTTACTTCCCCCTAGGGG',\n",
       " 'ACTCTTTGGTTCTATAATTAATGTCGTTACAACTGAACCATATCCGGATGCTTACGCCTGCACTCAAGCTGCTATGGACTCCGATCTTCGTATTCGAGGTTGATCCGTGAAATTTTATTTTCCTTCCGCCGCTATGGCAACAAGACAACTTATCGCGACGCCCTTCTGATATAAACAAAAATCCAGGATTATTCGTTTTA',\n",
       " 'TGCGAATATAGTTAATATCCCTGCCGACTTCCGGAACGATCAGGCACATTGGAGTCTGACATATGGCGGCCCATACAGTAATCAGCTCATATTTGGAATCCGCGTTGTAGTGACAGGTATTCACATCTTGCTAACGGGTCTAGTGTCTGAAAACTATAATCCTCTATAGGATTAGTTAGGTAACCCGGAAGTCTAGCCCC',\n",
       " 'AGCCACACTGCATCAATTATTAAAGGCACTAACATCTGATAAGGCGATAATCTAGTGCGATGTTCCGGTCCGTCGCTAGATCGTAATAATCTCCACAACCTGTGGATGTCGACATAATTGAGTAAAATCGAATCGAATGCTAGTTTTCAGTGCAAAATAGGTATGATGCTAGCGACTGTCTTCTTCGTAATTTAAATTGT',\n",
       " 'TGTAACAACATGTCATTGTATTCAACCCGTCGTATATCCGTATACGTATCATAGACCAACTATATGCAAAATTGGCGTCAGATAATAAACATGATCATGTTTCTAACATTGACCTTTGACCTCTCTATCGTGGTCCGAAGGGTCAAAGGTCAAGGTAACCTTGGAAACCAGTTATAACTGAGTTTACTGTACATGCGAAA',\n",
       " 'TAGGTTAAAGTACTTCTGGGGGCAACTCAAGTTGTTTGTACCGTTAACTGATACCTAATCCTGCAGATAATGGAAAAACTAATAGCGCCAAGTTATCCGAGTAAACCGTGTTGCCCATCATAGATTCTCGGATTTATAGCAAACAGACCAGATAGGGGGGACTACTGTAGTAGTGTTCCCAGCTGATAACCGCAACGATT',\n",
       " 'CTGGGTAGCTTTTTCATTTCTAAAGAGATACATGGCCGTGCATGACCTTTGACCTCGGTAACGTTCTCATTGAATTGTGGTAGAATAAACCACAGGTAATAGTATTTTTTATTAAGGGTCTACTTTAAGCGTCTAATCGCAAAGCCCGTGACCTTTGACCCCTCGCAAAGTGCTGTCAATAATTAGAACGCTAATGTATA',\n",
       " 'ATGAGGACTCAGTATTTCAAAAGGGGAGAGTTGTATTTGGGTTAAATACACTCAAATTGTTACAGATAATGGCTTATTCCCAATCTGGCGTTGTAATTGATGTGTTGCTTTCAATAGAGACATCTTTACACGCTGGAACATACTGCGGATACGATATTACACTTGCCTACTGTTTTTGTGCTTACCACGTTCGCTTCATA',\n",
       " 'CATTTTTGCGTTTGAGCCCCAGTCCCAGGATAGCATATTCTGCTTCACTTATGGTCATTCATTTTACAAAGAGTTTTGAATATCTCCAAAGTACTTAACAATCAGCGGACCTCTAACGGCTCTTATCAGCCTCCAACAATGAAAGATTAGAGGGATCCCTTCAACGGAACATAAGCGATAGTACAGGTGATAGTAAGGTT',\n",
       " 'GTGTTGGTAGTTTGCCCGAACGAGTAGCATTATTCTAATTACAACTGTTGTAACGACCTATCCTTCTAATACGCCAATTAATATCGAACGCGTGACCTGGAAAAATATAGTGATGATACTCTAATTCTAACTAGTATTAAACAAATAGACGATGATGACAAGCGTTCCAAGGCGTGTCGATCTAGCCTTCGTGCGACCCT',\n",
       " 'ACTCAAAATATTGCGCCTGAGTTACGCTTTAAATTAGCTTTAACTGTTAAATTGTATGACCTTTGACCCCTTGAAGCACTATAAATTATAATTGGCAGTGCGGACATTGCCCTAATTTGAGAAAACCCAACGGAGCTATATTACCGTAACGGACCTATACTTTATCACGAGCCAATCTATAGAGCTTATATTTCAACCTA',\n",
       " 'TAAAAACAGTTGCGTAAGTACCTCAGTAGCATTAAACTCAACATTTAGTAGAAGATGTCCTATGATGCAAGATGCATCGATAGTTAAGGGTCAAAGGTCAAACTTCTTTCATGTGACCTTTGACCCTATGGGCTCATATTAGCACAAACAATATAGGGGTCAAAGGTCACACTTTTGAAACGTTCGTGGATAATTATAGC',\n",
       " 'CTGAGTCGCGAATATTCAACTACTTCTAAATTCTTACGACGTCTTTGTAACTTCCTGGGTGAATTGACAGTGATGAATTAATGTCGGTAAGCGAGAATTATAATCCCAACACTATTTAGGTTTTGTCTAACAGGAAACCAGCCTATTTGGTCGTAAATAAAGTGAACCCTTAAATGACTTCCGGGATACAATGGGACTAA',\n",
       " 'GCGTGTACTCTCACATGCTTATAGGCGCTCTCGCAAGCGGCGCTGTTGATACGTCTTAACCCATTAAGTTAACTCACAAGCCTTCATAAGCCAACCGACGACCTTTGACCCCTTTTATGTATGACTCAACAACCGAACCAAGGATCAGGCAATAAGTGTTATCCGTTGATTCAGATTTATATAATCATTCATCAGTTCAG',\n",
       " 'ACTCGTGCTTCCGGGTTGTACCCACTGCGACATTTAATGACATCTAGCCCGGAAGTAAGTACAGACGTAAGTCACGTTAACTTTACTCCCCAATTTAAGGTCTTCCAATGTTACCGTCAACTACTAATACCAGGAATAACCCCGCCTTTCTAGTAGCAGTAATACGTGTCGTGCTTCTTAAATTAAGGAGTTAGTATCTA',\n",
       " 'ATATAGCAACTCGTAAACAATTTTACCCTATAATTGTTAAAATGCCTTTCCATGTATTTCCAAAGCATTTTTTTAACCCGGAAGTGTGACCGAAAGACCAATTACGGATTATTGGCACTCAAGTCCATAGGTATAAACATGGTACGCAGGTATCTCTAGACTTCGTTTAAGACATCTAATATCTTTGGACATCGAGTATT',\n",
       " 'TCTGCGCAGCACGGTGACCTTTCACCCTCATGCTGCTTACTCATTAAAGTAGACGACATTTCTCAAATGCAAATAACTTTGAGGTGAAAGGTCACAGGGTTGGCAGCTGTAACATGTATAAGCAGTCAATTTAACCGCCGATTGTACAAGGGACTAGGATTCTATATAACCTGTCCTAACCCTCCAGCTAGCCGTGATGT',\n",
       " 'ACATCCCATTTCTATGATCGTCTCAATAGAGGTCAAAGGTCACGGAGCTTAGACAAGCTACTTTAATTATTGGCTTGTAGACTATTAAATTTATGGCATAGAGGCAGCCTCCTCGTTTACCGAAATCGTAATATTGTAGGGTCATACTGCCATGTATTTTGATGGCTCAGCAAATATTGTCAGGTCGATGTAACATTGCA',\n",
       " 'ATAAAATGCTATGACAACTGTCGAATAAATATAGTTGTTTGAGTGACGAATCGGACTAGTGGATATATTCAGAATTAAACCAGGATCGCAGTATTGAGCATCTAACTTAGACTATTAAAAATACGAATTAGGATCGTTCGTTGGTGAGAGGGCGTGGTGCCGTCGGTAGGTTTCATTGAAGGGTCAAAGGTCATACCCGC',\n",
       " 'GACGTCCTTAGTGAGCTAATAGTAAATCGGTTCTATTTGATATCTCTTCCAACTTCCTAAACGATGGCTCTACACTGCAATCTAAATCGTCCATCATTTTCCGTATTCATTGTTCAGAACCCATTATCTTGAGAACATAGCTGGCTGAACATCTGGTTTCAGTATGCCAGTACCTAGGTCCGGATGCCGTCTCCGGTTTG',\n",
       " 'AATCCCCAAGCAGTCAGGTTCGTTGCCTAATACTGGTTTGACGTCTAATACGGAATACATTGTCGCATTTACCTTACTCCGCGTTCAGTTTATCTGACACAGTCTCAGGTTAGAACTTGAACTCACTGGCTGGAGTATTTAAACCAAGAAACCCGGAAATGGCTTGTTAGGGCTTATAGATAGAATGAGCGTAACGTTTA',\n",
       " 'GAGAAGCACGGTATTCTCTGAGAGATATCAAAGATAGCTCTGAAGTATTTATCAGACAGGCACCAAGCGTTTGAAAATAGCGCAGTGTCGGGTCCTGATTTCCTCTGTTGGCAAACACCAGTCGCCCTATTCATCAGGAAGACTCCGAGATTAACCCCCGGTAAACTATAGTAGATCGTTTAGCTGATAAGAAAATCATC',\n",
       " 'TCCCACTATCCGGAAGTGGATGAGGAGACCTCTAATGTAAGCTCCAGGCCCACCACTAGAGAGTAAAGTATTAATATATCGAGCCTAAACCTCTGGGCTGTAATATCAGCACAAACTTCCGGGTTTTTTGCTTGACTTTGAAGGCTAGGCTCCGAACCATGGTATTATGTAATCATTACCCTGTGTTTCATAGCTGCATA',\n",
       " 'CGGCATTTGTTTAATACAGGGAGGGTCTACGTAAAAGAATCATTTTGTTGAAAACAGCATGAATACGTTAAGAATTCAAGAAAACCGAAAACGAAAAACCTTAATCTTAAATTACATTTTTCTATATAGCCGGAAGTAATCCACTTCCGGGTTTACAACTAGCTCCCTTTACTAAGTAAGAAATACTTCCGCTTTTACTG',\n",
       " 'ACCTTTAATAAAACCAGCAGTAATAAAGTTTAGGGACCCGGAAGTGTATTTTAGAGTACGCTGACAATGGAATGAATACTTTAACGTCACACATCCGATGAGGTTTCCCAACCCGGAAGTGGGGACTTTCAGTGGTTTATTACCTCGTCCCAAACCCAGGAGGAAATTATTCCGTCATTGATCGTCGCTGGAATGCTTAG',\n",
       " 'TGGGTCTACGTTACTAATGCTCTAGACTCTTCAGAGTTAAGCGATGATTGTGTTTCTTGTCAAACAAGTCGATGATATTGGTACAAAGTACCGAGCTACAGCAGATAGTCACCAGTACACTGAAGGTAGGCCTGCTCCGATGCAACCTCCTCAGACATACACTCGTTTGGGTAAGACCATCCTCTTCCATTCAGTCATAA',\n",
       " 'AAGTAAATGGCCGATGTCATAACCATAGAAAGTTGACGGTATGTGCAATTCACGTCCTTGTGTCGGTTGCGATAAACTTTTCATTAAGCAACCCGGAAGAGTGAAAGGCGTTGTACAAGTGATATAGATTTATTGTAAGCTGCTTGGAGCGACTTCCGGGTCCACTTCCGGGTGTAACATGAAGTCCTATTTACTTCTCA',\n",
       " 'CCTCTGAGTCCTCATAAAGATTGGTGTCTCTTCCTAGAACCTTGAATGTTCAATTGTCAGGATCCGGGGTCAAAGGTCACCGGTACATCGCCGCTATGCAACGCATGTATAAAGGAATGGTGACCTTTGACCTCATTGGTCGACTAATGTGCTATTAGGCTCGGTCGAATAGGTAACGTCCCATTAAGATACATTATTTC',\n",
       " 'ACAGAGACATAACAACCACTAAAAGAGGTTATACTATAAGCTAGATCGTAAATTCTATACAGCTTCTTCAGCTAGTGGGTTTGCTCGCTAATTGAGGCAATAAGTAAATGCATAAAACAATATTCATTGCAGGGTTACTCAGGGGTTGACCTTTGACCTTTGACATATTGAGTCAGGTAGATGGGCCTATCAGTATCACG',\n",
       " 'GCGATAGCAATGATTAGTCCATAGGAACACGATTACGTTAGATGCTGTTAGGATCGGGTGTAGGAATCAAGTTCATATAGTATACTTCCGGCATTCTCGAAATACGCGAGGTTGCACTGGGGAATTCCCTAAGGTAAACCATCTAGGGAGCTAAGACTTCCGGGTCGCGGCGAATGTAACCTTACTCTGTGAACCGACTA',\n",
       " 'CTCGAACCCACTCTTATTTGAAAGCCAGAGGCGACCAAATTCATTCATTCTAGATTTATGTATTGCTGTAAGTGCTGTGAAGTGGGGCAGGGTGTTCTAAGAGATCAGGGGTCAAAGTTCATGCTGAACGTGCATCGTGAATTCATTTACATAGACATGCTAACAAAGTCGCTTGTTCCGCTAATTCTGAGCTCCCATTA',\n",
       " 'AATAGGCGGAACGAAGTAAGCAAATGAGTAAGTGTACCCGATACGCAGTAACTGAGATTCTCGCGCGCAATCACCGTATGACTTCAGCTATGTTAGACCAAAAAAATTGTCCACTTCCGGGTATTTTAAAAACGTACCTAACCCGGAAGTAATTGCACTAATTGGTACATGGCAACAGCAATGAATTTTTAATTCAGTCC',\n",
       " 'ACTCTCGCTTCCCGGAAGTGTATAGGTACAAGTAGGACATTTCTTTATTGTGTCTCCAATACACTGTTACCTTAAACGGCTGGACTCTTAAAGTCCATTGAATACCGTCATGTCATGGACAGTGTTTGTATTCATAGTACCACGATTTGCTAAAGAGTGACGAACAATTCCGTTGATAGTCCAGACTCAAAACTAGGTCG',\n",
       " 'CTGTCATGAACCTATTTAGTTAACCCGGAAGCAGCGCAATTGGATCCAGGAAGTAATTCAGGCCAAAGGAAACGAACATATTAAATGGGATTCACCCATTGAAGCAACATATTGGGTACTCATCTATCGGACGCACGGTCGCTAATACATTAGCTCTATCATAGACTTAACAGGAAACGCTAGCAGGAACTTGGTAATAA',\n",
       " 'TTCATGATAAGGAAATATACGAAACAACAAGGTTTCCTTTAACGGCTCATTTACAACATGCCGCAATAGCTATAATTGGAGCGGCATCTGTATGCAGAGGCATCCCTATCATGATTATCGGCCATCATGGGCTCCGAACGAGATAAGGAGTTACTTGCTATTCACATCAGGGGTATAGAAAAGTACAGGATCGGCCTATC',\n",
       " 'CGCCAGCAAATAGAAGTCACGCCGAGTAGTAAAGCTCGAATCAATAAAAACGAAACTCAGTTAATTTTCGGATAATCCCGGAAGCCTGGATTTAGTACCTGTGAACGTTCCAACTATTTGACACGGAAGTGGAACGCGGAAATGGTTCTATTCTTATTAATGTTACATGTTAGGAGGTTATTCGCGGCATATTGGACATG',\n",
       " 'TCACTGAAAGAGAAATTTCTCGGTTCTACCATTATGATAGGTGTCCTACGAAGTAAACTACAAAAAAGAGTCAAGTTCCACTGGTCAAATCAGTTCATCAGTCCTTAAATTTGTGATATACGGGCTTGATACACAGGTAGTGCCACGAAGGTATGCCAGACTACTACGGAGACATATGATATATCCCTCTCACCCTCTTC',\n",
       " 'TAATCAGCAAAGCTCGTCAAAATAATAATGCCAGTGCCTAGAAAGAGGGTGAAAGGTCATTTGAAAAGGCCACCAAAAGGTCAAAGTTCATCTAAACAAGACAAACGAACTGTAGCTATGCAGACGTCCAAAATAACTTTATTCTGAGGGAAAGGAAATATATCTGCTATGGGGTCGTATGATGACCTTTGACCTGTTCT',\n",
       " 'TCCAATCATCAGCTAGCATTCGTGATGTTCCGAAATCGTGCCAGACCGGAGATGTTACCTCAGATGTTAGAAACTCGCTTGACCTTTGACCTCCAAGATCTGTGCTCCGCAATCAGTAAAGAGAATTTTCAAGCGTATCATAATGTGCATTGCGGGTTACGTTATAACGAAATTGCCTTAGCGCATTACGTTTCTTTAAA',\n",
       " 'TACACTGACGGTTTATAGAATAGAAGTTCTCGTGTAGTGAATCATTAGGAACTTACTAACTACAGTGAGTCTGGACAGATAGGTAGCCGGACGCCCATCACTAAATTATACCAGATAGAGAGCAGGTTCGAATATAGACAAAATTTATATGATAAAGCTTGGTACTTACCTTGAATAATCTTGCTTTACCCATGAGAGCT',\n",
       " 'AATCCTTGATTGTAACTTGACGTAGACGATACCGGGACGCATGACCTTTGACCCTTGCGGTCAATAAATTCCGGGTCAATTGACCTTTGACCCATAGAAAAACTATTACGTCACACTCTTGTGACCTTTGACCTGATTTTTGTTTACAGATAGGACTGTATTTGACACGCTTTTAGTACAGAACACACTTTTTCCTCTTA',\n",
       " 'AACAGTAGGAGTCCAGAGTGGAGCATATTTATCTGAACGAGAAGTTTGGGTGTGATGCAAGATTTGCACCACGTAAATTACAACGAATGATAGCGGCGAACAGAGTGCTGAATGTTATTATGAATACATACCGCTCTCCAAATAAAACTTCGAGTAAAAAGCAAAAATTTTGTCCGGGGTTGTCCCTATCAGACTATAAA',\n",
       " 'AGTTCTGATGGGTAATTAACGAAGGGTCAAAGGTGATGCGTACCCCGGTGATGACTCAACCTTGATTCATCTGAGGTCTGGAATCTTATGGACTACGAACCTCTAACTCTGGTTCTGAATGATTTAGCACTCAATCAGTTCTCACTTGTTTGGGAGGGAGCCAGAGGCTCAGTGTTAAAGTCAATAACCTTAAGAGGCAA',\n",
       " 'ATACCCGTGGGCTCGGATTGATGATAAGAACCCCTTCTTGGATTACTAACAGTCCCAGCGAGTGAAGTCTAAAACAATATCTGTAATAAGGATATCGCCTTGCATCGTCTCAAGGCCCTTATCTCACAGTGGACGTTAATAGAAGTACACTCTATGCACTTAAGTACCTGCTAGAGGTAGCTGAATAACGGAGTCGACTT',\n",
       " 'ATCGGATTACGCCCCTTTGCTGACAAGCTTGACCTTTGACCTATGGAACAGTTACGAAAGCACCACCATATCGTTAAACAAAGTTCAAGAGAATCGTGATTGCGAGTCCATCCAGTACGTAGGCTTTACCTGTGCAGTTACGTAAATATTTAATTGATACTTACACGGATCGCCAATAGTCGAGGTGTAAGTTGCATGTA',\n",
       " 'GTCTTCCTAGAGGAATTGCCAGACCTTAGTGCCACTTCCGGGTACACTGGAACTAACCTGTTGCAACTAAGGCTCAAACTTCCGGGTCGACTTCTTAACATGAAGCTATAGCTGATCTATTCAACTTGAGGCGGCAGAAAACGCAACTAATAGTACGTTTCTGTTGTGAAATACTAAAAGTCCCACTAATGGACCGCCAG',\n",
       " 'TCGAAATTCGATCACGGCGAAAATTACATATAGTACCAATTAAATATCGATAACAAGTTTTAGATCGCAAAGTCCACCTATCGCCTATTTTACTCCTCTTTCGCTACTTCAGATAGGAAGATTTTTCAGAGAAGGATCATCTGTAATCGGATGGTCTTCGCAGAGTGTTTTAACGAATCACGGCATATCTGCCCGGCATT',\n",
       " 'TTTAGGCCCACCATCCAGCGTTTTTCTTTTTTAGTAAACAAAGTAGTATTTATATTTTATGCACCCATCTCCTACTATTTGGGGGAAGATCAACTGTTCAAGGTGTTTCCGACAGGATATGGAACATATCGTCCTTTGGATGTGGAGTCATCAAATAAACAGACCAGGAAGTACTTCAGAGTCCGACACCGAGCCCCTGT',\n",
       " 'GCAGTATCTATTCCAAATATTACACAGCTATGTCCATATTAACGATCCTTCGTCATTATTTAAATGCGGTGCCCTGAACCAATGCCAGTTAAAATAATATCAGACATACCTAGTAAATTCTTGTGGGTAACAAATTTCAAAGCTACAAGATAATAACCACTGTTTATGATCCTACCAGTTTGACCTTTGACCTTAGAAGG',\n",
       " 'AACACAGTCACTATACGACCGTAATAGCGAATTAAAAGAGTAATGAGATTGAGTGATCTAGAATAGATTGAAAGTAAGTTCGTTATCATGACCAGGCACGAGGTGATGCCCTATCTTTTACACGACATTAAAAGGTATATTCGTAATTTAGTTACATTGAGATTATTAAATGTACAGCTAGCACCCTGAAGATCCGAATC',\n",
       " 'TGTACTATAGACGTCTTAAAAACTAGTTTAAGATAGGTCCCTTAACTAGATCTGCGCATCCCTTGTATTATGTATCGAGTTTTCCTTCGAGGAGAACTATAGTTGTAAGCCCATATCTGCCTACAGTTAATTTTGGATGTTATCTGGACCCCGATTGACCAACGCTTCGAAGTCACTTGGGATATTTTCGTATCTAAACT',\n",
       " 'CATGCATTCAGAGTCTTTGTTCATTTTATGACCTTTGACCTCAGGCGTCCTCATCTTATTTAATCGGATAAGGCTCCGCACCCTCTGCATACTTTTCGTATGACCTTTGACCTCTTGCGGACATAAAAGGATACCTGCGCGATTACTACACATCACGCATTATTGGTCTTAGGGCTAGGGGTCAAAGGTCATGGTTTTAA',\n",
       " 'AGACAGAAACAGGTCGTTAAGCTTATCAGATAAGCACTTTTCGTCATATCAGTCAGTTCCCCCAAGGCTCAATCCTAGCAATGTGAAGCACGTTATATGATATGTTTAGGAATTTAGTCTGGTACGGCGGAATACGGATCTTTGGTAATGGCGCTACTCATGCTATTGGATCTTAGCTAGTCGTAAGGGTTATACCCTTT',\n",
       " 'TCAAGCCTTTATCGCCTTCCTTTTTCGTTCGGAGATATCGCTGATCTATATTTAAATCCGACGAAGTCAACCGATAAGTGAAATACCCGACTACATCACGATAACAAACGTGACATAAGCCGGGAAAGCCCTATCTGGTATCCACCGTTCATGAGGGGACCAGATATAGGAGATATCGGTCGCTATTTCTATAAAGGTCA',\n",
       " 'ACGTAACACTTCCGGGTTGGCCAAAAGTTTTAAACAAATTACCGCATTTCCGGGGTACCTAAAGAAGGTCGAGATCTTCAGTTAGGCCCTTTCCCAAAGACGCAGTTTATTACATTTTGAGACTTAAATATTACAAGTTGGTAGATTTCCACCATTGCATAAAACAGACTTAATTACCATCTAAAAGATTATTGTGAGGC',\n",
       " 'GAACATAGAACGTGGAGATGAGTTGAAAGAATAATTATTCCTATCTTAAGCTGAATTACTTGCGGATTTGAAATCAACTTACCCGGCTTCCCTTTACCCAACAAACACTGTGAGTCATTATCCAATACGGTATTTACGAAGACATCAGATAACAACCATGCGCTTATCAGAAGTAGATAAACGAGGTTCGTTTTTGATGG',\n",
       " 'TTTTCCAAACGAGGTTCAACACAAAAACTCTCGGATTTGAGTTACGAATATGGGGGGGGTCAAAGGTCAACTGTTGTACAGTTATATTAATGTGTTGTTAACAATCAGGGTTTTCATTTCACCCGACACAGCTGGCTAGATAAAAAAAGGATAACCGTTCTAGTACTGCTTACTAAAGCATAATTGGAATAGGTGTGCTA',\n",
       " 'ACAGTTTGACCTTTGACCTCTGCGTTCCAACGCGAACTGCGGACACGAGTAACTAGTTGTTAACCGTGACCTTTGACCCGTTCGTATTTCAATTTTCAGAGTTTACTAGAGAGGTCAAAGGTCATGAGCTTGCGAATCGTAATGACGAAGTGAAGATCTGCCTGCCAACCTGCTTAGAATAATAGCTCTACTACGAATTA',\n",
       " 'TTAATCAGTCTATGATCATCCTGATTGTGATTCGTAACAGTTGAACACAAGATATAGTCTGATCGGTATTGAATTTTAAAAGCAGTCTGACACGTCCACATATATCCCACTTCCGGATATCTACGAATTTACCTCCCCCGACTTTAGGCCAACCACTTATAAGATTACGAGAATGTAGGTTGTACTTCCGGGTCACCCCA',\n",
       " 'GAGCCAGCTGCTATACTCCAAGTCGTCATCTTCCCAACGAATTAACGGGGCAATATGATTTGTACTGGATACTCATGATGTGGATAGTCGTACGCGCACTTCCGGGTTCCTTATAATTGGAGAGTAACCAGGACGATACTTCCGGGTCATCGAGTTTCTAGTGCACGGGAGGTTTTACTTCCGGGTAGGAAGTCATCGAT',\n",
       " 'TCACTTCCGGGTTCTCAATCGTGTAAGTGCAATCCAAAAAAATATTCAAAAAGAAGGCACTTCCGGGGCTTTGGTTGCTCTCTGGAAAGATTTTAATATGTCAAAGAACGTATTTAGTTTATATATAGGAAGCACAAGTTAGGTATTATAGACATCTAATAGCTGAGATTAATGATCCTTGTGCCTAGGACACGTCTGTT',\n",
       " 'ACGTAGATGTATTAAGGTTGACCTTTGACCCCCCTCGGCCCCTTCATTTATGCGGTGTAGAGGTCAAAGGTCACAATGCATGGAGCAAGGGGTCAAAGGTTACCAGCTGAGTATGATTACGTATTAGATCCTTACTCCGGATCAATATAGCAGTAATACACCGAATTTAAGCTAGAAATCGATTATGTATGTCATCGTTC',\n",
       " 'TGTCGTAAATACTAAGTTGTTCATGCGCCCCGTTTCCTGATGACGATAAGACCACTTTCACCGCCCTAATCTATCTCCTTATCTAAATCGCAATCCTTACTGTGTTTACTACAGTTCTGTTTGAGCTAAACAGCACTACAGCTTCCGCCAACACTGCAATGTTACAGAGATTTAAGAAGGTTCGTTACGGAACTTCATAC',\n",
       " 'CTAATTGGGCCCGGTTCTTCCGGTAGTCAAGAACTAGATGACCTTTGACCCCTTCAAGAGACCTCGAGACTCGAAGTCTCAGTCTGTAATATCTGTGCACTTTAGGTATAACTTTTGAACATGGGTGAAAGGTCAAGCATCGTAAGGTACTATAACGTCTGCAATCGATAATTAAGTAATGTCTGTCGAATACTGCATTT',\n",
       " 'AGACAGCAGCCTCATATTATTCTCGAAAGTATGTTTTCCATTGTTACACTTCCGGCAACTCATTCATGCATCCCACTTCCGGGTTTACCGCTTAGTCCATGATTCTTTTATATGACGTTCAATACCTTACCCGGGTGTTCTCTATGCCAGCTAATGCAGGCTCAAAATGTCAAGCTATATAAGCAGGACTCATGTCTGAT',\n",
       " 'TCATAGATATCATTTAGACCCCATATCAAGATTTAATCGCACACCTCACTTCCGGGTCATACGCTAAAACCGGAAACTGGTTACAATTCTGTACACTTGTGACGTTTCTTTGTTTGTTTACTTCCGGATTCTCCTGACTTCACATTTTTAAATACCTAGGTCGGATACATTAATCTTTTAAAGTATTATCGAAGAATCAA',\n",
       " 'GTGCTTCTTAAGTCGATCCTACCTCAATCGCAATGAGGATACTAACCTCATGTAGATATTCGGGCTTAAGCTGCTGCTCGATAATCCCAGTTTTGGAGGAGATAACTAATAGACGAACCAATAACCTACCGACCGATTATGGAAACAGTGAAGAAAATCTATATTTAACTGAGTGGATGGACCGGCACAAGTTACAGTAG',\n",
       " 'CAGCGCATAATAATTTAGACTGTTGAAAAGTTTGATGACTCCTCCTATACACTTGAGAGGTCAAAGGTCACGCTTGGTGAAAGGTCAAAGGTCACATTTTCATGTCTTAATCTCAGGCGGGGCTTCACAACGACTTACCCTGTAGCTGTAGTTCTTATCCGTGAGCGCCAAGGTTGACCTTTGACCTCTCTACAACAAGT',\n",
       " 'TCTTGTGGATATTTCTAATCGACTTTACTTTGAACGTAGTAACTAAATTCACATCCGCACGATACTATAGAACAGCCCATACTGTGACTTCCGGGTAAATAGGTTGAGATAAACACGCACACGTGTACGACGATACTTCCGGGCTGTAGGCAATTCGTACAACTACCCGGAAGTGTTGCTTTATCATCTTGCTGTCGTTT',\n",
       " 'GGATTATCGGCAGCTGTGTGACCTTTGACCCGTAGTTGTTTGCTAAAAACTTCAATTGGAACGTTCTTTATTTTACCACTCGCATAGGGGGAATTCCAATCTGAAAGAAAAGACCCTAGAAATGTCTTGTGAATATCGCAACGAGGGTCAAAGGTCATGGAGAAAACATTGATTACAATAGATACGTTAGTATATCATTA',\n",
       " 'TAGTTTTTACGAACATGAATCCAGACTTCCTAATAGCCGATAGATCTCCTTCTCTCCGGCGAAACAGAGTCAACTGATTCACGTTCATGATAAAGATTCTATTTATTTTTGAGTTGAATAATAGGCTGCTCAGGCGGCCTTTATTGCTTGTGGATTACAATAACATTAGTGGCTTTTTCCGTAAAAGGGACAAACATACA',\n",
       " 'GCCAGAGATACCGTAGAAAGAAGGTTACCGATACAACATGTATTCAGTCTACTCCTAACAAGTTATCCTTATGGCGCTCGTAGGATGCAATCTGGATAAACATAGGGTAAGATATATAACTCATTAGTACACATTGTACATCATTATACACAATGGTCCATATACTCCGTAAGGTCAAAGGTCACGGTCACTTCTTAATT',\n",
       " 'TGAAAGAAAGTAACCACTCTGGGGTCAAAGGTCACAGGTCATGACATGTAAAAGAGAATCATATTCCCATAATGGTTATAACCAGGGTGACCTTTGACCTCTGTTCCATCCTAGTCTAGTAATAGTAGGAAACGAACACAATACAGCTTCGGGGCCACCTGCCTTTGTGTGTAAAGTACTATTGTAAATAGAGGGACCAA',\n",
       " 'TATGCAAAATAGTCTCATGTTTATTTCACCCTCCATCTGGTAGTCGGGTAAGTAGAAAAAGATCTCATATTCTGGTGAATGAACATCGAATTCAGCGCGGAAATGGATCAAATATGGAGATATGGGTTGGAACGTGCAGGGCTGATAGAACGTGTGCTGAACTATCAAAAGTCAGTCTAAAACTTGTAGATAGGGAATTT',\n",
       " 'CGAATAGGTGAACTTTGACCCCTGAGGGTCTTAACATCCTGTGACCTTTGACCCCATTATGGCAGCTTTACAAACCTGCAAACTCCAAAATAATTCCAAGCTCTAGTATTTACTAATAAACAAGGAACACTAGAAAAAAGTTAAGGGGTCAAAGGTTATGTTTGGACGTATCTCTTTGTAGGGGCTTGTTCATTGTCGCC',\n",
       " 'ATGGTCCTAAGAAGATAAGGGTTGGCTATTTATACTTCGTACAGAAAACGAGTATTATTGAATATTTCGCCAGCGTGAGCGGTATTACTGGATGATTGTGTTTTAGGTGCTCAATCTGGTAGCACGCAACAGAGTATTGGTAAAATTACGTCTGTGAGGAGAACTTGGCCTGATATTGGTTGTCCAGACACATTTTCGTT',\n",
       " 'AGAGTAGCCTTTGAAGCCCGTAGAAACTACACAGTATCTGATCGTCAAGTTCTTATTACAACATGGCGTTTACCGGGGTCAAAGGTCGAACTTAGAAAACTGATCGTGCTCCCTGAACCGCGTTTCTATTTTTAGCGTTTGCTTGATAAGAAGGGAAGGTCATAATACTTTCCTTCGCGCAACGGTACAAAAGAATCCTG',\n",
       " 'ACGAATAGTAATGAGGGTCAAAGGTCACTAATTCGTCCTAATATAAAGGAAGCTATCGGTTCAACGGACGGTATGGGCAACACTAGTCAATCTTGAGAAGAGGACAAAGAATTTTAATTGAGAATAAAGCGTTGCATACTTCAACATAAAAGAGTTAAACCAGGAACAAACCTTCATATTGATTTACACACGTCGTAACA',\n",
       " 'AATTTGACCTTTGACCCTTGAAGTTTAAAAGAATCTTAGATTCTTAATTTAATATTAAGCAGGTGTGGCACGTTTTGCGCGAGCGCTCCATCGGGATTAACAAGTTGCCTTTTAAAAATGCATCATGAGAAAGAGCCCTTCGTCCGGCATCGAACGGTAAGTAATAATTCTCTAGCCGCGTCATTTTGCTTATTACGAAT',\n",
       " 'GCACAACATCATTCCTGCAGTTCTCAAATCCGACACACATGCTCCATTTGAGCTGAATAGAATGAAATCAAACCCGGAAGTTGACGGCGGGGTTACTTGAGTTAAGATAAACAGAGAGTTTAACCATATTGTTATTTACTACACGCTATTCGCGCCTTGTGTGTGGGTTAATAGAGGAAATCTATCCGGCGTCGTAGGCA',\n",
       " 'TTTTTTATAATGTCATATATTATATTGAGTTTCTTATGCAACCGGAAGTGAGTTCCGGACTGTCAAATTAACTAAACACTTCCGGCTCTGAATTCACTTAGCAGTGTCACACCCAATATAAATGATTACACTTCGAATGCAAGGACTTACATACATTTCCGGGAATTGCTAAGTCTATGAAAATCATGCTATTCAAATTT',\n",
       " 'CACTAGAGTACTTATCCCCAATAATATCTAGGACCCAAGGGACCTATACGTTGATTTACCTAGTTGCAGTACTGAATGATCCCTTCGTTCAGTCTAGAATTGTCCAATATTTGATCTCAGTTAGACTTTACGGTCATAATAATTTCCCTCTTATCTGGGGGTGAGGTAGATGATTTGATCGGCAGATAAGGAGCAACAGT',\n",
       " 'TTGAAGTTGTCATTCGGGCATCTGATCTAGGGTCAAAGGTCGTCGTCTATCACTAATTATTTGGAATTCAGATTTCTCGATAACCTTTGACCCTCCGGTATAAAGTGTTTTGAACCTTGTCTATGACCTTTGACCCCTGGTGGCTGCCTAGGTGAATCTTGTCTTCGTCTTTTATTTTGCCAACACGCCACATGTAGCTT',\n",
       " 'CTTTTGAAAGCTTGCAGTGACATTAACTACCCAATTATTTCTCTGCTCGCTGTGTAACAATTTAACGTATAAAATCAAGTATACCCGGAAGTATAGCTAGGAATTTATGTATAGCTACTGCCTATTAGTCACTCAATCACGCACCTACAATTCCGGAAGTTGTGTTGAGCGCCACTGGCTGGGTATCAGTACCTGTCTAT',\n",
       " 'TACAAGGAAGTCTTCGGTTACGCACTACGAAATCGGAGTAAGATATCAGCGACTTCCGGGGAAAGCCGGTTCGAAACTTGGGCCTCTTATTTGACACCGATACTGGTTAAAAGTGTTGAAAACGGTTTGGACCTGGTTTATTTATATAAAAAAGACATGTACCAGTACTATCGTTACCTCCAGTCAAACGTATGAAGTGT',\n",
       " 'GCACTTATCCATAAATATCTCAAGAAGCGGAACGTTGGTTGCACCTTCTATCTACACTGTGAAGATGGTATTGTTTATACTTTTGTCCACACCCATCCCTATAATTTTCTTATGATAGGGTGCTTAATCAGTCCCGAACCATAAACCGTAGTACGCCGTGCTGGAAACCTGTATTTGGAGGCCGAAGTACTTAATTGGAA',\n",
       " 'TACTATAAGCACTGAGAAGTTGTACTCTGTGGAGCTACTAAAGATAGGGATCTTGTTTACGAGAAATACAATATGTACTAGCAATATAGCTTAGATTAGCGACATCAGTATCAGAACTTCCGTGTTACAATTCCCGGAAGCACCTTACAATAACTGACTATAAAGATATTAACGCGGAAGTGTATCACATCAGACGCAAC',\n",
       " 'TTAGTAATGATCAAGCACGAATCTCAATATTGAGGTGTAATATTGTGGGTTCGGGTTTATCGACCTTTGGTGCCAAGATAGGGGTCAAAGGTCATGTCTTTAGGTTTGACTTAGAGACTATGGTCATTTAATTGCCCGTTTCTCCACTTTAGGTAGGAAATTTTGTTGTAAAGACAGTAATGTTCTAGATACTACTGTGT',\n",
       " 'CTTATCATATGTTCATTCACACTCCTCTTATCTTGAAATTCAATGTTGCGGTTGTAGGTTTAGAGGGTTTTAGTGCTGACCTCCTATCCTCTTCTCACAATGTAATACATATGTTCTCCTTTAATGAGTACACGACTGCCAAAACTCATTTCTGTCAACTGTCATATTGCAATTGACGTCAGATCCTGTGAAGTTAAGGT',\n",
       " 'GTATTTTGCCTTTGTGAATGTCGCTTGAAATAGGAAAAGAGATACTTAACTAAAACCGGAAGTCCAAGTGAAAGCAAGTTACCGGTCCCAAGGACAGCGCAAGTCATACTGTTTTCAGGATACAGCATCTAGTAATAGGCGGCAAATATGGCGAATCTCTTCCGGGTACCCAATGACAGTTGATTTAGAAAATCAAGCTA',\n",
       " 'GCCGATATCAAACCACATCACTCCGCCTGAATTGTTGATTTGCCAGTACGGTTCAGAATATAAAGTTGAACTTTGACCCCATCAGTACCATTATTTTTTTTTCAGGGGTCAAAGTTCATCCTTATGAAATATTTAGATCGTCTGGGAACCGTTTGAGTGATTAGATGTAAAGCCTTAGCATGTCCTTATGACACTATTAA',\n",
       " 'ATGGATTCAATGTGTGCATGCGGCGGACACGAAATTTAAAGTCTAAAACCTTAGCATGATGTCCTCTACCATACTTTACATTGTCCCAAGCCCTTATCAGACTATGAGATAGTTGGAGATCGTCCGCACTAATCCTGGCAGCTTTTTCCTAAGTCCTAAGAGACGAAGGGATATCTGTAGCAGATATCGATCAGCTGTCG',\n",
       " 'ACGCGCGGAGCTTACACAAGCCACGTTTTACGTGAACTCTGATGAATCTCCAAATTTATAGGGGTCAAAGGTCACGGGTGTCAACAAACCTTCTGCATCTCTTCCAGAGAAGCACGATCGAACCTTTTATCGATGCGAGTCTAGAGGTCAAAGGTCACAGTCACGTAGAGGGGGGTCAAAGGTCACCCGTCAGGTAATCC',\n",
       " 'GTTTTATGCTGAATTCTCTACCGTATGGGTTTTTAGTACTTCCGGGATAAAATGTTCCCGATTGATATGCTGAGACTCGAACATTCAATCATACATCAGGCGTATCGGAGTGATCACGTATGAATGCTTAAATAATTGACTATAGCAGAAAATTAGTAGAGATGACAGACCATTAACCCGGAAGTGATGATAGACAAAAG',\n",
       " 'ACAGATAGTAAAAGAATTTAATCCGGAAGTGAATGGAAGTATTATACGCCACTTATCCTTAGGAGGAAATTATAAACTCAGACAAAGTCTGCACATTGGTACATCAAATTACCAGGAAGTGCGCGTCGGTTACTTTTTGGATACTAGACAAACCCGGAAGTCAATTTGCAATTACAAGAAATACGACATTTACAATATTT',\n",
       " 'CTAAAGTTAGTATGCGTCTTTCTATCCTTCCCACCGGCCGCAGGACGACTTCCGCGTACTGAATAGGACGCACCCATTAATGGAGGGTCTACCCATGATTAACGCATTCTATTATGATTGGTAGTTAAGAGGATCTGTTTTTTCAACACCGAGAGTGGGAGTTCCTTTCGTTCGCCATTACTGTCGCTAGTGCACGTGAG',\n",
       " 'ACGTCATGTACTTGACTTACTGGCTTCTGGAGAAATCTTAAGCACCTATCAGGAGTTAAGCTTTACGACTAACTAAACAGATAAAGCAATGAAATAGCAACGTCTGATTTACGACTGCAATTAACATTTTAATTATTTCCCCAAGTAGCCGTGTATTTTAGCGGTTCCCTTCCACTAAACTTTGAACATAATTCAGTAAC',\n",
       " 'CCATTGTTTGATGTGACGAAACCGTGTTTAACTCCAGCTATACAATTTACCACGCTAATCTGTACTAGATAGATCCCTACGACTCGTGCTGTTGTAATGGATTAGGACAATCACTTAGTTTCGCCTGATAAGCTCATGTATCCTTCGAGTCTAGGGGAGAATTGTCACGAAATTGCGCTGTTGCATGATAAGTAAGAGAT',\n",
       " 'ATCTGCAAGGTGACGTTTTGCCTACCCGTGTATTCCTGCTTGTACGTCCCGAAGGTCAAAGGTCATGAACCGGGCTAAAGCCCATTAATGTCATATATGACCTTTGACCTTTCCGGTGGACGCAAATATATGATAGTCTATAATGGTGGTCTCTTTAACGTCCATAATTCAGACGAAGTCTAGATAAAGGTAATCTAAAG',\n",
       " 'GTGTACAAAAGAACCATTAAGAAGCAAATATATAGTATGTCTTATCTGGAAACAGCTCTCAGAAGTCCCCACACTTCTTGAAAACATAACTAACCCACGTTTTAGGCTCCATTATCGCTTATAAAGATCAATACTCTAGTCTGCACCCAGTTTAGACTTATCCCTGTATAGTATATCACTAATTTCATACAATGACGTCA',\n",
       " 'CAAGTCTTACCACGGTAAGATAAGAGGGTTTGAGACCCACATCCGATAAGGCAAGCATGCCATCAGATAACTTTAACATGCTTGTGTTCTTATGCACGGGCTATGGATCTGGAGCTATCATTTTCTTGCTCTTATGACTACCTGCATAAAATCTTAGTTACTGCCTAGTCTTCTAAATTGCATGGTCTTAAGAGCTATTC',\n",
       " 'ACACTTCATTATTCCCGTTGACTATTAGCATTATAAATAGCCTTAACCTGTAAGTAGAAGGGACCTGGACAGGTCAAAAAACATAATCATCTAATGTATCTATCTCATATACATACCACTATTTATACTGTCCATTACCGGAGGTAAGAGATAACGACTACCCTTATCTCTCTTAGCTCTCTAAACGGATTCAAAATCTT',\n",
       " 'CTTCAATAAAATACAGTAAGTTCTCGTTTGTAGTGGAGGATCTGATGGACTTCGCACGCCAAACCGTAGTAGGTGACAGAGCAGTTTCTGTCCACTTCCGTGTTGTAGGATGACTACCACTATTTCATAGTCAGCACGGGTAGTATCATGTTAAACGAAAGCTGATTTTCTCGTATTCTGGCCGTTAAATGAAGACTAAG',\n",
       " 'GAAACATTAGTTTTATGGGCTGTTGCCACCAGTTAATAGAACTCGTAGGAACATGAATAAGTTAGGCCGTTATCTTGCATTCCTTTTCACTCAAGTGAGATCAGATAAGGGTGCAAAGCGGCCACGAAAGCATCAGTTCTATGCATGTACAAGTTACGCATGGTCATACCGAGTGCGTATGAGTGATTTTTTAATCTAGA',\n",
       " 'TGTCCCCGGAGGCTACTAGGTTCGCTCATATGTAAGAGAGACTTATACTTGTCTTTGAAATTATTTTAGACCTCTTTACTACAATACGCTCAGAGGTGTGTCGCGTTAGCTTTGATCGAGCTATCAGCGAGATAGGTAGAAGACTTGACGTTATCAACAATCTAGAGAGAGGGCGACTGCTAAGTACTGGATATCAATAA',\n",
       " 'AACCATCCGATTATTTTACGTAATCCAGAAGGGCTCTCAAACCCGGAAATGACTACCCATATAGGTACTAAGCGTTTCTACACTGAAACCACACGTTCAAGAACTGACTAGTACTGTCGAAACCATACCGCCTACGCGGAAGTTTTCCGAACTACTACCCGGAAGTGCAGGCGCCCACTGGTGAGTATCCTCTGGTTTTT',\n",
       " 'TTTAGTCAGATAGAAGTAGACACCACTTTGTTATTCCTTATGTGCACAGGATAGCGGCAAAGGACGAATGAAGATGTCGTATCAAATAAATGTCGCTTCAACGACATGACATTAGTTTCCACATTTGTCCGCTGGGGGTCGAGGAAAATTGCGTCGTGGTTAAATAACCTATCTAGGTGTTTCGGAATCTATCAACGAGG',\n",
       " 'TCTTTCGAATCCTCGAACGCCTGATACATGTTGTGTTTACTATATGTGGGTTTGATCAGTTACGTGACCTTTGACCCCAACCTTAAAGAAAGAATCAGGTGACCTTTGACCCCTGGATTTGCTTTATCCAAGCGGAGACGGTTACAAGTATCGCATGCGTTATGATTTTGTTTCCTGTGACCTTTGACCTCTTTGATGTG',\n",
       " 'TACCAGTAAGCTTGAATTAAGGCATGCACATATTACAGTCCCCGGGCTTAAGAGTCTTTTAACAATATTTAGGAAAGGCATTCACGGCTCTGTGTGCTTCGCGGGGTTGTATTCTTATCAGAACATTAGACATAACCACCAAAATGCCTTTATCAGATACCATTAGGACATCTTATAGTTGATTGTGAGCACCCTCTTAA',\n",
       " 'TATAAGAAATCACTGCTTACTGACTTGTCCTTTTATTCCCCGCAGTTACGCGACTACAGCTGTCGATCCGTCCGTTAGTAAAACAAAACACAGGATGACCTTTGACCTCCATAAAACTGCTCAACTACAAAATGACCTTTGACCCCTGCCTCCCCGCTTGGCTCGTGGAGAACATGTCAGGTGCCGAGATCGGACTTCGA',\n",
       " 'AATATTTCGCGATTCTTCGCATATAACTTACCTGGGCCGCTGGTGGTGTGCATACGAGATAATAGGATTAATGGCTCATGAGATGATAACTCCTATGTAAGTATTGATAAGGTCGCTCGGGAACTTTCCCACGATGGTAGTGTCTTTTAGATGGCTAGTAGAAGATTAGATGGGATATGGTTCAACTGATCAAGGTCGGT',\n",
       " 'GTTCGATTCCTTAAATGCGAGGCGGAGGTCAAAGGTCATGGACGGGATCTTTTATGACGCTGCTAAAACGGATTCTGAAAATTCTAATGAAAAGTATTCTGTAGCAGAACTCGAAAGACCCTATATCACCATTCCCCTAACAGCTGCACCACAAAAGGCCAGATGGACTTGTATGACCTTTGACCCCTGCTAATAGCCTC',\n",
       " 'GTAATGTCGGTGTAATAATAATCTATTGGTTGTACCGTTACAGTCTGGAGTCATTCTAGAGGTCAAAGGTCATGTGCTCTAAGGCTCTCACCTAACATCATTGGGCCCGTCCGTACAACTACGCGCGGAAACATGTTTAAGCATCTGGTTGACATTGTATGGACCCATATACAATTGTCTACGTTGATGTTATCTCTGAT',\n",
       " 'GATTAAGATAAGAAAGTAGCTTGCAAGCCCAGCCTTCTTATCTCTTTATTTTAAAAGTGACAGGAACAAATTTTCCCATCACGATATTCATAGTTCCCACGTTCTCTGCAAATCTTCAACTTGTTCGATATCCTGTAGTCATATCTGCCAGACGAATGACTAATTGTTGTGGACCAAATCAAATGTATGGGCGTTAGATC',\n",
       " 'CAAGTTCCCAGGGCACTAATATGTCACAAAACCGGAAATCCACTATATTTCTACATAAATACGACAACCCGGAAGTGACGGAGTTTCCAAACTATCAGACCATAATGACATAGGTCCATATTCACGATTGGATCATTAACCTTGCTTCAATCAAAATTCAGTCACACTTTTCGCATGCTATACCCGGAAGTCATGATGAT',\n",
       " 'GTAAGCTTCGATGGCGAACATTTGGAGCCCGGCTTGCAGCTTATCACCTAAACTAGTTGGTAATCACGATATCACTAGCGTGGTAACTTCAAAAACTAAGCTAATCTTGTGATGTCAGGTGAAGACATCTGTGAGTATGTACCTTTTCATTCCATCCATATCAGCAGGAGGTAGAGAGATTTTTTAATGCGCACAAATAC',\n",
       " 'ACAACAACGTGATTATTTACGGTACGCAAGTCCTTAAACGATCGTACGCCTCCCGTAGAACAGGTATATTTTGTATGTAGGGAATCCTACACCCTGTGACCTTTGACCTTGTAACAAATGCTCTTCGCTATAGATGACCTTTGACCCCACTGGCGCAAGTAGGCAAGAGCAGTAGACAGCGACGTGCTATTTACATATAC',\n",
       " 'TAATTCAATAATGTATCTAACGGTGTCATATTAAATCTAGAGTAGAGCATTTGCAAGAAAAAATCGTTGCTCGATTAGGAGCGAAAAAGAGAAACCCGGAAGTGAGACTCGGGGTTCGGCTGAACCCGGAAATACAAGCCGCGAACATAACATAGTATTCGGTACCTTTTTGTGGATTAGTCAAGGCACTTGCGTCAACC',\n",
       " 'TGGTGGCTACAGATCCGCGTTGATTCCACAATTGATCTGACATAGTCGTAGATCTATTTACGTTACGTTGGGTTACGGAGTGGTTTTCTGTCATCTAGAGCATTAGATGTTGGGTTTCCGGCTGTGTAAGTATTTCATGTCTTTTAACATAGGCAAGAAGATGCCAATTATCTTTGCACAATGTTATTTAGAGTTCTTAC',\n",
       " 'TGATGTCGGTTGACCTTTGACCCAAGACTAGATGGCCTTTATCCCAGAGGTCAAAGGTCATCAGTTTACCTCAAATCAGGTGAAATCGTTATTGTTCGTTGCCTTTCAATCGGCTCCCTCTTTTAACATGTAGGAAAGGTTGAATCCAGGGTTCAAAGGTCAACATGGTAGTTGATTCTAGACAACGACCTAGTTTTTTG',\n",
       " 'GCGTACCTTGAAGAAGAAGACAATAGCGAAGTTCCTATCACAAGGTTCAACCTGAGCCTTATCTGATGAGAGAAAGAACACCCACCAATCCCTGTCTTGATGCCTAAGTAGTGTTACAGCTCGTCATTAATGTGTATGGAGGGGGAGTTAACACGAATACACCAGTAGACCTAGCCAACCTCTTATCAGACAACAACATG',\n",
       " 'TGTGGTTCAATAGACTCCTGGTAGATCGGAGCTTGACTTAGCTAGAACTTTATCTATAATTAAGTGGATAGATGTTAAGATAGATTGTTGAACTTTGACCTCGAAGAGGCTGAACAGTGATTATCACCTTTGACCTTTTTATTGACCAAGCAACCTACCTGTCGACCTTTGACCTTCATAATTGCCCTTCGGTACGTTAT',\n",
       " 'CTGTCAGAGGTCAAAGGTCACGCGTATTGAATATATTATTGAACCGGTCTCACAACTAGGTTGAAGGGATGTTGTAACGTATGGACTTAAGGGTGTCCCAACCATTAATATTACTATAAAGGAGAACCTACTTTTTATCACCTTGTAAATAATCATGGGTCAATTAACTTTTAGTCAAGAGCGCTCTCTATCCCCAGTAT',\n",
       " 'GGGCTCTAAATATTCTCATCCGTTGATCTTGGTTTTATTCCTACTGACCATGTGACCTTTGACCTCTGCGTTTCAGAGGTCAAAGGTCACCTTCGGGTATTTTTTTTAAGTGGGTCATTATCACTTTTCGTAAAACTAATGGTACTAACTAAGGGGACAAACCAATCTTAAGCTGGCAAATTCTCACGGAGAGTTAATTC',\n",
       " 'GCCTTTCTTTTACCAGGTCAGCTGGTACATATCTAGACGGTGTGAAAACAGTAGAACCCGGAAACAGGGAATTTTATGTCATTGTTCTGTTGCATGGCATTTTTGCTGTCAGATGAATCAAGCAAATCCCCGCGATGACGTGCACAAAGTTATTTGTTTCCGGTGGTCTCAAAAACCCAAACAAAGGGGTTGGAAGTATT',\n",
       " 'TAAACACAAGGTCAGATTTATGCACAGTAATTAGAGGTCAAAGGTCACACGTCTTATCTATGACCTTTGACCCTTAAAAGGTCACAATGCGAGCAATCTGGCCGACTGTCAATTCAGCGACCGTAGAGGTCAAAGGTTAACTTCGAATGAAACATTCATCTAATTGCGAGGCATTATCTGACATACGGCATAGTTTCTGT',\n",
       " 'GTTTACTTGAGCAAGGGACGACCAGAAGCATTTATCCTCGGTTCAGTTAACAGATAGGAGTTATCGAGGGGACTCTTGACCCGTGATGTATCATTCACGATTCCACTTATCAGGCCCACACACCAGTTACTATACTTCAGCCGCTCAAGAGAGTACGTAGAAGTATACGCACTATAGATTATCTTTAAAATAAGTAAAGT',\n",
       " 'ATCGCAGACAGGGCTCATCCAATGCTATGGTACCAGTAAAGGTCAAAGGTCATGTGAATTGCATCTTGAAATTTGGCGTCTGTAGGTTTATTTGGGCTACCCACCTGACGGTAGATGATTCTGTTCGGATTGACCTATCGTCTACTAGACCGTACGCTCGGTTTGATCGGGGGTCAAAGGTCACATTTATGCCGGCATGT',\n",
       " 'TTACAATTATTGTCATGTCCTCTGAAAAGGGAACATATTTTTTACTTCCGGGTGCGGTTTGATCTTTCATTTTTAATCTACACAGTTATCTCATTGGTTTGTAAGAAGTCTAGTCTGAGCCTCTTTTGCTTATTTACGTCCACGGATTTTTTTTCAGGCCAGGTTAACGAGAACAATAGACTATCTATGGTTATCCACAT',\n",
       " 'CGAAGTGAAGATCTGCCTGCCAACCTGCTTAGAATAATAGCTCTACTACGAATTAAAGTAGCTAGTACGAAATTCCGCTCCCGGAAGTCCTGGTTGATACGATGTAATACATGGAATACCGAAAAATTATACAGGGATAATGTGAGAAGCACCCCGATAGTCACGTGATCCACCCGGAAGCGTATTAAACAATACCGAGT',\n",
       " 'GCAATTCTATAAATACCTAGGTCTTAATACTGGGTACCGCGCCCTTGAGAGTCCCTTATCGTAATAATCTTTAAATCATGATATACTAAGTTGCAAGACACCGATCATCGTTGTACACAAATAACGAATCAAATCTTCACAGATGTCGGATATCAGGGTCAAAGGTCAAGTTAGGGGAAAAAATAAGAGCGAATGATACC',\n",
       " 'TATTAGAACCCGGAAACGCTTCGACTTATAGTACATTATTAGCTGTACAAGTAAAATAACGACTTACGGTAGCTAGTGCTAAGTATTCTATCTCATTAGGGAGAATATTGAATCCTAGGAAGATTACGCTAGTGGGAGAGCGGTCGTGAAATAACGTCGGCCGCAACCTACAGTCGTGGGCTCGAGCTGTAAAGCTGCCC',\n",
       " 'ACTGATCACTCTAATATATAAAAACTGGTAGGGTATTGTAACTGCACGTATCTTGCTTTCACCCTTATCATGCAACACCACCGAAGCGCAGTCCTGAAAGAAATGGGCCGAAAACACTCGTTCTATTTCCGCTTATACCCACGATTAGAAAATCCATGATCGAATCGTAGTGATAAAGGGTAGCGTTTATCTTATGGGGC',\n",
       " 'GCTCTAATGTACTTATCACGATCACAATTAGTTACAAGACGACTCAGTCAAGTAACTATATATAACTCATTATCCTTTTACACACCGAATTTCTGTACTGTGTCATTATTTAAATCCTAAAAATTATTACATAGTCGTGCGTAAGCTCTTTGTACGCCCACGATTTGGGCAAGATAAGGTCGAGTGATAGCGGAAAAAAG',\n",
       " 'AATCCAACTTAAATCAGATAACGGTATAGAAGAATTAGTGCTATGTAAGCTTCCAAGTTTCCAAAAGCATGTCTACACCGATAGTAATTGGCAATACCTTCAGGATATCCAATTTCGGGTTGTCCCGGATTGTTAACTCGATAATGGATTGGTGCACCCGCCTGAGACGGTATCTGTACCACAAATATCGTCTCAAAGTT',\n",
       " 'CACGCATCATCGTATATCTAAAAAGTAGACCATACAAGTTACGCACTTCAGTCGATCCCATTGAGCGTCACTAAGAATGGTGGACTGTGAGCAAACACAGACTTGCCGATTTGCCATGGCGTGTTCGTTCACACGGCTACGCTTGAAAGGTCCGATACCATAACCTCTATTATCCGTTATGAATAGAAATGATTTCAGCT',\n",
       " 'TACTCTATTTCTTAGGATCAGGATACTTTGAAGTTCCCTAGATTTTAGCCCCGGAAATAGAGCTTAACGGATCCCGGAAGTAACTTGGTATGACCCGGAAGCAAATAATACCTGCGCTACACTTCGACATCCATACTTCCGCAAAACTGGGTAAGCTTTGACTGAATAATTTGCAGCCCGTGACCAATGACTAGAATTGG',\n",
       " 'GATATATTAGTGAGGACTAAATAATTTTTATGTCATTAGGGTCGCCGCAGAGTACACGCTAAGATCTCTCCAAAATATTACGTTTTAATAGTTGTATACTATTTGGGTGCGTAAAAGACCATGCTCCGACTAATGTAATTCAAAAATTGTGGCCTCTCGATAGCGCGGAAGTGTATCTGGGAGGAGTCACCCGGAAGTAT',\n",
       " 'GCAACACAGTCAATAGGGGTCAAAGGTCATCGGGAACTCGCCGTTGCGTGGCACCAAGTTCTAAATGGCTTGGGCGCCCTTGACCTTTGACCCCTACATAATTAACTGACTTACGGTCGCGGGTTTATCTTAGACCCATAGTTTGATACAGCAGAGGACGTGGGGTCAAAGGTCAACGGATAGAAAAGCTGGTGAGGTGG',\n",
       " 'TGCTCACAGACACAGTTTTTGTTATCTCCAGAAACAAGCACTTATTACTATCTAATTTCGGCAGTTATCATTCACTAATATTTCGTCTTTCATGAAAATAATTCAAACCTTAAGGCCAGTTGTCCGCCCCCATATTTATATAGGTAGTGAATTTAGTTGTATCGGACCTCGTTGTTCACTGAGCGTGATGATGACAACGG',\n",
       " 'GCTCATTTATGCTGCACATATTTTTCATTCCACAGGTTTTGTTATTCTCATCCCGAAATTCCACTATCTGCCTCAACTCCAAACATAGGATACACTTTCAAGGCCGGGCCCACAACCGTTGAGAGGAAATGATAGTTTATTTCTTTTGCTCCGGCTAACAAATATCTGCTAAGTATATTCATTGGCAGCCGATAAGGAGT',\n",
       " 'GCTCTTATTTGGCTGTCTAATTGTGCTCTAATTAAATATCGCGCTTTAATCGGTCACCGCGTTATTCGATACCGCTTATGTCCATACTTGTCAATAAGCTATTGTGTCAAAAATATAGAAACATACATAGCTGGCAAGTTCGTTTCGGAGTACTCAAGTGAAATTGAAATGGTTGACCTTTGACCTTTGGACGTTACATT',\n",
       " 'GCCGTATAAGTTGTGTCCTAGATTGGTTGAGCTGGGATTGTTTAAATACTCGTTACGGATTCCATTCGTCACTTAAATATAATTTACCGATACCTTTGACGCGGGCTTCAACCCTCGGGATCTTTATAGTGAGCTGAAGTTCATCGCTTAATTATTTCCGGGTTAAAAAGGTTTCCTTGTGCCAACCAATAAGGTTGGAG',\n",
       " 'TCCACTTCCGGGTTTAAACATTACAAGGCAAATCCAGTGGAGGGGTCACTGTTAGCTCATAGTGAATCCCCATACAACAAATTCTCTAGAAATCCGGAAGTGCAATCTATTTTTACCGGTCGACTGATTTACTTCCGGGTGAGGCCTCAATAAACAATCTATACGTTTCTTGTAATATCGATTGTAAAATTAACTCGATC',\n",
       " 'ATAGATCTTACATCGGTGGTTCTATTAAAAAATTTTATTCTTTGGATTATAATTCATCAATGAACTAAGTCTAGCAAACGGTAGCTATGGGGTCTTAACCGGAAATGTTTCCTGAAGGTAATGAATAGACAGATAGCATAAGATCGTCAGGTCTGCGTTCGGGTAGCTTTAACGTATTGATACTTCCGGGACATCTCGGG',\n",
       " 'CCACTTCCGGGATTTAAGGATTTGCAAACCCTCGTTAATTTACTTCCGGCTCTATATACTATGAGACCAAATTTATCTCAGGCTCGGGCCTCGGGTCGAGTGCCCGCGGAGCATCTGATACTACATAAACCTACTAGCTGGTATAACTTACATAAATTTATACAAAATATCGAACCCGGAAATGTAACTCCAACGGGACC',\n",
       " 'GAGGGTGGGTTCTCGTTAGCTCACCCACTCTGCTGGGAGGGATCTTGGGAGATTTGTGACTGTGACATAGGGCTAGCAGGGGTCAAAGGTCATCGAAATTATTCTCACGTCTTACGTCACTTGTGAGAAATTATAGTATAAATCAAGTGTAATATGAGAGTTCGTTTATATGCATATCTGGTGACCTTTGACCTCTGGAA',\n",
       " 'CCCTTATTGTAACCACAACAAAATGTCCTGCCTAAGGGATATTTTCCACACCCAGCTTGTAATCTGGGGTCAAAGGTCACGTTGGCACGTGTCTTGGGGTCAAGGGTCACCTGCGTAGCATAGCGTGGCCAATGAAAAAAAATCCTGGCGTAGGAAATAATCAAGGTATGATGACCTTTGACCCTTCGTTCTTTTTTCGG',\n",
       " 'AATAACGTTATTACCCGCATTGCTAATCAGTCGAATTGTTTTAAGGACTAGTCTGTCAAGCATTCACCAGGTGACTATTCACTGTTGACGCATGACCTTTGACCCTGTAAACTTTTACTAAAAGGTCAAGGGTCACCTGTCGAGATATGGGACGAACCTAAAAAAGGGAACTCCATAAGACCAAAATGCATTGACTAATA',\n",
       " 'TTTAGAAGATGCGCCGCTTCGATAATTCTGTTTACTGACAATTGTCTAAATTTAAGCAAGTCGAATTTCTATACTGTTTGCTCTATGACAGGAATAACTCTAGCATCAATAACGAACGTGATGGCCCAGGTGAAAGGTCACGACTTTTATTCGTAGAGCTACAAACATATAAACTAGTAAAGCAGCCGGCACTTTCCTGC',\n",
       " 'TACAAATAACGTTAGGGGTACAGGATAATTAAGCTAAAATGTCATCTTCACGAGCTTATATTTGAGATAAAAGCATTTTCTATCTCATGTACTTCTTGACAAACGCCATTGCCCAATGCGGCAAGTAGGAGATAAGCATCAGCAACACTGCCTGGAGTGACGCAAGGAGTGTAATCAGCCTAGATGATCCATTAATTCAA',\n",
       " 'AAACAGCAGATAGGGGTCTAACACTGGAAACTTAATTGAAGGGCAATCTAGTGGCACACAGGTTGACTTCAGTAAATACTTTATCCAACCAAAATTCTCAATCCGTTTAGTGATGCTTCGTAAAGCCTCTTATCAGTCACTCATCCTCTCTAATATGAGCCACTTTAGTTATATAGTGGCTCAGTGGACATTCTGACAGC',\n",
       " 'TCATAGACGTCGAATGCACCGCCAATCAACCTTTTGTTCGCTAACAGAGCTTTCTTTCATATTAACGTCCGAACAATTTGAGTCTTATCTGGCACCTCTTTGCAAGATATTATTCTTGAAATGTGAGAAAGCCCGCTCTAGTATCTGCTTCTTTAGGGCGAAGATAAGACGCCCGAAATATGATTGTACGCATATGCAAA',\n",
       " 'AACCACAATTTCATAACTTGCCTTAAAAGCTGCTCAAACTGCCGGTATACTTTGTCCCGGAAATGGTCGGGACCGATGGTCGCACACTTTGGTCTGCCCATGTATGCCGGAAGTAAAAGATCGCTCACTTGACATATGATCCTGGTGAAGCGGTTATGCGTTAAGATAAACGCATACGAAAGCGCCAAAACAGATACGAG',\n",
       " 'TGTTATCCTGCTTATTATAGCCCTACCGAGTGAAGTCTGTAGGGTCAAAGGTCACCCGCTTCCGAAGAGATACGTTTACTTTCCTGTTTCTCTAGACTATCCCCTCTCAGGAACGGGTCAAAGGTCACCAAGCTGAGACATATTCCGACCATGACCTTTGACCCCACATTACCTAGCCGAGGAATAGACTTACTCATTAA',\n",
       " 'CCAAGGGCTGATATATTAGCGAAAAAAAACCCGGAAGTGCAAATGTGTAAATAAGGGTTATTTCATCTCGTTGTTTGAGAAAACCTAACTACAGCGCTCGCAACCGTTAGACTCTTGGACGTATCCTGACTGATGGTGATTCAATGTGGCTCTCCTTGTCTGTGAGTCTTTTCTGCGGTACAAAGGGCCGCGGTATACGT',\n",
       " 'GATAATTTGTTCATCTCGTTTACCAGAACATTGCATTAACTAATTTAAACATGCGTGAGTAAGAGTTTCGCCCAATCAACTATCATTGTGTAATAGGAAAGGTAGTGTTCGAGCTAATATGTTTTGAGGTCAATTTAAAGATTGATAGTCTTACTTCCGGGTCGGTCGATATCCACACATTACTTCGAGGTTCATTTGAA',\n",
       " 'ATCCAGACTCCCAGAGGTCAAAGGTCACAGGTTTGCTTAGTGATTCCCACGCGTAACGTCCTGGGAATGATGTTAATATTTCACTTTACACGGCCAGATTATTCTAGGGGTGATCAACTATACACCATGCATCAGATATTACAAGCTTATCAGTGTACATCCTCGTGTAGGGGTCAAAGGTCAACTAGTGGTTACTACCA',\n",
       " 'TTCGATTTGACAACACAAGTTGGCGTTTCTCGCAGGAGGGAAAACATACCTCGGCTCGTTACTCGCACTTCCGGGTTCACTTTTTTAAGATAATAGAACATGTAAAGGGATGCAATAAGGTTGCCGCTCTATCCCTCAATTGGGAACATGAATTTTAAAGAAGAAAATACGCCCAAGCCAACGTAACAGAAACCCCATTT',\n",
       " 'CAAAATTGCATTACTACATAACCTATGTTAAATTATCTATCACTCATACAGAGTTTATCCCGCGAAAAGAGTGGTCCTTGGAAATGTTGTTCAATAATGTCCATCTTCCTTTTTTAGACACATGATGGAGCTTTCCTGCTTATCAGATTCTGTCCGAGCAGTTTTTTGTATTATTCGCAACTTTGTAATCGCAAAAATGA',\n",
       " 'GGACGATTTCGGAGGATAGGCGACCTTAGTTTTCCCTTAGTATGCACAACGCCGATGAATTGTCACATATGCGAGCTTATTGACTTATACATAACAAGCGTGCAAATGGAGAGCTATTAAATCATAGATGATCTTTGGCAGCTGCCACTTAAGTCGACGAATCTACGATCTGCTTAAGCGTGGAGGTTAAGTTGACAAAA',\n",
       " 'ATATGTCCATTATAAAGAGGTCAAAGGTCACCTCGCCTCTAGATCGTGATCAGAATACAAATCATCAATTCTCCGGTCGTAAACGGATATCGATCGAAAGCTCGCCCGCCCAAATGCCGCTGATACAATAAGTAACTCGGCATGACGTGTTGACCTTTGACCCCATGTAGTAGTCTTGGGGTCAAAGGTTATCGGCCAAA',\n",
       " 'CTGTACACGCTGAAAAATTTCTAACACTACGATCTCCGACGTTAATATATTTATCATCCTATAAAATGCCCATTAACAGCTGGAATATAATGGATGACCTTTGACCCCAGTAGCGTAGATAATGAAACGGTCAATCCGAAGGGTCAAAGGTCATCGGTTAACACGGACTACAAGAAGGAATTTAGTAAAGGGTTTAGGCA',\n",
       " 'TTTACATATTATATTATTGAGTGTCTGTCTACGCAGCGAGTAATTCAGTGTATATATAGTAATGTACACATCTATGACTACTCTAAGTAGTCCGGTTCTACAAAGAGACTCTTATCATCGGATCAACTCAAGCCTGGTATATACCTTACGCTAAATTATTTTTTACATCATAACAAATGGCAGCCAAATATATTCACAAG',\n",
       " 'CTGGAGCAACTGAGCGATCCGAAAACGATTTAACGTGAGCTCTTTTCCAATTACTGACGCTGGAGTATCTATGGTAATTCTAATGCGATCCGTCATATACAGACTGAAGGTTGGGTTCAAGTGATAACAATTTTCTCTTATAACACAGAGGTGGTGGTGAATTGGCCTCGCTCAGGATTTTTCTCAATTAACATTAACAG',\n",
       " 'AACACAACATTGCTACTAATGTCTAATATATTAGACACCGAACCATCTATAAAATGTTGTCGAAAACTAAACTAGGTCTTTTATAGGTCGAAAATTACTCTGAATCATATTTTAGCCCAGATTCTGGACACTTCCGGGTCCGCTCTGACCCAAAGATTGCTTCACAAACTTCATTTGTGAAGAGTTGTCGATGTATATGT',\n",
       " 'CCACGTGCTAACTTTATGGACAGATAAGCGGTTGTAAGGACGAAGCACCCATCGATTCCGGAATTCTTTGTTCTTGTTTTTCACCTAAATCAAGATAAATGGAGCGATGACAAGCCAACCATATTGAATAACTAATAAAGCGGCTGTTATACACATTACTATCATGTTATCGATCGCCTTGTTTACAGCGCTATCTCCAC',\n",
       " 'TGAGCGTCTAACGAATCTGTTCGGTCCGACCGACAGACCTAGTATGTACTATATTTAGTTCTCTCCGAAGGTTAAGTTGGCAAGCTGTCACAGGACAGTAACTCGGACATGTTCTAAGGTCAAAGGTCACTGGCAGAATCTTCTACTTACCAAAATACCAGAAACGGAATGGGGCAGGGGTCAAAGGTCAACCTACAACA',\n",
       " 'CCATTCCATCGATGAATAGTCAAAAGGTCAAAGGTCATACGTGATAGGACCCAAAATGAGAATATGCTAAGAACATCTGTTCTATAATGAACTTCGACGGATTAGACGGACTAACCATGACCTTTGACCCCAACATACACACTGTGATGAATGAATACTAGGCTGGTATGACGATAGCAGACAACGAAACGGTGATACGC',\n",
       " 'ATGAGCAATACTTACGAAGCTGGCCAGAAGTGTTTTTACCTGTTTATACCTCTTAATAAGTACTCAGTCTCAACGACAACTTCCGGTACAGGGGGCGATGCGGTGAGTTGTGAGGATATTGCCCCGGTTACACTAGAACGTAGTAAAGAGCTTGGTTTGGTGGGTGTTGTGTATCGAAGGATCTCAGATCGCAAGCCTTA',\n",
       " 'AGAGTTTACAATTTCACTATCAGTGCGGCTGTATAAGCTTTTCAGATATAGAGGCATGATATTTGCCAGGCTAATCGGCTACGAAACGGTAGCCTGATAAGAGCTCAATGTACGTTTTCCTTTGCGGGATGAACACTTATCCGCATCCCTATCTAAAGAGTATCCGGGTAGGAAGCCCCTATACCTAAAAGTGCATAAGA',\n",
       " 'GCACTTTTCTCTATCCGAACCATCTGTCCATATATATACTAGATAAGCTGTAAGCCTCTCCAGTAAGGTAGCCACGTTTCATAGGGGAATGTATTCCCATCATTAGTTCTTGACATTTCTTACCGCTAGCTTCACTTCACGCAGCGTGCCTTTTCGACGTATTCCTCATAGACTGTCACTTCATACGGCTGAATGGATAT',\n",
       " 'CATTTCAAACGGATACATGTGATGTGCTCCGACATACCCCATCCAGCCTTGATTACAAAGCCAAAAGTTAAATCCCATCGTTGTGTAGAGTCCTGTCCCTCTCTCACTGTACACGTTGTTATTAACTTCCGCAATTGCAACAACCAGAGACTACATCGTACCTTTGTAGATGTCGATATCTAGAGTTGGTTGAGAGAGAG',\n",
       " 'GTATGCAATGAAAGAATTTGATTGAGGTGTATCTTCCTCAATAGCTACAATTTCTCCGGTATTCAATGCTATGAACATAATACGTGTGAAGGCCAAGTAAATGATGCGGGGTCAAGGGTCATCGACGGATGACCTTTCACCTCTCTCCTATATTATAGACTCTTAAAACTAGATAGGAATAATTTGATAGGAAATAGAAA',\n",
       " 'ATTTGTGAACAGCAAGCATCTCTCTCCACGTGAGTAACACGTCTTTTTATTGGACGGTGCTATGCGATTCCTAATTTTCCTCCATTTCCGGGTTTTAGACTGTTAGATAAATTGGATAATCTGGCGACTCATGATAGGGCTGCAACATTGGAGGTTACATTTTTTCTCGGATTTCGTCTGGATTCTTGATGCATCCATAG',\n",
       " 'GATTACGGCAGGGAACCCGGAAGTGGTCCCCGGAAGTCCGGACACCCTATTCTCTTGTTGAAGAGATGCCCTGCAATCACGATTATAGCTAGAAATAACTGTCCACGTGGAGATTGAACCATGTTAACAGTTCAAACTTGTGGTTGGACCGAGTCGACACACGAAAGTACTAACTGCATTATACACGGAAGTAATCCATA',\n",
       " 'ATAGACGGATGAGTATGTCGTAGGTTGCGCATGTTGTGATGCTAAGGCATATTTGTAAGGAACAATTACTAAATAAATATGATGTTATAATAATTTGCGACTTAGCCGTTATCAGTCGCTGTTGAGTGATGAAATTGGGAGATCATCCGTTCTTTAAAGGGAAAAGATATGTTGGGTATGTTTTGACGTTAATTTTTTAC',\n",
       " 'AGTTAAACTTTAATATCATTGAGCCCGTTTTCAACCATCCAAACAACCGAAATTACCGGCCGTTCACCATGATAATGTTCTACTATTACTCGGTCTGTCACTCTGAACCAGGATTAATGATCTAATTATAAACTTATATAGGCTGATAGGTCGTTATGGTCAACGCATCATCAGTGTTCACTAAGGTATATCGGGTTGTA',\n",
       " 'TAAAGTTCCGCGATGACCTTTGACCTTATCAGAAATTGAAGAGGGTATATATGGTATAGTTTTCTAGTGTTAACTACCTCCGTTGTAAAAGGTCGGAGGTCAAAGGTCACTCTTTGTGGTATAGCGCAGGTGCCTTGGCAAGTTGTCATTGTTCGTCTTGCAACGGCAAGAAGGCAATTCTGTCTCTATTCCCATTTTAT',\n",
       " 'TGATGCAAATAGTACACACTAGTTTCAATGAACACCATTCTGAGCATAAGAATGTACTACTGGCACATAGGGGTCAAAGGTCATATTTTGAATTTCCGTCACCAACGAATGACCTTTGACCCTTCATTAGTTATAAACCTTTCTTTAAAAACAGAACGGACGGACCCTTTTAATGTTGTCGTTTGGTAAGGCAGGAACTT',\n",
       " 'TCTACGCGATCATAATTATATGCTCCGGTCAAATCGTTGTGTGGGTATATATTAATCTAACTTGAACCCGGAAGTGCGAAATAACGAAAAGAATGTTAGGAAAACTTAAACCCGGAAGTGAAAGTAAATATCAGTGACTGCTCAATTCAGGATTGATACCTGCGTACTTCCGGGTCGCAGAATAACCTCTTACAAAGTGA',\n",
       " 'AGGTGAACTTATTATTCTTAGGGTCAAAGGTCAATTGTTCGCTAACACGGGATGATGGATCTGAATTTAATCAATTCCCGAGGACTATTAAGCAACGGAATGCCCTATGATTAATTCCCATCATGTTGACATAAGGAAGGGATTCGTGAGGTCAAAGGTCATCATGTGTAAAAAAACTCCGGTGAATTACAAGTCGCCTT',\n",
       " 'CTCAAGACTCTACGCGAGGAATTTGCCCTTATTATCTGGTTTCCAAACAATTTAATTTCGCACGTACGCATTGCATTACATACCATCCATTCTTTGTAGAGTATTTCAGGCCCCTGTTTACTTCGCAAAGTCGACAGATCTAACAGCCGCGTACTCAAGAGATATATTTCAAGAAGAAAATTTTTGAAGCCTACTACCCA',\n",
       " 'CATGCCGTTACTTAGCCTATGTAGCCGGTCTGCACAACACCACTTTTTCTTATGATAACGTGTCGTTACATGAGGGCCCAAACTTGAACAAAGTATCTAAAACAAACTTAAGATTCACATCTACAATTAATTAAGGGGATTCTTTCCAGGAAGAATAATCACCGTTCTGTCTCTATGGACGCCTTGAAGACACGCGGAAG',\n",
       " 'GGCTTCTGTCCCCCTTCATATCAACTTAGCCACCCAGGAGAAGCTGTCTACGAACTACGAATCTTAAACAGAGTTATGACCAGCATTTCAATGTTCCATTTCAAGAAATTAGATAGAACATTGTGGCAGGATATGTCGGTTGCCAGAAGCGTGTAACGTGCCAGATAAGTAAGAATCGGTAACGACATTGTTGACGATCT',\n",
       " 'TATCGTCCCTGCACATCAAATGGTGACTCTAGAAAGTCGAACGTCGCGGAAGCTGATAACTCCAACCAGCTTCATCGAATCAACTGATATGGAAATTAGAGGTGAAGAGGTGTTCCGTTGCAGCAAGTACAATGGCGTACGATGCACAACAATAGATCCCCTCTCAGCAGATATCGTACTTTTATCAGAATATTATAGAT',\n",
       " 'ATGCGTTGGCCAGTGTGCTCCCGTTCTTCTTTAGCAGTATCGCTAGTGACATTCAGCTCTTCGACTCAGTGACAATTACTCAGAAGTAATTTTGAGTTTGACTCATTCGAGATAGCTGATATGCAAGAAGTTTGCCCTATGCTGTGAGTTTCACCTATCTTGTTTCTACCACGTGGGGATGTAAGCGATTACGATGCACT',\n",
       " 'TCCCCGTTTATAATGCTTGACCTTTGACCCCTACATGGTTCTTTTCTAAATATTGAACAGTCCCTTGTGGATTTATTATGATTTACAGAAAATTGTAAGTAACACTATCCTCATAGTCATTATTCTTGCGTTGCTCGTGTAAGCTAGTTATTTCACAAATGTACGTACTGCTAGGGTACTGGTAAACTACAATGGCTTAT',\n",
       " 'ATTTACGTGAGTTGTAGTCATCAGATCCTAACACTATGTCCGACTTGTGATTTAAAAGGGAGAATTGTAGTCGGCAAATAAGCAGATCTTTTAACTATTCCCTTGACCTTTGACCCCTATAACCACATTACATCCAACAACCATTCTTACAGGTAAACAGCCACCACACCAGTATCATAGACTGCGAACTTCCATACTAA',\n",
       " 'GGACGCAATTTCCAAGTACTCTAAAATTAAATGTATCTCGCGTCCGATGCTGTACGAGCGATTTGACTTCATTATCAGTTCGAGGGTATCAGGCTTAATTTAGCGATTATGTTTACCGATGGATGACCTTTGACCCCTGGAAATTTCTCTTGGGGTCAAAGGTCATCCTACCATTAGATTAACCTCAAATCAGTTAGTTA',\n",
       " 'TCATATCTCTGCGCACATGGTAATGTCTGATCCATAGGGAAGACCTGAGCATTTTAATGGGTAGTACGTTGACCATTTCAGAAGAAAGGATATAAGCATAATGAATAACCCCTGAGGTCAAAGGTCATGATTGACCCAAGGGCAATTATTACATGCTTCCCATCCACCCCGTGTCCGATGACCTTTGACCCCCGTATGAC',\n",
       " 'ACTATTTCAGGTAAGGAATATCCACTTACGAGCATGACATAGGTAATTGTAAAATGGTTGTTGAGATTACATATGATTATCGCTTACAGTCTAGCCGTCGAGCCTTTAAAGATGCCTATCAGCACCCAGCGCAATGGATAAATTATTTCTGATAGAGCCATTTGGAGGTTTACTGAGTGGAAAAGCTTGCGAGTAAAGAT',\n",
       " 'AATACAATCGTACAGCGTGGAAAATGACCTACTGGTACCAAATCTGACTAACTTGTCCTGGAACTCATTCATTTCGGATTTATTTAGATTTGCTAAGAATCCAAGAAGTGAACCTGTTCGGTGTGGATATTGTTGACTTCCGGATTGACAAGCTGTCAAGTCTAATACTATAACTTCTTTTCTCGATACATGGACGAGCT',\n",
       " 'GTAAGTCCTTTTGTCATTAATCCAGACCGAATATCAGAGTATAGCTTACTGTCCAACGAGGGCCTATGTGGATTGTTCACTAAGGGCCTCAAATTCGTGTTTTAATGCCGCACTTCCGGATGTAAGGAAACATTGATACACTTCCGGGTTCTGGAGAGTTCTCCAATCCACCATGATAAGGTATACTCCAATTGCGCATA',\n",
       " 'CTCTGAAAGGCAATATCGCCTATTTACTTGAATTAGATGTGGAACTTAACATTTTGGACAATGCAAGACATAGAGTAAAGATGTAGTATTATGGGGGTCAAAGGTCATCTACGACGCTCAAACCCACCTTTTTAAAAGTCCCGGAGTTACTACTTACGTGGCCTCGGTTTTGGCTTTAGACGATGTAGATATGACGACCG',\n",
       " 'GCCGTCTATTAGCAAAAGTCACTTGAGGGAGGGAAATATTCGTAATGCTTTTCACGCTGACTAAGCTAGGTTGAGTCTGGAGACATGATATTTGCTATACTGTGTTGGCCAGCAACGCGGTTATCGTCATTTGGCGACCATACGAAGTTCGTGTGTCCGTTTAGATCTTTTGGCGAGCAGTGACCCCTTGTCCGCGTCAA',\n",
       " 'CCAATTGTAAATTCACGCACATGTCCGTGAGTTGCCGCCTTATTACAAATATCGTCCATTGTCTATGTTTTGGCATGTCTATACAATGGGTGTGCGCCAGGGTCAAAGGTCAACCTACTCCGAAAGTACTGAATAATGTAAATTTAAGCTTGAAGATACCCTGAACAACTTGCTACTACATGATCGTTCCTAATAGATTA',\n",
       " 'GGTAATTAATAGCTCTTAGCATATACTCTAAGGAAGAAACTAACGTTTCTCAAATAGGTATTGCAACGTATATGTGGGCGAGGGGCGTGAAAAGTCTAGGCTTCGCCGTTGTAAAGCAGAGGTCAAAGGTCAACCGTATAGTCGATGTTTGCCTCTAATCAATTCCCTTTGGTAATTATTGCTGATCAAGCTGGGTGACT',\n",
       " 'TAGGCACTATCTGACAACAGTTCGTACCCCCAGTTGTTGTAGGATTCACTGAGTATATTTAAGTTCCAAGTACGAATAGTGCGTGACTTGTCACTGACTCGATAATGTCTTATTGACTGTTAGTTTCCTAAGAAGTTTTCTGACGTCAAATAGGGGTTTCTCATACTTAGCCGTTTACCACCCATGAACTGATGCATCTT',\n",
       " 'ACTAAGGTTACACACCGGGCAGTTTCCAGATATTTAGGATAAGGCGATGCTATAACCACCATCGGGCTCCTAACATTCCTTTGCGTAGTTGTCGTTTAGACTGGAAACGTTAAATTAGCAACCAGTAATACTCCTGCGAAGCTACTTTTGAGTGGGACCAGAAATCCGAGCCTAATATTCTGACCGTGGCAACATCAATA',\n",
       " 'AGTATGCATGTGTCAGCCCGGAAATAGATTAGTATCACTTAGCTGGTATATAGGCTTATTACTCATCTGAACCCGGAAGTATTCACTATGTGTATTGTTTAGGACACTAAAATTGAGAAAAAAACGTATGGATTCAGATTTTCTTCAGGTCCTACTGATCAACACGTTCACTTCCGCGCTCGATACTCTGTATAAGATCG',\n",
       " 'AATTTACTCAGAGTCTTACAACTAGGCCTATCAGTTCGGAAAATGCGCACACGGCACATGAACATAATATGTCCAGCCGATTGATATGAGTATCTCTGCAACGCAAGAGAGGGGTCAGACTAACATGTTTCCGGCTCAGGGTGAAACATTATAAAGCCGTGCTAACGAAAATTAGAAGTGGATCTATCAGCGACGATTAA',\n",
       " 'TCTTATATGACTTCCCGGAAGTGAGTGCCGATCACAATCGATTATAGATTCCTGACGTGAAAATGGTCGTAGTACGAAATTAAGTCGAACTGTGACTTCCGGGATCCTTATCTCCCGTTCCAACGTAGAACAAATTATTACTTAACCACCTATAACCCTTACAGGTGGGTTTTCTCCTACTTCCGGGGATAAGCTCCGAA',\n",
       " 'TTTACAGAAAATTGTAAGTAACAGACTTCCGGGAATCATTATTCTTGCGTTGCTCGTGTAAGCTAGTTATTTCACAAATGTACGTACTGCTAGGGTACTGGTAAACTACAATGGCTTATTGGGTACCGAGCGAGGTAATCTTCGCGAGACTTAAAAGTTCAAAATGTTTGCGATCTCACCTCTACTTGAGGATAGATTCT',\n",
       " 'TTGTTGAAAGCGGATTAATCCTATAAAGACTACTTGCATTAAAATAGTATGACCTTTGACCCGAGGCTCCCAAGTCCGGGCTTGTAAATAGGTCAAAGGTCACGCACCACTGCGAGTCGATCGGCTTTTGTGTCCAATTTCATAGACCTTGCAAAAAAACTACCAGCTGTTATCCATTTCTGCCAGTGGTCCTTTATTTA',\n",
       " 'TTTTCGAAGTGTTACAGGTGTCACCCACAAATTACCGGGCACTTGTCATTTACATTGCTTACATTTGCGGAACTTCCGGGTTCGTTTTTTTTAAGATAAGCGACCTAAATTCTCAAACAATGGCTTCCGCAATAGGCATTTAAGACAACGTTTTCCCATAAATTGACAGGCTCACAATCAAAGCGAGATCAGTGGTCCAA',\n",
       " 'ATATGAATAGAGGCACGTGATGCCATTGCTATTACAACGAAGGTCAAAGTTCAAACAATCGAGAGGTCAAAGGTCACTCACATACGTGGATACGGTATTCCCCTATGACCTTTGACCTTTCTTAGTGAGCGGTAGTATCACATGCGTAGTACTAATAGCTAATACGTGTGGTCAAAGGATAGAAGGCTAAGCTGGTACTT',\n",
       " 'AAATGATAGACAGAGTGTTGGCCTTGATATTGTTCTCTTGTGCAAGACAATGCTGAGCCAGAAAGAATTGCCTAACGTTCTATGGCCCTTGCAAAGTTTGGTCGCGCTTGAATCAGAAAGGCTCAGGCGTATCTGCTAGTTCATACATTGTTCAGCGATTACCGTTGTTTTCGACTTCGAGGGCTCATGGCAGATTGAAC',\n",
       " 'TTCCTTGTCCAGACTTAGCACGTTTTGATATTTCCAATCCGAGCCCGAAGGATTGTTCGTTAACTATGCTTAACCTTTGACCTTTGTCTCTGCTTCACCTTTGACCCGCATACAGAAAAAAGTTTTAAGGTGTTCTGAGGTAAATACTCGCGACCGCGGACCTCGGGGTGACCTTTGACCTCTGTTATTATGTTCATTTG',\n",
       " 'CATCGTAGGGTCAAAGGTGAAGTACAAATTTATTCCTTCTTTTTTGTAACGGTAGGTCATACGTCCTAGAGCTAGGACTCCCAAAATTAACTTTCTGTGTAAGAATCTAGAATAAGGTGACCTTTGACCTCTTATGGTTGTAACTGACATAACCATGAGAACTATTGGTCAGTGCCTCGATAATAATATCATCCAACTTT',\n",
       " 'TATTAATAAACATACGCCAAAACCCGCCTGGGGTTATTATCAGACAAGAATACCGTTTCTGTTCAGTTGAGGCTTAGAGTAATTCTGGTAGCAGTAAATGAATTAAAAGACGTGTTCTATCCAAGCTCATGGTACAGGGTCAAAGGTCACCTTGTAAGTTTGATTTTTATTGATTTCTAGAGATTTTCTTTATTGTGCAA',\n",
       " 'TTTTAAGCTTTCCGTATAAATCAGAGTCTGGGTTATCATACTAGTGGTTCCAAAACAACTTGTTTGTTAATTTCTGCTAAATCACTGACCAACTTGTACTTGAAGATCATCTTTCAGATAAGGAGATAAAGTCTATCTTTCTACTATTATTGAATAGAAATTATAGCGCATCATTTAACTGTTTGCGTATACGATTTTCG',\n",
       " 'TTCGTTTATGGCGTCTTTCCCACTTTCGTCTCCCTATCGTCATGTGCTATATATTCCATCTCGTCGAATGTTGGGGCAGATAAATGAAATCTTACAGCAAGTATATTGCGTCGGGGATCACACAGTAACTATCTTGTGCCTTCTCCTCACAATTTAAAGGGAATAGCAAGTATATCGAGTTGGCGAGAGCTGATACTAGA',\n",
       " 'CGGTTTTCCCGGAAATCATAATAACTCACACAAGTCGATGCACTTCCGCTTGGAAGGTAGATCATCTAAGGCTAGAACGGTCAAAGAAAGACTTCCTGGTTCGGAGACTAAGAAGCTTGGGTGTACGAGTTTTTATCATTTAAGAATTAGAACTCTCATTGACAGCCCACATTAAAGAATGAAAAAAAGGCAGATGTATG',\n",
       " 'CGTGTTTTGATTGTGACGGTACTCACTCTGACGTTTATAATGTGTCCAAACGTCCTGATCGTCGAAAAACCCTTGGTTAAACTCATACTTTGAATGCTCTGGTCCCAACGTCTTAACCTTCTTCGCAACACACTGCCGTGCGTAAAATAACAGAAGTCGGTATCGATTTTTCCGGGGGGTATGCCGCCTTATCTCCAAGT',\n",
       " 'ACAATTAATTTTTTTGACGGCTTGAAATTAAGTAATCCTGAAGGTGCCTCCTAAGCCCCCTGCTTTTGAGTTGGCTTGTGCCCGCATTATGCGAAGTTTGAACCAACCGTATATCGCAGCATATCTTGATGAAGAAAGGCCAAACTCAGTCCCTCTCCGCTATAATTAATCATAAACCCGGAAGTAGTGTACCGTCAACG',\n",
       " 'GGACCATACCTGTATTATATGATTATGATAACAAGTTGTCTTATTTGTCGCTATCGGTTAAGGGCGGACGGACGAACTTCCGGGTTCCGAAAGGAATCGATAACTGTAGAGGGAATTCTGTCCCATGCCAATGAGCTGGCAGTAAAAGTTGGAATATGCGCAAACGCGGAAGTTTAGTTTTAGCTAATGCAAAGTGTGAA',\n",
       " 'CTTGTACTGCCGGCACTGTTTAGGTAGTATAATGTGCGTGCTTTTTCTGTACACTTTATAAAAGGGAAGTTAGGTTTGTCTCAGGGAATGTGGGGTCAAAGGTCACCTATTGGAGGGATTTTTTGGCGGTTATACTGCCTCGATATATACAATAAGGTCAAAGGTCACGATAAGCTTTGAATAAAGTACTCAAACACAGT',\n",
       " 'CTTCTAATAGCCAGGCCGAACTTAGCATGCAGAGCTTTGGCGAATTATTAGGGCACTTATCTGCACTTTTCGCAGTCTCCCATTAAGATTCAATATAAGCCTGGCTGATAAGGAGTAGTCATGTGTAGTTAGAACAATTACATAGGTTCGTGCCCCCCTGGCATCCTGCCTAAATTTGAATGACAGTGCCCTACTTCATC',\n",
       " 'GCCTGATATCTCATCGATATATCATGGTGCGCAGCTGTCACGAACTCCGTATGCGGCTGCCGTAGAAGGAATTACTATATTCGCCGACAGCATATTGGACGCATGGCCCATAAGTCGATTTCATCGGTCTTTCTTACCTCAAAAAGCGTTTACTAGGTGCAATTCCTAGAGGTCAAAGGTCATGAGTTTTGTCTATGTGA',\n",
       " 'ACAAAGTTTCAGTCTAACAGCAACTACCGTTTAATGATCCGGTAGATTATTTCAAACGCCACCAATGAAAGTAGTTAGGATCCTGCGAGTATGTGCCCGGTGCCGAGTGATAGTACAGAATTAGAGCTTTTTCGATCAAAATTAAAGAGAATATATATCGTTAAGTGTCCCTATCTCCTTTTTTTAAGATCGCAAGTCAG',\n",
       " 'TGAGTGAGACATGAACTCGGAATTAAAAAGAAATGGGGCCACCCGGAAGTATTTATACTTCTTGGTCATTCTGGGGAACCACTTCCGGGTGTGACATATATCAGAGTCGACATAGCTTGACGGCAGGAAAATTTGTACAGGAACACAGACAATGCATTGGCACAATTAAATTTCCGGCAGGAAAATTTGTAGAGGGCCTC',\n",
       " 'CGCTGTGGCGTTAGATACTATTTGTCAACGTCCTTCATAATGAACAGTTCAAAATTTACTTTTTATTAGGTAAATTTATAAACTCTCATACATGGGGTCAAAGGTCAATGATTCGGATTTTCTGTGATATACCCATGACCTTTGACCTGTTCTTCGAAGGGGTCGTGCATACAGAGAATGACCTTTGACCCCTGCGGCAG',\n",
       " 'ATCACGCGCACAACAAAATCTCTTATCTCGTACAAATATTAACGTACAAATTCGTAAAATCGACTTAATAAAGCGTATGGTATTAGCATACACACTATTCAACCATCAAGAAAAAGATCCGTAAGTAAAATTCATCTATAGCCGGTTATGCATTATCAAATTAGATACCAAGTGTTTTTTGTAATGTTGACTCAGGTTAG',\n",
       " 'CAATAATGCTAGAAGACCAAGTTTAAGCCCATGAACGCCTATAGCCAATTCTAGCTGACAAAATGATGCAATAATACTTGCATACTGAGCGTACAACTGAGATCCTATATCAATTGAGCCATCAATGGACCCGGAAGTCCTGCGCAAGAGACACTTCCGGGGGATGATTAATCCACGAGTACCATATTTATGGATAATCA',\n",
       " 'ATAATAATGATAGAAAGGTCAAAGGTCAACTTGATTTTTTTGGCAAAGTCATGCTTTTCTGGGGTCAAAGGTCAAATTGGGAGTTCCCTTTGCATCTGGCGCTTAGCAGTAAGTTTCTAATGTGCGAACAGTTAGAAATAATTCACCAGGAACCCAGTGTCTTTTTACACATCCATATTGAAATTGTTAAATTCCAGACT',\n",
       " 'CTGACCTCTGATCGTAGCGTTTGACTCCCGATACTAGATTAGGTTAACCTCCCTTCAAAAATATTTTACAATATACTTGTCAATCTCTGAAGACACATCGATGCCAGTCTTATCGGATAACCTTATGACAGATATATGTATGAACCCCCTAGTATACCACTATCTGTTTTTGCTGAATCTAATCTAATTTGATGGTTACT',\n",
       " 'CGCATCGATAAGATACCATAATAGGTTTACGATTGAGTAAAAGGAGATAGCGACGTTGCATTATTTATGGGCAGTAATCAACCTATCAAATCGCAGTTACAACCAGCCAAACGTATTTCCGAAGAGAGTTATCCTTTGATCAGATCAACCTCCAGTCACATGTTGAAAATTTAACAAGTGATAAGTAGACATTCAAGTAA',\n",
       " 'AAAATTCAACGTGATCGCCTATAGATATCATTACCGGCACCCGATTGTTGGCTCCGATGCACTTTAAATACAGCTCTCTAGCTTACGCTCAGTGTGTGGAGAGATAGGAACTGGGAACTTGAAATAATGAAACAGCCCGCATCCTATTATAGAACCTAATAGCGTTCTTCTAGACCTTACAGGAAACGGATCAAACAAGG',\n",
       " 'CCCTAGTACGTCCGCTTAACACAGATAGCTTATATATAAGTTTTCACACTGGAATCAGTATACAGATAGGGGGATATTACTTAAGTACACAAATAAGTGACTCTTAACAGAGGCACATGCAATAAGGCGTGGTGCACTGACGGCGCCTTGAATAAATGAGACGTTCAAAAAAGGATTATCTGCCACTTGGTATTGTCACA',\n",
       " 'ATTGTTGAATTTGGAGTACAAATAAAGAAATTGGGGTTATTTGAGATATCTACTCACGCGTCGCAAGTGCTATGTAACTTCTAAGAGTATCCCTTGGGGTCAAAGGTCATCTGTGATAAAACCTAAGATATAAAGACAAGTCCCTATCGGAAATCTGAGCTTCCTTGAAACCAATAAGTCAGCAGCTATACAGATATATT',\n",
       " 'TCTGCAATACATACATAATTCCACCTATTACCCGGAAATGCTATTTATCTCTCCATACGAGGGTCATACCATGAATTACATTAATCGATTCGATACAAAAATTGCTCTACGGTTTCTGGCTAGTCCCTTTTACAGGCGTAATCCTAATTACCTCGGCATCCGTAATGAGTATAAACGCTAAACTCAAAACCGGAAGTTCC',\n",
       " 'CTTCCCTTTTTACGAGCGAGATGGAAACAACATGGGCCTCTAGTTCGCCTACCAGAAAGCATTGGATATTACGTAATTAGATCATGACTCTCTAGTGTACATGCTTGAGCACTTCCGGGTTACGCATTTATATGAAGCCGTCCTACCGATCCAACACTACATACGGCTGATTTAAGACGACAAGGAATCCCGGAAGTACT',\n",
       " 'ACTATAACCCGATATGACAGTCACACGACTACGATGCGCGCAAGTAGGTCATCGCTACTGTTACTATGTTGTTAACCTGATCTCAACATATAATTTTCACATACATCTTTCCGTACTTGGCATTCAATGCACTAGCAAGTAATTACGTCATACTAGTTGATTACCATATCGTAGTCGATTGACTTCCGGGTGCCACTCAT',\n",
       " 'GAATTTTTTGTTTACCAACCCAAACGCATCAATGCTAACGTTTAATAACTTTTTTCGCAATATCTTTATGAACAGCTCAGTACACTTTGTAATCTACCGTTGAGTACAATTCTTGTCATATAATGAGACTGTTAGACTAGGGTCAAAGGTCATGCGGCCCTCCTCAACACTCATATTTCTTGCAGGCAGTTCAAACAAGT',\n",
       " 'GTTTTTCGTTAAGAATAAACTCCTCGACATCGGCTAGCGTATCTAAGAGTGCCAGCGCTTCCTTTTCACTTGATACAGTTGGTATCAAAATCAAATCTTTTCTATATGGCTGCAATTAGCTTGTAATACCCAACTATACGTTCCAGATCGCCTACCAACATAACCTGACTAAAGTTGAGCATCCCGGAAGTGCCCCCCAA',\n",
       " 'CGTTCGGATCATAGTCGCTGCTTATAACTATAGCATTGGAGCAACGGTTGCTCGCCGCGTTTGGCCTAGCACTAACTGATAATTACTCTATCTGATAGGGACTTAACGGAACTACATGCCCAAACCTTAGATATTACACATTTTCGAGTTGATACTGTTGTAAGAATTTTTATATTGAATGACAGTTTGTTTGCACTTTC',\n",
       " 'AAATGAACATATTTTGAGGGTCAAAGGTCAACAACACTATACCGGGTAGTTGTTCTAGAGTTTTAGTCCATTAAGTAATGTAATGCAACCTGTGACCTTTGACCCCTAGTTCGCTCCCCCATAAAGGGGAGTTAGACTTACTAAAATTTTCTGTTGGCGTGTTAGCCAGAAGCTAGACAATCAATCGTGACGATTTCTCC',\n",
       " 'CAGTCGGGCAGACCGCACAGATGATTCCATCAAATTGTAAAAGATAACCCGTTGAACGCCGACGCATCATTAGATTAAGTAATAATCTATAGTTAATCCGTAAAGAAAATGCTGACAGATCGCAAGAATTTTACCTAATGTTTTCTCACAATCAACGGACGATCTCCTTATCAGGTATAACTAAGTACAGATAAAAACAT',\n",
       " 'AAGAGACTAAAGACGAGATAAAGACAGCGTGGCCCTCTAAACGAAGCAGATAGAGGGGTGCGGAAATTCGTGGTAGCGTTTGAAAAGGGCCACAATCTGACTCCCAGCTATCTGAAGAATATTTTTAATTGATTCGCATCTTCAGACAGGTAAATAAACTAGCGTTATCAGTAGAATAGTGGAGAGTAAAGGTCGGTGGC',\n",
       " 'GCTCTTTAGTATTAAAATGTTCTATCTGATATTACGAATTCGGAACCACTTGCAAAGTTTATTAATGTGTAAGTGTTAAATGTATGATTTGGGAGTGGGTGTAATTATTTGGGTACGAGAACTAGACTTTTAACCATCATTAACTAACTGCATGTCACCTGTCAATAACATAGTAGGGGAATAGCGTCGTGCGGAAAAAA',\n",
       " 'TTAAATTGGATTCGTTTAAAGCTCTGGAGCTGAGATCGTCAACACTTCCGGTCGGTATATTTAAATCGATACCCGGAAGTCAGCGTTCTGGAATCCTACTAACCTATCTCACACGCGGTGTTGATAGGACGTCGATGACTTCCGGGTCTAAATCTGTATTCGTTTTTTTTAAGCAAGAAGGTTCTCACAGACTCATAGCT',\n",
       " 'AACCCAAGTTGACGGTCTAGACAAGTCAGTAAGTGTTCGCACAGGACACAATCACTTGAATAAAGTTACAGTAACAATATATGTTTTGTACTAATACGAGATAATGGCGAACACAGGACAGTGACATCGCGACAAGAAGGTGTATATCTCAGCAATGGCTATTACTTTCTCTGTACGTCATATGCTTGGAACGAACAGTA',\n",
       " 'ACTCACGGATCCTTTGCGAAAGTTGACCTTTGACCCTTCGTCCACATTCCAGGCGAGTATCGTGTGTTGTGCGGACAAGCGAGACGAGGGTCAAAGGTCATATGCTTTGTGACCTTTGACCTTTAACGAACTAGGCTTATGTAGGAAAAAGAGTTGTCTATAACAGACACTATTAAACTTATCACCGATGTCGACTCAAT',\n",
       " 'GATTTTTGACACCACGAATAGCGGCCTGAATTTTTTGATACTATCGTAGGGAAAGGGGTCAAAGGTCATGGACTACTTATGACCGAGCGAAACGGGTGTGTCGTCTAGGGGCTCTCCATCTTGACGGATTCATTCTTTACAGAATTGTCATGTATGGGGATTATAGTTAAGCATATGCTGAATATTTATGCTTTGGTCCT',\n",
       " 'TTGATATAGGAATTAGCCGTTGAAGTATGCCGGCATGTACGCTTACACATCCTATTATAATCTGATAAGGAGACTATATCCTAGTGAGTCGGGCATTGGAAACCACCCCCGCGAGAGGGAAATATGTCTGGATAGGTGCTCGAATGATCTCTTCTTATGGACACACTAATAGTATACTTTTCTAATCCAAAGGTTCTTCA',\n",
       " 'TGTAGTATCCGCCTGATACTATCATACAACTCTCACTCAAACACAGCTCGAGCCCCTGCTTAATAGTTTTCATTACCATCAATCTCAATTATCAGTCTCACTATCTACGTTCATTTCTTCTCGTCTCAGGTATATTTCTAGTTGCGGAGAATAAACAACCAACGAACAGTGGCAATGGCCGCTATCAGGCGAAGTTAGTC',\n",
       " 'TGAATGGCCATTCGCATGTTATATCTCGATCGTGTTGCTTGGGGTCAAAGGTCAAAGTGTGGCATTTAATACACACTTACTCGGATAGCTTGTTATGCTAATGAAAAAAGGCGCTGTGATGTCCTATGACCTTTGACCTATACCATCCGACACACGTATGGTTAAGTCTAGCCGCCATGTCCGAGCTAGGCGCCTAATTT',\n",
       " 'TATGGCGCAATTGTGAAATAGGCGAGCCAATTTTACGTAGTACCCCCCTAAAGCTTTACTTCTATGCAAGGTGAACGTGAAGCTACCCGCCCAATGCGGTTAACCTCCCCCGACCAAGGATCAAACGTTCTCCTGTTGAGCAATTCCTAAAATTATTACCGATATCATATATGGTCGGCACGATGACCTTTGACCCTTGT',\n",
       " 'TCATAAAGTCTCTTGCCATGAGAGTTTTGGTACGTCCGTTCTAAGAAGGTTATTTCTTGTAACTGCAAGTAACCCGTGACCCGACCAGACAGACAGATAGAGTAGTGGTATGGGGTACCACACTTGTGTATATGGATAAAGCAGGGTAAGAGGGGTCAAAGTTCACCTGACAGGTCTTGAATGTTGTCTCGTAAGCAATG',\n",
       " 'GGTATCCTTTTCTTCAAGCGTCAATGCCCGTCGGTAAACCGCACGGCAGCTATTAACTACCGTCGTGAGCTTGTACAAGGTAAGATCTCTCATATGGATTCGAGCCCTTATCTGATAGAGATTGTTAGTAATTAACAGAATGGTAAACGTTATCTGAATTTTGAATGCTTAGATTATAAAGTCCGATGAAGTAATGTTTA',\n",
       " 'ATTAATATGTCTAATCGTGGTTGGAATGGTGTGGGGGAACGGAGAATAGTGATGATAGAATCAAACCCCTTCTTTAGACCAGTGGACTACTACATGCCAAAGGTTTATTGCTTGCGATATCTTATTTGAGAGAAGCCCTTCCCCTTTAGTGACTAGAAGGAGATCACAATATAACCATATCTGTTGTAGATGTCACGCAA',\n",
       " 'GTATTCAATTAAGTGGAACCCGGAAGTACTTTCAGTTACTTCCGGGGTTACGATATGTCTTTATCTTTGTTTATGGGTCAGGACTATAGGCCAATGAGAATAAAGTGAAATTCTAGAGAAAGGATCACTATCCAGTTGCAAAATCATAGTATCTATTATTTCATTGATGAGAAAGTAGTACCACCTGAATTTCGTAATCG',\n",
       " 'AAATAACTTCCGGTTTTCTTCGTTATACTCCTCCCCGAATTTGAACACGACAGGATCTAAGACTTATCGTAAGCATAGATTTCCGGATCAAGTGATAACCTCACACATCATTGTGGTTGATAAAGAGTATAGCAACCCGGAAGTATAGACATAAGCAGTGGGAATAGCTCAGTCGAATGTATACAGATCAAAAATGGCGT',\n",
       " 'AAACTAGATTACTTCCTGGAAAAAATTCATACCAGGATCCCATTCTTGAAGAGGGGTCAAAGGTCAGCGGCTGGCAAAAAGTATGGCAATGTGTCTCTCTCCAAGATGAGGCGTAGCAAGGATTCATTTAGAAACAGAGTTTGGATGGTATTTGAACTGGCAATCCTTTAGTCTGTATCATAAATAAGTCATTTCATCAC',\n",
       " 'TGGTAAGCGTCCAGTACTTCCCAGGACAGAACCCGGAAGTAGCTTGAGCACCATACTTGACCTTCAAGCTATGGGCCTTTCAATTAGGGAGACAAATGTAAAAAACTAGAATGAGTACTTCCGGGTGAAATTTAGCGTTCCATTGTCACTGAGAGCAGATATCCATATGAATCGTATGACTTCCGGGATTATCAAACATA',\n",
       " 'ATTCATAACACAGCTCAACCAACATAATGATAGCGTACGTGACTTATATTTGCAGCCAGTTGACGTTTCGCTTAGAAAGGTTTTAGGTATCCATAATTAACAGGTTACCAGGACTCTAGTGACTCATAGCTGCGTGACGGTAAGATGACCTTTGACCCTTCGGACTCAGTCGGTAGATTACCAGTCCCTGTTTTTTCGTC',\n",
       " 'CCTTAGAAGCACCTTAGAAGTGGTATCTGGTACATCTGTTCTGATGAAGGATTGTGCATTTTAAATCACTCCGAGAAAGTACGTTCTAGGTGTCTTATCAGCTCTTTTAATCTAGGACTTATCTCAAACGCTAACTCAACAATCGGAAACAAAACTGCATAGTGGATCAGACGATCATCTAAGACATTATAGGAAAGAGA',\n",
       " 'AAATGGCCTAAGGCTCTAAGCGATGGCATTCCAACGGGATACATTCACCTTATGAATCGGTGTAAGAGCATATGACACGTTATTTCATCTTAGAGCGGTCCTACATTTAGTTGATTATTACGCTTTCTTAATTGGATGAGCAGAGGTCAAGGGTCGAGCGTTCATCTTAGCTATGTGCTGCTGAGGAATTGAAATCCAAT',\n",
       " 'AACTCACAGGTTAGACGTCATGGATTATAAGCCGATTGTTGATTTACCCCTTAGGTATAATTCTTCTTTCAGCTAGTTTACGATCAGATAATCATCGCTCTGGCTTATCTGCTATAGGGAATACAGCGTATCGGATGAACTGACAGACATGTTTCCAAAAACTTGTTATTTAATGCAACCTGACCCACCGCAACGGGGCT',\n",
       " 'CATATTCTACCTTCTGTGATTAAAAACACACCAATTGGGTGTCACATCTATCGAATTGAGTGCAAGAGCATCACAAAGTCCCCATCGTAAAATCGAATTAGACAATCGGTGACCTTTGACCTCATCATTTTTCAATCAACAGTTGACCTTTGACCCCAAGCTAAGGGGTCAAGGGTGATGCACATGATCCAGCGAGTTAT',\n",
       " 'TCAAGATAGCTGTACTAACGCTAGTAACTTCTTGTATGAGTAATTACATAAATGTGCTTTTTGAATTATTTGCCACTGCGGCTACCTTGTGATCTATGTGACATTTAAAAATATTTCTGATAAGTTCAGATCAGGATTGGGATCTATCTATCTTTCAGTATCCAGAAAACCGGTTCTGACATCCTGCATACTAGCCCCAA',\n",
       " 'CGAAAAAGTCGAAGTCATGACGAAGTTTTAAAATGGACTTCAGCACGAGGATGTTGCAAAGGAAGGCTTCGTTCAACAAGTTCGGTCCCATGACCTTTGACCCCTATTATACTTGTTGTGAGAGGTCAATTTCATATATTAACACAATTGTAACCAGGTCTAACGAGTACGGGCTGAAGGTGAAAGGTCACGAAAACCGT',\n",
       " 'AAAAAACCACAGAGCATACTCGTGATGTATTACAACAAAAAAAATCTATAACAGATAAATAGCGGATATGTCGTATTCAGCATCATTGGCATTCTGACATGAGAAGTCACGGACGTCATTAAATGATTGTCGAAACGGTTAGCCGAAGAAGTTATTTTGTCTTGTCAATCGGTAAAAATGTCAAATAACATGTCTCCTTA',\n",
       " 'AGGAGAAATGTACAACACACGTAGCTTTCCCTATGACCTTTGACCCTTGGGCCATGCATTATGGTGTCATGCTCTGTTTGACCTTTGACCTTTGATAGGCTTAATACTTCGGAAGTCGTATTAATCTAACCGTTTAATAACCCGGGTTGATGTAGCAGTGGACTGCACTCGGTGACCTTTGACCCATTATGATCTGACCA',\n",
       " 'AAAAAAGGCGTAGACCTGCTTTTACCATCATCCGACGATCTAAGTTGGATCTCCCATGACCTTTGACCTAAGGAACACATGGGGTCAAAGGTCATACGTTGGTGTTAAAGCATAAGGCCTCGATAGAAGACTGGTAGGATCATGAACACGTTGTATTTTTAGGTTTATCTAGGCAGCTTGACCTTTGACCCTCTGAGCAG',\n",
       " 'TACAAGTTAATTGGTCTCTGGGCCCGAATGCTGACTGGTGGAGTAACCCGTAGCAACACAATGCATATCAAACAGAATAATATTAGAGACCTTTCGAGCTATCTGCCCACAAGTCGTGATTAATGGAATATCAGATATGGATTTACTACTTACTTCATTCCAGGGGGCATCCCGGTACACTTTGGTAAATACGCTAATAT',\n",
       " 'CCATAGGCAAAATTGTCGAACCCGGAAGATACAATATAGTAGGTGTACAGACCTTCCGATAATCATAGCATATTGTATTCTTAAATATCAGAAAGAGTAACTTTCAGGATACAATTTGTTATAACCAGATCTAAGACAGCCGGAAGTAAGGTGCGTATCATGGTTCTTTACATACATTGGGCCCTGGAATGGTGCAAATG',\n",
       " 'AAAGACTAAATGGAGAATCCGTTAATGGAATCCCACTGTTATTTAGGAACCCGGAAGTGACTAACCCCGGAAGTGTTTACTATAATTAATGGCAAGTGGATCGACAAATAGGGAGAAGTTAAGTTTTAGCTGTAGTTCTCCTTAGAAACCTCAATAATTGGGATAACTCGACATTTCAGAAAACCGCAGGGTTTCGAGAA',\n",
       " 'GCATGATGTTGAGATCAATATTAAGATTGTATTCTTCGGTAGTAAGTGTTTGCACAGAGGAACGTGCTGTGGAACAAATTACATGGAATACCGTTATTGGAGTTAGTTCGTCTGGAGGTCAAAGGTCACCGCCGTTATATGCGCCAAGCCAAGAATTTCGAAGCACGGTTGACCTTTGACCTCTACCAGATTTTACGTGA',\n",
       " 'TTGTGGGGGGTCCGAAGTTCTCTCCGTGGTAAATAGTATATACCGCTGTCGAAATAACAGTCCTTGACCTTTGACCTCGTTTGTCTTCGTAAGTAACTTCGGGTTTACTTGTATAGGCCAATATTAAATACTAGAAGCTGCGTCGTGTAAGTTCCGCATGTATCATAGTAAACACTAGAACTTTTGGTTCCGATTCACAA',\n",
       " 'TGACTTCCGGGTACAAACTGAGCACATTGGCAAGCGCTAGCCGTACAGCCGTCAGCTTTGAAAGGGATTTGAGGGGTGTACGGACTAGCGTTGACCCGGAAGTGAACGACAGTTTATGCACCGTGTAATGTTATTAAAGAGAATCTGTTAATCATCCTGGTAGTTCTACCCTCTTTTGGTAACTTATCCATCTTATCGCC',\n",
       " 'GAATATTCTATAAAGACCTTCTGCTTATAGAATGATGTTGGCGAGGGTTCAAAGGTCAAGCACACAAGTTGATTTGAATTAGAGACAAGACCGACGAGCACCTTGCCAAATGACAATAAAGGTCAAAGGTCAAACTATATCCGAAAAGAGTTTCTTATTAATCTGAAATCAATCCAAGGTAGATGCATTAGTCACAAGAA',\n",
       " 'AAGTCCGGCTGTTGGAATAGTTCTCATAATACTAGTATGCAGATAAGGGGCGTTATGTTTTCTTATCGATGTAATAGTAGCGCGGTGCTCCAACGATGGTATTTTAGTGTGGGGAGTCAGTGACCTTAGGGGTCCACGACGAAACTGGATTGTACCGTTTTTTAATTGAGAGTCGATCTCATACGGGAAGATCTCAGCAG',\n",
       " 'CGTGCCGGATGCACCAGATAGGGGCTGCTATAGGCATAGTTGAACATCTATAAGTTTTTTTTCGCTTTGTCGTTATAAGTCGGATCGGATTGACTTGGCGTAAACGTTCCTAGCCCTCTGCGGAGAATAATGCTTATCTGATGTTTTCAGTGATGCTAAAAGAACTGTTGTACACGCACATCGATATTCAGATATAGACG',\n",
       " 'AGGTCTATAACAGCCTATACTTCCGGGTTTTGATATTTTCTAAACGCGATAGCAGGTGTACCCTTTTCAAAACGTGAGATCTGATGAAAGCTAGCGTCCCGTTCGTCACTCCCTTGGACTTAGTTAACCACCTCAGATACAAATTTTGACACTCATGCACAACCCGGAAGTGGAACCCGGAAGTAAAGCTTTTTCCTGAG',\n",
       " 'ATCCCTAAACTTCTGGCTGCGAGGTATTTCAGCTAATCTAAAGAATCGTTGCAGGGTCAAAGGTCAAAAGACTTGCCCAGGTAGGAGGACTGAAGCCTCTTGTTGGACACCGGACTCATAATACGAGATTTGACGTTTGCTTATTTTAGAGTCCTTGCCCATTACTCATTTCTCCTGAATGTACTCTACGTTGTAGGTTT',\n",
       " 'GATCTGGAATCTAACAACCCGGAAATGACAGTAAAGCTTATGAACACTAAACCCGGAAGTATTATATCTTAGAGTACGGAACTCGCTTAACTCTTAGGCAAGTTATAATTTATTTAGGTTGGTACGTTCAACCGCGGAAGCCCAACAAGTCGCAAAAGATCATGTAATTTATCCTACAGGAAATTAGGCAGCAAGCGTCA',\n",
       " 'AAGTATATATAGATCATGAAAATCAGATCGCGCGCGAGTATACGTATACTTTGACACACAGTACCCTGTTTTATGATGCTATTAAGATGTGCTACTTTAAAGCTATGAAAGTGTAGTGTATATATACTCTGTCGAATTTTACAGCTTGACCTTTGACCTCTCAATCTGTGCAAATAACCGTTCAAATGAAAAGTATCGTT',\n",
       " 'AAATACGATAACATTTTCAATGTAGGTCGATCCAACTTACGGACAACATTCAGAGATCATGAATAGCTTCCTTATCTGATACTCAGAAAGATCGGGACCCTTAAGGTAGATAGGGAGAGCCAAAAAAAGTGCAGCACGATCTCTCGCAGCAATTTGTTTATAGAGTATCTAACGTACGTTTGCGTTATGATTTAATCATC',\n",
       " 'CGCTCTGTTTTCCGAACGCATCAATATAGGCGATCTTAGTCTGGTAGACTATAACTATTCGTTTGGTGTCTGAAAAATACCATCGAAAAATACTGCTCCCCTATGACCTTTGACCCCGATCCCCATCGAGGTTGTAGTCACAAGCAGTGTCCTTAACACTCACGTAGGGTCAAAGGTCATCCAACCTTACAGCTTCTTTG',\n",
       " 'AGTATCCTGCAATCAAATTTTTTCTTTCCAGGTACGTTTCAGCCCACAACCTTCCATGACGAAGTAAAAGCGAGAGCCGGTTGACCTTTGACCTCCGTGCAGAAATTGAAAGACTCGATATCTTCAATTGATCTATTGCCGGTCCTTGACCTTTGACCCCAACGAAAGGAAAGAAGAGCAAGTATGGCTAAGAATTAGTT',\n",
       " 'AAGTGACTGATGTACCCGACTATATTTACTTATCATGCTCTTCTCAAGGCGGAAAACGACTTACTTCTCTGGCTCACCGGTCAGACTTCCGATACCGTGAACTTCATTATGGAATATATGCTGCTTACTCTTTAACTAGTATGATATACCAATCAACGCCTGCATTCCTGTGCGCCATTGACAAGCTTAAAGGTGAGAAC',\n",
       " 'GAAATATTCCTGAACGGTTCTTATCTCGCAGGTGTGCTGGCGTCTACCATGCGTATCAGTTCAGCGAAACGTGATATGTTATTGTGGGAGTGATTCGATTCTGCCAGATAATGGGGTAACTGATACAACGTTTTAACCGTAATTTGCTGACTTTCCTATTGTACACTTGTAGAATCCGAGAGGTAGAGACTCTCTCTATT',\n",
       " 'AGTGAGGGCTACTGTCGTTTTTACTTCATTTTCCACACCAACTTCCGGGTATTACTTGAATGGGACTATTTACTATTGGGTTGCTACTGTCACACAGGTGAACCGCTTTGACTCGACTCATTTTCATTAATTATAGCCCCGCAAACAGGTCCTTTGATCGAAGATACTTCCGGTGTGTGCCCGACTTGTATAATAAAACC',\n",
       " 'ATAGTCAGTGTAGAGACTAGAACTTTCTCCGCTAGTCAGATAACGAGAATACCTTTGAAGTAAAGACAATAGTTTAATCGTTTGAACTTAAATCCCTTATCACGCTTACCTTATTCGAATTGACCAAATGTGATGAAGATCGTAAATCGATGTTCTACGTTCTCCAGATAGAATCACGATAGCTTCCGATGAGGTGGAAA',\n",
       " 'CGTAAAAGATCACGCCGCATGTCAAAGTTGAGGTAGCTTCTTCAGAAAGAACCACGTGTAGGGGTCAAAGGTGAAGCACATAAAGTCTAGTCAGCTCATATGAATCATAGCATTATTATGGTAAAATAAGGGACGGATCTACCTGATCTATGAAAGTTGAATAGGCTCAAAAAGATATACGATTTCCTACTCCAGAGGGC',\n",
       " 'AGGAAATTATGCAACGAGATTTAATTCGATGTCCATCACTATAACCTCTGTACGGATCAACCCCCTATTAGATGGTTGTTTTACGGCAACGTGACATAAGAGAATAATAATCCCACCGTGACATCTCCGATCATATACATACTAATTGACAAAAGAAGGCACGCCGTATATTAGGATTGGGGTCAAAGGTCACGCGAGTT',\n",
       " 'AGTGCATGGGGAGATTCTGAGCGTCCTAACTTCTGTTGTACCAAAGAGTTTTCTACGAGTGGAGTATGGAGTGTCACTTCCGGGTTACGCGTTGTAAGATTGAAAGTACACATGCGTATGAAATTTATCTCCATGCACCCCCCGTCCTTTCGTGTAAGCTATTAGCACTTGTATCTGGAGGTGATGTACGCAGCTCAGGA',\n",
       " 'TGTTCTAGGGACATTATCAGGAAGAAAAAGTCGGGAATCTGTTTGCACTTATCTGCCTATTGAAATTTAGGGCATGAATACATGTAGTAGACGGCATAGGAAGGTGTTTCGTAAATTAGTTTATAAAATGCACGTGATATATAACTATTACTATGGACTCCCACAGTAGGCTCATTATAGCTCAAAAAGAGACATGATTA',\n",
       " 'AAAATACTCATTCTCTTTATCCGCGGCGTTGCCCGAAGATACGTTGCTAGCTATCTGCCTAAATCACGACTATAAGAGCAATAACACTGGGTATGCTTTTCGGACATACCTCTTGCTTATTCAGCACATGCACAGCTGATAAGGCGCTATAAGAACAGAAAGATTACGTTATCATTATCAGATTTTTGCTTTATGCTCTC',\n",
       " 'GCAGCAAACTGGATGAAACAGGGGATGAGTTAGCAAAATGGGATTATCTGTCTATCTCTGAACCCACACATAGAGATATGTATCTAGAACATTCATCAGACAAGAGAGTACATCTAACAAGCATTTTGTCACATGCCGGTGGAATGCACGTACCCCCTATCAGTTATTTTTCTTGACCCAACTCACAACGCGAAGCCCTC',\n",
       " 'AATTCATAGAAGGCAAATGTATGACCTTTGACCTTTTTATGCACGAGAGACCTCACTTTTGAGCGTACATCTAAACATATGTAGGTACAGACGCTTGATACCATGAGCAGTTGACCTTTGACCTCATAATACATCTCAAGAGTGACCTTTGAACTCTGAAAATGCGATCTAGTTGTTATATCCTGTTAAAAACTTTGTAG',\n",
       " 'GCGAGCCCTGAAAATAGGATTAAAATTTCCGGGTTCATTTATGAAATCTAAAACAAATGACCCTGATGTAACATGTAGATTGCCTCTCTCTGTGTAATGAAGACGCTAGAACCCAAGCATTAATAGATCTGGGCATCACTTCCGGGTTTTGTCATGCTTCAAATATCCATATTTTGTTGAACCCGGAAGTTCTCTAAATA',\n",
       " 'TCCAACCGTATATGGCGTCCTCTGGCCTGTAAGGCTGGAAGGAGCCGGCGCTCCTGACTCTATCCCTCGAACCTAGTTAAAAGAAGAACCCGGAAGCGCGTCACCACTCTCAGACCCTATATAAGCCTGGAGGATAAAACCGTAAAGGTTTTAGTATAGACGATATACCTGCGGCACTTCCGGGTTTGCAAAACCGCAAA',\n",
       " 'ACTTGTGCATTTAAACCTAAGCCCAAACACTTAAATACTACAACACAATAACATTCTCTATCAGGACGCTTTATCAGTGACGAGCGTGATAAGCGTCTAGCCTGGAATTTTCATCTGGAAAAGCCGCTTATCTGACAAAATAAATATTTGAAAGATCTAGCTCATATTAGAAGAGAAGAACAAGATCTTCCCTGATATTC',\n",
       " 'ATACGGAGGGTTAGCCTGAATAGTTAAAGTTACAATTAAATTGACTGATAGGTCTATTAAAATCTAGTTAGAAGTCATTATTTCAAACCAATTGTATAATTATCCCACCAGTGCTATCACGCATGAACGCATAAATTATCTTGAAGAAATGCCTCGAAAGTTAATGTCTCTCGTCTTACTTGCCGCGCTATCACGTACTT',\n",
       " 'GATTATAAACCGGGTCAGTTATCGGGGTAACATAAATATTTGATACCACGGAATATTCTAATACAGGGTCGAGGATGAGATAGTTATCGAGTGCAGTTGCCCGATTAAAAGTGATTAAGGGTCATGTCGGAAGTCTATCATGTTGTTTTTACCCGTACCAGGGTCATTGATTCATAAAAAGATAAATCATTAAGTATACT',\n",
       " 'CTTCGGAAATAAACATTAGGATTAAAGTGTGGAAGACTAAAGAAATCATGTCACCGCCATGCACCGACTTCCAATCAGGTGTTCTATGCCTTTTACTCATGCGAACATGATGTCAAGACCATACGTCCCCTAAGAAGGTTATAAAGGTCAAAGGTCAAGTGATCATATAGAATACGAACCACTCTATGTAGATGTGGTTA',\n",
       " 'GTACTATATGCAAAATAGTCTCATGTTTATTTCACCCTCCATCTGGTAGTCGGGTAAGTAGAAAAAGATCTCATCACTTCCGGGTTGAACATCGAATTCAGCGCGGAAATGGATCAAATATAGATGTACTCACTTCCGGGAACAGGGCTGATAGAACGTGTGCTGAACTATCAAAAGTCAGTCTAAGACGCGGAAGTTAA',\n",
       " 'CTGGACTCTGTTCTCCCTAAAGTAACGTTAGCATAGGGCTGAACCGGCTGTGCCTGTTCAGTAACATTCCATAGAACCCGGAAGTCATCCTATAATTGCTAATTATAAACGGTACCCTCAAATTCGGGGATGAGGAGCAGTACCGGCGCCTTGGTTGCATGAGTTATCCACATCAATCCATAATATTCACCGTAATTTCA',\n",
       " 'ATACCAAGGGCGTTGACTCGCAGAGGTCAAAGGTCATGGATAGTCGAATAACGTGAAAGACGCATCGTTCTGTTTTTTTTACGCGATATTAATTTCTTCCGAGTCATATGAACCTATTAATTCAAACATCGAGCTGTGACCTGACCATCCAAAATGGAGAGGTCAAAGGTCATCTATACCAATAAAACCCCGAGAAATCT',\n",
       " 'AAAGAATGCGTCTGAATCCAGGTTCAATTGTCACCTAAGTTGGGGCGCGTACGTGATTAGCCTATCTTAACGTTTATCTCCCATGAAACAAACTTACTTCCGGAATTTCCGGTTTTTAATAGCGATTTTCGCCGTTAAGTATTCATAGAAATGATGTAAACCCGGAAATGCACTGACTCCTCACCTTTCGAATGTAAGCC',\n",
       " 'TAGCTTAAGGTCATAATTGAAACTAGGGGTATGTCCTCAGAGCTGCCAGGTATCTGGGGACTTTATCGCATTTTGTCCAGGATATCGTAAGCAATTCCCATCGAAATGTGCATCATTAACAATCACAGGTAATAGCTCGAAGAGAGTTGATTAAATTGTACTTCGACTTCCGGGTTAATACTGCCACCATGTAAACATAA',\n",
       " 'GCCAAATAACCACTTCCGGCACTGAAAAATAAGTCCTCAGTTGTGCTTTTGGATGAAATGACATCCTGAGTACAGCCATTTTTATTACTACCTGCATGAACCCGGAAGTCACGCCTACTACGGGCTTGTTTACAGTGCCTCATATAGAACTTCCGGTTATTTATTAAATAATAAAATCCTACTCTGGATATCGCCGAGGA',\n",
       " 'TGTAGACTAGCCTATATTAATATACTGGGGATATAAGATCGAGTCATCAGAGATCTCAGAGTGATCGAGAGGTCAAAGGTTATAATTGTCGTTACAATTAAATGCCTGACATGCACCTCTTGTCAATCCGAAGTAGGAATATCAGCTAACAGTTTACTGGCGGTAGATCCCGCAAGTTAAGTAGGAACAACTAGAGTTAC',\n",
       " 'TGCTAAGTTGTGAACAGCAGTTATCTTCTAGTTGACGTTAACGATCAGACTACGCATTGGAGAGGGTTTCTTATCTGATCCAGGTAACGTTCAGACGCCGTCATGTATGATAAGTGGATAAGGGGAACACCGTATATGCTAGTTTGTTATCGAAGCAACAAACAATAAATGCATTAAGATTTCCTTCGTTCAAATGGGTT',\n",
       " 'TCCAGATAAAGAGTGTAATCATAAATGACACGGGAATTACAGATAATGCTCATATTGATTGACGTAAGGCTTGATTTAGTAGTCTTTTGGCATATACAGGCCTTTACGGATGATTAATTATATGGAAAGTCCAGGGCTTCAACATTTTATTTAAAATTAGAGGTTTATACTATGGTTAAGACAAAGTCGTTGGTGCTAGG',\n",
       " 'GATAAGGATTCTACCGACCCTTCTTGTGGGAATCGCGTGACCTTTGACCTTTCACAACGAACCCCGCTCCGAAGTCTGAGTCTATAATCTACGGGTGACCTTTGACCCCTAGCCCATGGGGTCAAAGGTCATAATGTATTAGTGATATCGTGGCCTGTTCTCATCCATTGCCTAACAGAATAGCTCATAGCTCAAGTCGT',\n",
       " 'GCTCCTCAAGCTTAAACTCCTGAACGAGACACCACCTCATAAAAAAACGATATCGTAGGACAATTAAACAGCAACTGGTGACGAATCCTAATGACCTTTGACCCCCGATAAGGAAGACTGTTATATAGAGGTCAAAGGTCAAGGTAAAAAGGAACTCTCAAGTTGACCTTTGACCTCCATCACATGGGACAGTACGTAGA',\n",
       " 'CTTCAATCATATCAAACTATTACTGAACTGCAAGGAATACTGCTTAGGGAGTTAATATTACGAAGGATGGTAATAAGCCGGCGTATACTTTGACCTTTGACCCCAACCAAGCACATACTGAGTTTTGTAGCATCTAATGGGTAATATAACAATAAGCCTTATTAACCTCCGCCTATGTGGGCACAGATCATATTTCAATA',\n",
       " 'CCACGTGTCGGCGTGAGATTCTAGTACTAGTTTTGCTTATCCAGCCCCTGAGGTCAAAGGTCACACTGGTGACAGGAAAATCATACTAGGCTTAAAGCTTCATCCAAACGTTTCCAACGGTGCATGGATAATTGTCTACAGATATCTCGTAAGGGGGACAATGACCTTTGACCCCAAATGGATGTTTTACCCTCTTCCGA',\n",
       " 'ATTTCCACGATGAGAGCAACATACTAAGGTCGCGTTAATGAATATGAGACGATTGACACTTCCGGCTTACGTACTTAATCCATAAATTCTTTCATACCCGGAAGTGGACCCGGTTGAACTACTGAACGCGTCTATCGACGACTATGAAAAGATGTTAATTTGAGATCGACTTAATAAACCTTTACAAGCTACCACATACA',\n",
       " 'ATAACTATATCGAATAAATGGGGTAAACTCAGTTTAGGGGTCAAAGGTCAAAGATTATCGCACAGTTATTGCACGTGAAGTAAAGCCTTCTTTTGAACCCTGTCCCGGGAGGGTGGTGAGAGTTAACAGGTTCACGTCCTGGGGTCAAAGTTCACATATTGCCACCAGCAGATTACAATTTAAACTAGTACTCGAATATC',\n",
       " 'ACCTCCTCTCTTATCGAGCCCATATGTAGATTCAATCACGCTCGGAGATCAGGCCCGCATGCCTAGACCAAAGCAGGAATCTAGCTGTCGCACTTCCGGATCTACCCGTTATGGACAGGCTGCTATGTTCTGATATAGCCAGGCTACTTAAAATTTAGCTAGAATAAGAACAATTGTGCTTAGATCGACGATTGCTCATG',\n",
       " 'GTGGCTACACTGAGTCACTTTAAGCTATACTAGCTACAAGTGCCAAAATCTCTGCATTTCCAGCTATCCGTTCTAAAATAATCACTCCTTATCAGTTAGAAGGTTAGCATCTGAGACCGCCCTACAGCCTGTACAGTGCACCGAACTATCTCGACACACTGGACGAATTGGGTATAGCTTTGGCACGCCGACTTTTGAAT',\n",
       " 'GGTAGATCCTCTATTCGTATGATCACCTTTGACCTCACGCATGACGTAGTACATGTAGAGTACCGCGTCGCATATCCAGACATTACGATAGGAAATTGCAAACACTATTACTGAACCGGATGCACCTCAGGGTAATACCCTATTTCAAATATTAAGAAATTGGGGGTCAAAGGTCAAACGTGTATGAAATACTCAATACC',\n",
       " 'CTCTCACCTAAGAAGACTTAAGCAGTCAAACCTAACCCAAGTGCGAAAAGGATGCATCTTTGGTATATTCATGCCCTATTTTTAGTCCATCTCACGGCGGTTGTACAATAATGTAATGATTAAGTCAAAATATTACGGGACTTCTCAGCAGTTTTCTAATCATAAAATGCTTTAGGTGACCTTTGACCCGCTGGATTGGA',\n",
       " 'TGATTACTAAAGAGCACCATAATCGGATGACCTTTGACCCTTTGGCTGACCGGCGACTTGGATCTCTGTTGCGGTCTCCGTTTTTGAATCTCTGAGTACATATCTGGCACAGATGACCTTTGACCCCTCAGTACTTATGCTGTGAAAGAGTCGTCATAGACTTGTCGTGACTAACCATAATTAAATTTACCAAGCAGCCG',\n",
       " 'CGCAAACTAGACCTGACATGTGATAACAATTGTTCTATAGGGCAAGAAATGTCAAGGCACTACGGGTGCATTTATTTAGATAGAAATGCGAGTAACTGTGATCATTGATATTTGATATCGTTCTTAGGTGGCCCATTGACCTTTGACCTCTGCATACCGATAACAAACTAGATGACCTTTGACCCCAATAACACTCTGAT',\n",
       " 'TCTTTCCCACATAAAGAACACCGCAACCGATGGACTCTCTTACTTATCTGAAGAGCGTCCAACAATTATCATGTGAGCAACTGAAGTGTATGATGATAAATTATTTGCGACCCAAGGAACGTAACGAACTTTGATAACGCTGGTTGCGCTCCGGGATGATTACCGAGGAATCCTCGCCTCGCATTCAATTCGATAGGCCT',\n",
       " 'ATAGAACGTAACTGATATTTTCGCGAGTAACACGGTAACCGTCAAAATTATAAATACGTCCATAGCATTCAGTGTGTTATCTGCGATGGTGAGCTTTCTTTGCACATCCGGTCTAAACTTTAATGGTGATGAAGAATTTTACCTTAAAGAGCGTCTTATCCTGCTTAAACTGCTATAGCTGGGTAACGCTCGTCCCGCAA',\n",
       " 'AACCATCTTTTTCCCTATAACCTTCACAATTGGAAAAGGGGTCAAAGGTCACGGAAGTCTTACTATGTCCAAATGATGACCTTTGACCCCTTCAGGCGTGCGTCTGGTCCGATAGAGGTCAAAGGTCAAGTGTCGGGGTGTTAAGAGTTGGATTAGTTTGCCTATCAAGGGCTACTCATGAGAACGTAGGCGACTAAGAA',\n",
       " 'ATTATAACCTTTATAGTGGAGCGCAACATGGAACTGGTTCTATACGTAGGGGTTTATTGTTTAAGCTACACTTCCGGATTTACACACGTTACTAGGACCCCAATTAGTTGAAGCTTAATACCTTCGCCATATTTAGCCTAATGGGTTTTTTGTTTATTAATACCCGGAAATTCTGGTACTTGACTTCCGGTTTCAACCTA',\n",
       " 'TAGACTTTATCTGGTTAAACTAGCATTTTCTTATCTCGCCAACATCCTTGCAAGCTAATATTGAATACAAGGCTAGCACTGGAATAGCTCTCAGGGTATCTATCCTAAGTACTAATTAGATACGTCAATTCAAATAATTGATCATCGTTTCGAGGTGAGGATTATGACTATATCTCGAAGAAACAATACATCACTTTAAA',\n",
       " 'TGACTAAAGAGTAATAGCTAGTTCTGTAATTCTATCCACTTGATGTACTACAACGCTCAAGGTAAGATGGAATTATCAGCACACCCTGGTCTTAAGTAAGAGAGTACTGGACAGGCAATGCGTAACCACAATTTGCCGTTATACGTACTATGATCGACAAGCCATATTTGCGTGTCGATTTGTAAATCTGCGGCATAGTT',\n",
       " 'GGTCCCTTATCTGATGTTTTATTTTTAGGAATGGGACGTTAGACGAAATGATTCTCATCGTGGTATCAATTAGTATACGGGGCTTGTATTGGTACAACACGGAATGACTTTTATGTCAATCACATATAGATTGTGACAACGAGACAACTAGTCTTATAACCTTCGTCGGACCTATTCCAGTGTCCTGGCATCTGTGCTAC',\n",
       " 'AAAGTTAAAAGTCCTATCTCCCATGGTATAACTTCTTGCATCTGTCACATACTTAATGGAAAGATTAAACTTTGTTGTTCTCGCACTTTTTCGATTGTGAAACCTAATCCAAATAAAATATATTTCCCTTTCCCCCCTATCATTCTGGCATAAGAATTTGATACTACTACCTGATAGGGACGTACCATCTAAGCTACAAT',\n",
       " 'AACCCGGAAGTGCGTAGTTACATAAGTTCCTATTTCCGCCATTACTTGGATGAAAAGGGATCTCACGCGGCTGATAACTTTGAGTTATCATGCTCCATTAGAGTACAGCATACATTCCAACGTTCGTTTCTTGATCGCAAACTTACAACTTTTGTTGGTTAAATCGTCAAATGTCAGAGACAGCTTGTGTAGCATTCATA',\n",
       " 'ACGTAACATTAGATGAGACCATATGACCTTCGTCGAGTACACCCCTATATAAGGCTCTTCGGATGCAATAAGCCCACTTATAATTACTGGTGCATAACTTCCTATAAAAAAAGTGTGAGCGCTACAATTTTGTTTGTCTCAAAGTGACCAAATCTTGATGCTGAATGCGTGACCTTTCACCCTCGTTTTGCGTTCATAAC',\n",
       " 'GAATTGTCAAACGGACCTTCAAATAATGGGCGCATGATACATATAGTAAATGTAGGTTTCAACTGATAAGGCGGTACGTCTTCTAATATAAGCCACAGTTAAAGGTAGGTAAGTGTTAAGCATAGTAATGATAGAACTGAAGATTTGAAATATCCCTTCAAATATACCATGGTGCCTGCGTTTAGTGAGGGCCCATAATG',\n",
       " 'AATCCAAAAAGATAAATTCCACTGTTTCTTGTAAGCTTTTAGCACCAACTCGCACAACAGTTCCGATCTTGACCTTTGACCTTCGTTTATTTGGTCTTCACGGACCTTTATTTGTGACCTTTGACCTCCTGGTATCGTTTATTGTAATAGGCAGAGGAGTACACAAATAATGATTTGCGTTGCGTACCTTGGATGATTCC',\n",
       " 'TCTTTCAGATAGGTCATCAAGGTTTCCTGGTCAGATTCACTAACAATTACCTGAAGGATAGGATGATTTTCGCAGTGCAGTATACTTCTTAATCGTTGATGTAAAATTGTTGCGTTCAAGGTCACAGAAACGAAATTTCCAAGTATCCTCATCATGGCGCTCTAAATGTGACACCAGTCCACATGAATCGAAACATTCTC',\n",
       " 'TTATTATGTACCGCACCATGTAAACCAGTTCTCGATGTCACTACTTCCGCGTAATCGGCCTATTTGGTACCATCGGTAGTCAAATGGGTCTATATAGATGCTAGATTTGATCCGATAGATTATGTAGATGGCCTCAGCCGGATTTATACTAGACCTACGTTGCTAGGACTCGGGCCCATCGTGAGTGGGATTATGCGCAC',\n",
       " 'AGATAATGAGTCTCGAGTCTACGTCCACGCGCCGTGGATGCAAAGAGCGACTTCCGGTTGCAACTTAACGATGTTAAATCAGTAATGTATATCCTACTGCATCCTGGTTGATTTAGAGGCTGTTATCATGTTGCGGATACGGACAGTGAACGAAAGCAGCACAGACCGGACTCTTATTGATGTAGGGCACTCAGAATCAT',\n",
       " 'CCCAGGAATTAGTGTGCGTTTTAGCATGAAGCCGGAGTACATGTTGCAGTTATCAAAGGGTCAAAGGTCACCGGAAAGTACCAAATTCTAAATACCCTAGTCTGAAGTGTACATCATTGCTAGAGGGTAAATAAGTAGACGTTGCGTAACCTTTGACCCCCCATTAGTAGGAGTCGCGCAACTGGGCCACAGCATAGAAA',\n",
       " 'GGTCTTCTAGACCAAAGAGGATCGGATGAAGCTTGCACAGAGCTCGTGAAGCATTGCCAGGACTAGGGGCCTTGGCCGCATGACCTTTGACCCCTGCTATAAAAATGCCCTCTGTCGCTGAACAATTATATGGCGCCCTGTCTTCGGGGTCAAAGGTCATCTAACTTTAAGATGAGGGGTACACTAAATATTAAGCACCT',\n",
       " 'TCTTGCTAGTGCCTAGGCAATTCTATAAATACCTAGGTCTTATTTCCGGGTTCCGCGCCCTTGAGAGTCCCTTATCGTAATAATCTTTAAATCATGATATACTAAGTTGCAAGACACCGATCTACTTCCGGGTTAAATAACGAATCAAATCTTCACAGATGTCGGATTTCAATCTAGGTCAATACGTCAGGGGAAAAAAT',\n",
       " 'GTTGTTCGTGGATTGTTCTCTACCAATGGGCTGTATCTTAGAAAGAACAATATCAGTTGGTGTGAACCCGATTCTTGATAGCAGTCCTTTCCCATCTCGCTCGCTTCAGTCTGTAGACTAAGATAAATATCATAGGTCCGACGCAAACCCGGAAGTATCTACCTTACGAATGTCCCGGAAGTGTAGGTAATTTGATTAAA',\n",
       " 'GTTATATTGTTGAGATCGTAGAAATTTATGGCCACCTGGTCTTTTGTCATGCTCTCTTTTAAGTGTGGCGGTCGTAAGTAATGAACTGCATGTTACATCCTGAAATTCACGTCTTGATTACTTGTTATTGATAAATGAAGGAAGTGAACAAAAATGACCTTTGACCCTTGAAGTTTATCAGTCGTAAAACCGATGATAAT',\n",
       " 'GAGGTAGAGAATGAGGTCAAAGTTCATTCGGTTATTGAGTTAATTTATAATATTCGCCGTTGTCAGCTCTTGAAGGTGGGGTCAAAGGTCATCTTATCTGTATAGTAGTATAGCAATTCATTTTAATTACCACGATGACCTTTGACCTCTTGGCTGGGCAACCAAGGTAGACGCATATGTATTTTTATCCATTAATATCA',\n",
       " 'ATCTCTTACTATTCACGTAGGCTTATCTTCTAAACTCTTATCAGAGTTCAGAACCTACGCCTTATCACGTGAATTGTATCAGTCCTCTACCCCTCTATCTGGCGTTATAAAGTTTGTTGAAATTATACCCATAAACCAAAAATCGCTTACAAAAGTATTTCGAATGAGCCCGTCTTCATTATTACATATTCTATACGTAC',\n",
       " 'TGGTAGCTAGAATAGTAATTCAAAATTATAATAAAACTATATACTTTCCTGATAGGTTGCTTTTTGGCCTTATGCTCCATCATGTGCCTGCGAAAAATAGGCTGATAAGGTGAAGAGAACCAATTGTTATCAGCTCTATTTTTCTGGTAAGCTCTCAACTTCGAAGTTTCATCCAACCCTATTACGTTACGTCAAGATCC',\n",
       " 'GCTCCGTATGGATTATTCGGTTTCTAACAGATAGGGTCGTTATTAATAGATCACTTTCCACGTCACGCGACGCAAAAAATTGCATCGGTATTTATCGACAGTTCATTGACACATGAGTTGTGCGGGTATTAAGGGGTACCCTTATGAGTATGAAAGAACGGGATCATTATATGTCAGAGCTTTTTTGATAAAACGAATCA',\n",
       " 'TATAGTATGTAGTACAGGTAAACAAGTCCTCATCCCATGATGGGATCCTTCAAATTCTAATCAATCTTAGGAATTCCTGGTATTAACCTCTCAAATTTTATCTCGTTGCCATCACTGAGTCTGTATTTGAATACTGATAAGTGTCGATTATCGCTATTTATATCCGGTCATAATTTCTGTCGTTACAAATACGAAGTTTG',\n",
       " 'GTAGTATGAAAGCCGGTAAAAATTCCGAGGTAGCTAAAATCTATTCTCAGAGCATATTTAGAATTAAACTTTATAGCTGGCTGTATCTGGTTATAGCTTGCTATAATAAGTATATTATAGTTTCCGTGAATCCGTTATCGGATTGTCGACTTTACCATTACTGCCTGTAGTGGAGATGCGCGTGATACATTGCCTCCGGG',\n",
       " 'GGACCCTTAGCGTTCCTAGCTAGAAACATTCGGAACTTCCTATCTTCGCCTGATTCGCAACTAGTTTGCGTGAACTTTGACCCCTATACAAAGAGGTTTTCTGGTCACTAGAACTAATGATCGTTAGAAGTCTGTAACCACAATCCGGTTTTGGGGGTACGCCCATTAACCCCGTGACCTTTGACCTTATATCAAGTGAC',\n",
       " 'TATTCTAGAAACTAGTTTGGACTGGTCGTGTCGTACATACCGCATTATCTTAGGGCCCACCCGGCACAGGGGAAAAGTATAATGTTCTTCGACATGGGACCCAGAAATAAAATAGAGTTGATGGGTCAAAGGTCACTTCGTCTCTTCACGATAAAACTAGAAACAAACCTATTATACGAACCTCACCATTTTACTCCTTG',\n",
       " 'AATTTCCATTTGATTAGCGCTAGCCTAACGTGCACCAGCACCTATGACCTTTGACCCTTGTGCATACTTGAAAAACAACACAAGCCTAATGCTCTTTCCAATCTGTGTCCTATTAAGTAAAATTCTTAAATCGCTATTCGGGCCGTTGTCTATACCTGAGGTCAAAGGTCAAGATTTAGGCGTAATCTGCCGCAGTTCGA',\n",
       " 'GTTAAACCCGGAAATCAAGCTATATATAATATTTGATACGTATCAACGGGAGCTGAGTAACCCGGAGTGCTGATGGGAATAAATGGCTATCTGGTGCGGTTCTCATATATTGTTTCAGTCTGACATATAGCTAGCATCCGTTGTAGTTAAAATGTAAGCCAATAGACATTAAGAGCTACACCAGAGAGCCGAGAACGCTG',\n",
       " 'TTGACCCGGAAGTGGTCGATCGTCTTACACAAATCTTCTTATGATGAGGGACCACACTTACACGCATACTTAATTTGAATTCCCCCAATGAACAGTTTCAACGTATTTCAAGCCCAGCTAACGTCTCTCTATCGGTAAAGTCTCAATAAAACCCGGAAGCGGACTAATTACTTCTAGGGACTTCCGGGTACGAAAAAATG',\n",
       " 'GATTCTTGCAATTTAGAAATAATATACACGTATTTCAATTCAAATATGAGGTAGACTGTCTATCAGATTTAGAGTTCCAAATCATTGATGGGATGAAGGGCTCCATACTAAACCCGACAACTCTAGATGCTACTCATTTATCAGTTTCAGGCTGTAGCGTGCGTTATAATTGTTATCAGATTATTCACAACATGTTATCG',\n",
       " 'TTCGTGGTGGAAGTGCAGCAAACCGTGGGTTCTCTGTATCTAACTTCCGGGTAATTTTGACCTAAATGTGTGATCGATAATATTTTATCTAAATTTCCGCGTTTTCGGTAAACTAACACTCTCGAGGTATCCATTGCCACTGGGATTTTCTTTCGTGGATACACTACCCCTTCTAAACCGTTTTGACATATACATACCAT',\n",
       " 'ACGTTGCATCTTATCTGCACCTAAAAAGCTTAAATGTAATTTAACTTCAACTCACACTTTGAATTTTATTTTACAGCACGCGCCATTATCTTTTATCCGCCTAATCACAATGTCTTAAAATGACATCACGTTCGCGTAGTACTTACCAAGATATAAAGTGCTACAGAATGCGTTAGAACGGGTTAAACTGCGGGACCTGC',\n",
       " 'TACAAAGCGTGTCCCTCTTGGGTCAACCTCAGGTCAAAGGTTATGCCATCCCGTAATTTCCATTAAAGATGACCTTTGACCCCTATTGGATCGCGTAAGAGACGTCGTGCGCGTTAAAGGTCGATGACCTTTCACCTTTGGGAATTCATAGGCAAGGTCTGACCGAGTGATATTCATGTCTGGGCACTGCCGTACATGCA',\n",
       " 'TAACTACATGTTTTGTCGTGTTGACTCAGAAAAAGTTTTGAGTATTTCAGCCTATGCAAAAAGCAAAGGGTCAAAGGTCACGGTCGAGATCTTAGAACTAAAAGGATGATGCTTTGATGTAAAAAGTAGCTTTTCACGCATCATCCCTGAAATATCCCACGATTATCAATCAATAACGATGTTTGCTTTCGTACTTTTTC',\n",
       " 'GGAAATGGTTAATAGGAATTTTGACCTTTGACCCTCGTGATATTCAAGACTCATAGCCTATTCAATTGGCTCGTGTTTACTGTAACGGTTCTAAATAGTAGTAGTATTCTTTTGTTTGTTATTATTCCTTCCTTATCATCGCGCAGCGAAAGGTAAACCTGTGACCTTTGACCCTCGGCTACGTTTTTGAGATATTACTA',\n",
       " 'TAAAGAAATCCCGGTTCGTAGGATCGGGTACTCCAGATGCCAACCAAACCTGTTCAACGATATATCGGTATAATCATTTTGAACTTGGTGGATTTATGTAGGGGATCACTCAGAAACGTATGTATATAAGCGTTGTGTCTAGAGGTCAAAGGTCAAGCAGACGTTAAAACACAAATACTGTCAGAAGACTAACAGGAGAT',\n",
       " 'CACATGTTAGCCCGGTCGTAAATTTATTTTTGAAATGGGTGAAAATGGGCTGGTATCTTCCAGCTTTCCTTTGAACATGTCGGCAGGGGTGTGTACTGATAACGGCGACAGCATTAATATTATTGCGGTAAAAACTCAGTTACCATTAATTGATACCATTTATGGACCAAAAAACTGCCGCTTGCTCGCGGGTCGCAGAT',\n",
       " 'AACTCCACAAGTGAAGTCAAGTTATATCCATGAAAATGGCGTAAACGGGTATAAACCGAAACAAGCTGCTTGGTCTATTAGTGATAAAGATATTGTTCAGTATATAAGATCACAGTATAAAAGTTGAAAGTACGTATTGGCGGTTAGGGGTCAAAGGTCATGGGTCCACCTTGAGTGTCCTTGAAAGTCGTGAAGAGTGA',\n",
       " 'TAAAAATGGTACTTGCTGCACCTTTACCACCGTATAAACGCGCACAATACGCCCATTGACATCTATATCAGAAAGAAAGCAACATCAATGGGAACGATAAAACTCACAATTAGGCTCCTTAGTGCCTTTTCTTTAAGCCGGGCCCACACGGGGTATAGCGGAACATATGTTTCGACACTTAACAATAGCATCCATTCAAA',\n",
       " 'AGCAGACAACGGGTTCAGTAAATACTTTAATTCGTGCATCCAGGAGGGTCAAAGGTCAGCTACAATTTCAGACTTTCGACCTATATAGGTCGCTCTGCAGATTTAAGGTCAAAGGTCACATTCTGGAGTACGAACACTATTAGCCCCTTTGCTCTTACAGTGACAATCTATAGTCGGACCATGACCTTTGACCCTTTTAA',\n",
       " 'CCCAAGGACTGGTAACATAACACTTGGTACTAGTCGATGGGTCAAAGGTCACTGTTTACCGTTGTGCCTACATCGTCACAGATAACCTCGACTGTTTGAGCCATATAACTTAGCCGACAAAAAAATTTGGGAGACATATTCGACTGTTCCGGGCATCGATACTTTATCCGCACTTAGACCCTAGATCTCTCGTTCTTGAT',\n",
       " 'CGCTGAAACACGACTGTTCATCTATAACTTCCGGGTGGCGTAGGAAAAAATAGTAGTACCCGGAAGTTGCTTTTTCTATGCTGCTAGTAACAAGCGCTAAGCAGAAGATGGAACTGTCAATTGAAAACTTGATAAGCATTATTTAACGTTACTGAACGAGTGGTGTATCATCAGAGACCAGAGTACTATAGAAAGGCGCG',\n",
       " 'GCTGATACATGGCGTCATAAAGGAGACTCAGAGCCATAGCAGCCAATTCGAGATCTACTACCGCATTGAAACAGATAAATAATTAGGTACTTTTTCTTACATCGATCGTGATTTCGGGTTAATAACAGGATGACCTTTGACCTCACGTATTTAGAATCATGTTATTATAATAGATTGCAACGATTTTTCGTATGGTTTTC',\n",
       " 'ATTTGTTGCTCCCAACGGACTCTAGAAGTCAGGTTCTGTGGTGATTATTACTTCAGGGGTCAAAGGTCACAAGCCGTTAAAACGGTCTTATTTACTTCGATGAGGTCAAAGGTCATCCATTGTCATCATTAGATGTTTGGTTTGTAATCCACTCCAGTAAAATCAGAGATCAGCGTACTCGTTGACCTTTGACCCTATAG',\n",
       " 'CACGCTTTGTCTATGCCAACGCTTATCACTACAGGCAACCAGTTCATCCATTCTTCCCTGTGGCGCTAATGGAATAACATGCTTTATTTGTAACGTTACCACCGAACGGTCTGTATTTGTATGATCACTAATCCTGCGTTCCAACCCTAACTACACCTCCACCCCTATTCACTAGATCGTCACACGTAACCCTGCGGACT',\n",
       " 'CGGAATTTCCGCTTTTCAGCTATGTTCCAATTTCGGTAAGTTTTTCCAGGCATTGTTGACACATTAGATCCGGAAGTAACCACGTCGACTTAGTAGCGCACGATCAGTCTAATATTCGTATGGACCCGGAAGTGCTAAAACCATATCTCAATGGCTCGCAATTGGTTGTTCTTGTGGCACAAATCTTGGCTATTGATTTT',\n",
       " 'AAACTCTGAGTATTATTAGAAAGCGGGCAGGAGTGATAGGGCGATGCCATAAGTCCTTATCCGAAGACTCTAATAAAGATTATTGCTCCCCCGGGTTTAATTCACCAATTGTAATAAGGGCGCGTATACAGCTAACCGGCTTGTATCGGAAACGTAAAGTGCGGTTGTGTGATTTCACACTTTAGGAGTTCATAATACTC',\n",
       " 'ATTTGAGACCCGGAAGTCGAAGATTTTATAGAATCTTACTAACTACAAACTTCCGGGTTACTCAAAGTACCATCATCCGTCTAAATTGACAAAAGATGTAGGACAACAATTTATTAAAATGAAAAAGTGTCCTCATCAAATTTAACTGATTTATACTTCCGGTTTTCTATTTGATACCCTCCAGATTATTCAAGTCAGTG',\n",
       " 'TATCGGATGTGCATTGATTACATTTAAGGGCCCAGGATCGCTGAAATTCTATCACGTGTATAGGAATACTTCCGGGTACTTTATAGATAGGTACTGAGAGGGGCTTTCGAGCAAATTCATAAATACTTTCCTACTTCCGGGTCTGCATTCCTCTATACTTAAAATGATTTAACGAACGCTGCTGCTTTTGGAATAGCATG',\n",
       " 'TCCAATTGATCTTCGATATTAACGATGCATACTCCGCAAGCTTCCGGTTTAGCTGTAGACTTATATTTCGAGACAAGGAGTAGGTGTTGGGAATTTATACTGCGGTGGTATTTCCGCGTTCCGGAGACCAGGATACAGCCTTGATTAGGTGAACAGAACCCTTTAAGTGCAGTGTATATATTGACATAAGAATTATATTT',\n",
       " 'TCTAGAGAGCCAGATTACAATATCGGAACATGATATTCTCGAATCATTCACCTATATCATCTTTGTTAAAAATTAAGAGATAAAAACATTATCTGAACCGCGAGGGACAGCTGCCATATCCGCATCGACACCCATCTGCTTCAAAATGTCACATGAAGAAACTCACCCCTACTGATTTCTCCTGATGGCAGGACGTCGCT',\n",
       " 'ATCTTAGCACATAACTGCCCGAGTAGCAAATAAAAAGATTCGTAGAAGCCTAAATAATACATCCCCAGGAACAGCACACAACTATAATTAATTAGTATACCCCGGGGCAGGATTAACGGCACCTCTTTTGATGTCTAAATATAAAATATGGTATAGCTTAAGACTGCTCGTCTATATCTATGACCTTTGACCCCGATTTT',\n",
       " 'TTTACATGTTGTATTCAGTACCTTGGTGCTGTGGCTTTACTACGACCGAGTCAATTTAGTATTCACTGCGGAAAAATGGTTTGCAATACGTCAATAAGTGGTCGTGAAAAATTACACTCTTATCAAGAAAATTGAGAATCTTTATTGAAATCTTCTCTCAACTTAATGATAAGAACTTCGTATTGTCAAGAGTGGTGTTG',\n",
       " 'GTAGTATAAACTTCAATAACTCGCCGCCCATAAAGCACAACTGACTCACTTCCGGGAACCAGGATACTTCCGGGTTCGACTTCCTCATTGCGTAGCAATTTACAAGAAATTTTGTCGAATCAGTCGAAATACTTAGCGTAAGAAGATAATCAGTGCCTGATGATGACTTGATATAGGATCAGAGGAATAACATTCATTCC',\n",
       " 'TTTTTTAGTACTCCGCGTTTCAGGGGTCAAAGGTTACCAACAAAGAATTAAGCGCTTTGTAAGCATTCGCTGTCACAACTATGCCGTAAGATCTATCTGTTTTCCCATCAAAGGTGTTTATGCATACTAAAGAAAGAACAAGCATACGGTGATTGTTGAATGTAAAGGCTGAAACTCGCCTTACTACTTCGGTTACACGA',\n",
       " 'CCTACGTTGTCCGAACAAAATGTTATGCATTCGATGAGCCCATACTCTTCAGAAACACTTTTATCGCGCTTGATGACCCTACGTACGATTATTCAAACATAGCATAGGCTTTTGACGTGGGGTCAAAGGTCGTGGTCTTTAGGAATTACGTCGATACGTTAAAATATTGATTATATAGGAGAACGTATTACAATGTTTTG',\n",
       " 'ATCATGGTAACTTACAATGGTCCTGTGAACAGGTCAAAGGTCATTGGAAGTTAGACCTAGGGGTCAAGGGTCAAACGCCGATTTCGACCTTGTAACCAGGCTATGATAACTACTTATAGTTATTCTTCCATATCCAATTATGCTTGTTATGAACCCCAACTTCTCACTGTTGTAATCACTTAATCAACGACGAACGCTCC',\n",
       " 'CACATAGAGATAGTCGGGGAAATAATATTTAAATTGTAGAACGAGAGTTCCCCCATTTTCACATCAATCCACTACGTAAACAACGATTATAGCCTAATTAAAAAAGCTTAACGTTCTTATCTTTTCCCCGATCTCTTCCCGGCTAGGTCTATCAAGCAATGTGAATACTGCTTTGAAGAACCTCTATGGCACTATCATAC',\n",
       " 'AAAAGTTTATATTCTTAGACTCAGAGACGATATTTTCTATGGCTCAAGACAAGACAGAGTGTAAATGAAAATAGATCCTCTCTTATACTGCCGTAGTACGCTTATAGTCGCTACCGGTAACCTTTGACCCCCAAAATAGCGTATCCATAGGAAGGATAGTTTCGCCCTATCCTGTGACCTTTGACCTCACGATGCCACTT',\n",
       " 'CGGTTCTCGTTGTAGGTAACGTTCCATTTATCGTTCTTAAATTCTGTCGTCCTAGACATTCTATTAAGTGTATATAAACAGGAAGTGTGCTAGGCACTCCGCATTGCTTGACCCGGAAGTGAGCGACTATGCCGTAAAAAAGAGTCCCGGTGCAGAGGTCTATCACACAAATACCGTGGAATTGTACTTCCGGGTACTAA',\n",
       " 'TCCACACCTACAAAGAGTACATTCCTGTTATATTGTAACAGATGTATCGTATCGCACTTTGATTCGCGGCACTGAAACCGTTTATTGCGAGTTGGGCCACATAACTTGCATCAATCACGATCGAGCGAGCTTCGTAGATTAAGGTCAAAGGTCAAAAACCGCTCAACAGACTCTTAAGCTTGTATTAGGGTTTCTTAACT',\n",
       " 'AAAGGTCTAAATTGAGTAGCTGCCCAGTTTCTATGATCGGTTTAACTGAACCATGTTTTGAGCATGGCCTACTGGTGAGCTGGATGTATTCCGGGTTCCTACCCGGAAGTATGTATTAAGCACCCATAATAGAAGCGTTCACATCAACCCGGAAGTCGCTCATCGAAAATCCAATGGGCAAATAGTTCCCGAGTCTTGTC',\n",
       " 'CAAGTCGCGAACGCACAACAACATGCTATTGTGCTACATGCATAGGCGACATAATGTTAAATCGCTTAGCCAATAAGTCTCTATATCAACCGGATGTGGTGGAATAGCAAACCACGTGCACGAAGATAAACGGCTAGAGCATACGGTTGAGACAGCGTATCTTTAATCGCCCGGGTATATTGAAGTAGATTCGAGCATTG',\n",
       " 'CGGATTTATAAGACATCAAGCAAATCCCATTCTCATTAGATGTTTTATTGCACCTGAGTTATCTTCTATTGCTACATAGGAGTCTATCACTTCCGGGTAATATATCCCTTTATCAATATTGCCTAACTCACATCAGAAGGTATTTTATTTCATACACTTATATTTCATTATTAAAATACTAAGATACAATACAGTAGGGG',\n",
       " 'ACGATATATCTTATTCCAAGAATTATCAGGGATAGTAAAATATATTACGGTAAGCGAGTTAAGTAAGGTCAAAGGTCATCGAGGAAAGGTTATGCAGAAATTGCCTTCTTATTATTACCACGCGATCCTGAACTGGGGTCAAAGGTCACCCTATCGCAGGAACCATTTACGCGGTGGTCTTTGAAGGGGACCTCATTTGT',\n",
       " 'ACTGGTCTATGTCACGACCACTTAAATACTTTTAATCGACGTCACTTACTATTAGTAGGAGGAGTGAAGGTCTCCGTGTGGTGAGGCATCCATGCCGTGTAGAAATATTGTGTTATGGGAGAACTATGCGCGCACTAATCAATAACTATGCCACTTGAGTTTGTAATCCAATTCGATCCCTTGACCTTTGACCCCCGCTT',\n",
       " 'CAGAGTCGTAATTTTAGCCGTAGGGTCAAAGGTCACACTACGGATAGTTTCGTGTCCAATTGGTTCGTAACTAATGTGTTTTAATCACCCTACGAAATCTTATGACCTTTGACCTTGGTGCTGTATTATCTGTCGCAATGCAGGGTCAAAGGTCACGCATACAACGGATATAACTGATATGGTAGAGAGTGTTGATGACT',\n",
       " 'GTGTATTAATTCACTTCTTTAAAGCCCACCGGCCACTTAGACGATTAGGAGAAAGCAAAGAGTCATTGACGTAGGGATGGGCGTTTCCTTACTCGTCTTTATACAGACCCATGATAACTATATCTGCTAAGTACTGTGTCGGAATACATAAGATGATAGCTTGCGAAGACTTTAGCAGTTTGTCCATCTGATAAGCATAT',\n",
       " 'TGAGTATAAATACAAGCGGAGTATCGCACTTCCGGGAGCGATAGGATGCACTTCCGGGCCGCTTAAAGATGATTGGTACTGTAGAGGTAAGACATCGAAAACTGGCTACGTTAAGCAATCTAGAAAAGAATACTTCCGGTATGATTAAAGTCTGAGACAGAGTACTGTTAGAGTCTGTCGTTAAAAGCAGTGCCCGCATA',\n",
       " 'GAGGTACGTCCGAGGCGACAATAAGAATACTATCACATATAATTCTTCCCATCCCTTGGATTTTATATAAACTGCACCACTCCTCGGTTGTAAACCTTAAATAATTGACCGAATGATCGGTACAAGCATCATCTAAAATGTACTCTTATCACTAATTCAGATAAAACTATCATATATGGGTCCCCGTGAGCAGATGGTCA',\n",
       " 'CAGTTCCCGGAAGTGCACGAAAAACGCTAAGAGTGTCATGTGCGAATTAAAATTACCAACCCGGAAGTGGTCCTGATGTCGGAATAAAACCCTTGACCGTCAAAGAAACAGAGCTCAACAAGCTTCGATTTATACAATATTGTTAATCTTGGCAGTCGGAGACCCGTATTATTTGCTACGTTCTCTACATCCAAACGGAG',\n",
       " 'ACGATACGTATCTCGCAATTTTATGAATAAAACATAACAGGCAACAATACCCTCAGTCATTCGTAATTCATACCCGCCAGTCTTCTCGCCATATCAGAAAATTCCGGTCCGAAGAGTTTTGTGATGGTCAATACGTTACAAAAGTTTGGTTTATTTGTACTGAATCAAGACATGCCTTCCTTCTTGAGGCGCGCACATAT',\n",
       " 'ATGATAAAAGAAACAAATGTTGACTTCCGGGCCACTCGGGATTATTAACCGCTATTTGTGGATTCTATTGACGTGTCAGAATCTGGAGGGATTGAACTCTCTATGAGCATCTAACAGGCGGATATCTCAGTAGTTTTTATCAACACATAGGTACACTAGCTTGAATGTTACTTGTGCCTATTAAAGGCTGCACCAATACT',\n",
       " 'TGGTACGTAAAAAACCCCCATGAACCTCGACATTAACCGCTAGGATATACAATGGTCAAATTAACAACTGATCACGACGAAAACCTGGTTTCTAAGTTTTCGGACTAGTATACACATCAAAAGCCGATTCTATTCATGCTGAATACGAGGAATTAAAAAACAAATATAGGTTTTACTCTCAGACACCGGAAGTCACAGTA',\n",
       " 'GGATATAGTCTCCGATCTTATATAGCATGTTGACCTTTGACCTTTCTATTCCATGGTATGTCGCTACAAGCACAAATTATTCACATTATATCATAAAAAGTATTCAAGACATTGAACGAAGTATATCATTAGAATTTGGGCCACTGAAGTATGTCTGAATCGATACGTGACCTTTCACCTCACTACAGATAGCTTATATA',\n",
       " 'ATACCCGGAAGTCGTAACGCTGTGAGAATTTAGTTCACTTCCGGGTTAGCGCGACAAGGTTCTCCACTTCCGGGGATCAAGCCTGGCACAGTCTATCATCGATATATTTCATAATGCGATCACTGTCTTCGGGTGCATAACTGTTGGAGAGTTCATATTATCTAATTCAGTACTTGATCCGATCCAACCACCAGTTTTTC',\n",
       " 'GGCCGAGACGGTTGAACTTTGACCCTTGGCACAACTACAGCTTGACCTTTGACCCCTGAAGTCGTTTTGGAGAACCCTCCTCACACTTTTAGGGATTGTTATACCATCAAGCGTGTGGTGGACGTTGCATATCGCCTGTGACGTACCTATTTTAATAATCATCGGTTTTCAAATACCGTAACATATTGGAAATAAATGGC',\n",
       " 'ATATCTTATTTCTGGCTCGAGCGAGTGGCGTATGCTATCAGATAAGCAGTTAAAATCGTACAAGCAGAGGAATTTGCATCGGACAATATACTGCTTGTCGAAATTATCAAGGAGCCTCTAACGAAAGGTATTAGAGCTATGATAAACTTAGTAAGATAAGCGTAAATTATATCATAACCTTGACGCTCTCACAAAACACC',\n",
       " 'ACTAGCAAGAATTTCTCCGGATATGGCCTATACAAATGTCAATCACTTAGGCCTCTTATCAGACTATCTATAGGATCTATCTTGAGCATCTCGCAAGATTAGACAACTAGCAACGCTTATCTTACAAGACGAGGCGTTTTTAATACAGACACATTAGAGCTAACGTACTACTGTTTGTTAGTCGTGTTGGTACAAATGAT',\n",
       " 'AACTGCTTAAAAATTCCAAGTTAATCATCATATTTTGATCGGACTGAGTTTTAACAGATGTCTTTATTGTGCTCTTTAGTACTTTTCCGGGTTCCCATACAAGCAACTTCCGGGGTTAAGGAGAAAAGGCATTAAACGTAAATTAAGTCACCCGTAGTCACATGGCTAGCAATATCATAGAAACTAAAGTCTTTATCGCT',\n",
       " 'CAATTTTTGATCGTGGTACCGTTTTTACGATTCAATTGGATAAACTTACCCGGAAGTGATGAAAGAGCACCCGGAAGCGTACGGATTAATAGTTCCGCAACAGGGATGAAAATACAATCTCCTGCTATTTCGGTCCAAACCTATCTTACGGTGAGTCTAGATGAATGAAGGCTTACGCCATAGTGGCCAACAATATCAAC',\n",
       " 'GACTTGAGCCACAGAATATCTACTAATTGTCAGATAGGTAGGATAAAGTCTTGGCACACCCGCTACAATCGCTGTCGTGACGCGACAAATTATATTGCTTTAGACACACGGCGAACTCTCAGCTATTCTTATCAGATAGGCGTATGCTAATAGCTCTCGAGGTTTTTCTTATCACCGTTTTATGCCGTTTTGTCTAAACA',\n",
       " 'AGTAGGTTAAAAGGAACCCGGAAGTGTCTTACACGAATGTTCCCCGTAGCATAGACCGATACGACGTAAGTATATATCCCCTAGTCCTGACTCAATGTTCTAGACTTCCGGGTAGTCTGGGGAGCTTTGTACACTGGGTATTGAGGTTACTTCCGGGATAGATTCTCTCGGCGGTATATGTAGCTTTACCGGATAAGTAT',\n",
       " 'AAGGAACAAAGCCCCGTCAGATGCCTTACCCTACTTTTTAATATTATCCGAATAGTGATCTCCGCATTAGTAGACCCTCTAAAGGGAGGATATTCAGGATTTGTGAGAACCTGTACTAGATATAGACCCGCTATAAAAAGTACGTCGCGTCGTATGGTAAGTTCGGGCGAAAATTGAACAGATATTACCTAGCCGTCACG',\n",
       " 'CATTAACCTCTCTAACTCCAATACTTCACAAGATATGCGTCAAACCAAACTCGGCACAGTGTAAATACTATGAAGCCTCGGATCTGATAACCAGTATAGTCCCTGGGACCATATTTGCAAGTACGAAAGCACCCTGAGCGACAGCTTGAAATGCTCTCTCCTGTTCTGCCGACAATTGATATCTTGTGAGCATGAGCTAT',\n",
       " 'CACCGCAATCCTTAATATTTTAGTGGTGCGAATAATATACTGTTTTTCAAAAATTACCTTATTTTGAGTAAGGGTCCCAATCTTTATAAATATAGAAGTATATATCGCGAGAACTTCTACCAGGTTTGTCCAAGACCGTTTCCGTGGAGAGAAATCCGTGACCTTTGACCTTACGTAACGCATACCTGATAGTTCATTGT',\n",
       " 'ATTTATAGTAACAAGACAAAAACTTGACGACATAAACAACGGTTGGATCACAAGCAAATATTGCAAGAGAATCGTTTTTATTTCACGCTACAGACCATATGTGTCAGTTAATAATAGTCCGGACGACAGACGAAATTAGTTCATAGTTCACGTGCACACTTCCGGGAGACTTCAAGGAGTAAATAAATTTACGTAAACTA',\n",
       " 'GGAGGCCCAGAACACTTTGGTCAGGTCCAGTTGTCCAGAGTTATCTTATGCTTATGTATCCCCAGAACATACTTGAATCCAATTCTTTGGTCGGAAGGTCACGTACAAAATACCAATATAGTATTGGATAAGATAGTCCTGTAGATCAGATCCGCTAACTATTCTTCGATAAACTATTTGATATGTATACGGCAAATTCC',\n",
       " 'CTTTACCTAGTCGGCACTGATGGAAAATTGCAGATAACTCATCTGTGGAATTAGGTTGGAGATTTTCATATTCATTGAACCTTAAATCCCCGATTTGGGACATTATCAGTTCCAGACTTTGTCCAAATTACAAAAGATTAGCACCTTTTTGTATCATTATCATTACTATCAGTTTTGCTAGGACAATAGCACCCATATAG',\n",
       " 'TTCCAACTCAGCATCAGCAACTATCGTATGACCTTTGACCTCAAGCACGACCGTTAGTAATACGTCGACACAACAAATATTAAGCGAATCTTATGTATAAGCTAATCAAAAGGTAAGGCAAAGATGAGGTAAAAGGAGTAGCTGGATAGCCATTCGTTATTCCTGAATTGGGGGGTCAAAGGTCGCCCGTCTTCACTCGA',\n",
       " 'TACCTAATATCATGCGCAATTCAAGATAAGGATAGAGCGCTATCAGCACTCCGCAACACTTTGCCTAACTCATTAAGATATCATAACTTACTTCTGGTTAATGCTGACATTCGCTATTGTATTAAGCAACAGCGGTAGTTTAATCAGGTGAGGAACGATGTATACTTTACCACGCTCAGTCCAGTATGTGCGGTATAAAC',\n",
       " 'CTCAAACTAGGCATATACCAATCCCTCCTTTCAGGAGAACAAGATTCAGTGCCGAACTTTGAGGAATAAAGTGTTGCAACTGGATAAAATAATTTTATCAGCCCTTATCAATATCTCATCTTGTCGCATGAGCAAACTTACAGAAAAATATACAAAAAATGTTCGCTAAGGATTCATCACGTTTATAGGGAACCATAATG',\n",
       " 'TCTGACAGCAGGCAAAGTACTCAGAGGTCAAAGGTCACCGGCACCCATTTCATAATAGGTTTGTTATGTCTCTGGGTAATTACTCAGCCCTTATGAAACGAAACATGCTCTACATTCCACTGTTCAGGCACTGGCCTATCCAAATTTAAACAGCAAGCAACAGTCTAACCAGGTATGACAAAGGTCAAAGGTCGACAGCC',\n",
       " 'GCGAATACTCAGATTAATATTCTCTTTCACATCGCGTTATTCGTTATGATCAATTTGCAACTCCGTGCGCGTGTTAGAGTGGGTAATAGGTGAACTTTGACCTCTGCCACGTCGAGTACGATGTAATGGTAGTATGACATGCGATGGGGTGAGGGGTCAAAGGTCACGTTTTATCTAAAGGAATGGGCTGCGCCGAAGTC',\n",
       " 'TTAGCGCCGATCACTAAGGTTTCCGAGGCGACGGGTCAAAGGTCATCTAAAACCTAAATACACATCATCTAATCTATACGCCCTGTCTCCCAAGTGGGAAATGAATAGCGTTATATAGTTCCTCACCGAATCCTCCGAGACATAGGGGTCAAAGGTCACACGAGCATGTCACATTCGGAAAATGCAAAATACGGGATTTT',\n",
       " 'GTGCTAGCCTGGCTTTGGATCTCGCGTAATAGGTCGCTTAGGGATAAAGCAGATAACGGCTAGAGACTATTGGACACCTGCCTTATTACGTAGTAGGACGAATTCGAGTTACAGGAACGGGTAGAGTTAGAACTACGTGTCATCGGAACAATGTCAGAAGTTAGCAATTATCTGACTCGACCCCATTCTGCAAACTGTCT',\n",
       " 'GGGGTCGTTTGGGATTCTTGTTAAAAGTACCCCTACCTAAAATAGAAAACGATTACGCCCCTCGTATTGTCAGACAAAATTTGAATGGAGATAATATTGACCGTCCACTCCTCATAAGTTCGTGTGACGTTTGTCGGTCACTTCCGGCTTTATTACTACAAACCGGAAGTAACCGAACCGGTTGTTTGGGTTAACAAATT',\n",
       " 'TCCACTATTACAATAATTGTAGGCTATTTCGTTCAGATGATGGTAGGTTTATTCTATTCGCTACTTGGTGAAAGTGACATTTGTCGGATAGAGCGGTTCCTTTATCATCCAAATAACTGATACCATGTAGCAGCTCTACGCGCATGCAGATAAGACGACCAACCCGTCCTTATCTTGTTTGCTGCTCCCCTCAATAAGGA',\n",
       " 'ATAGTATAGAACAAAGCCCGTTTTTATTATTTTGACCAATGACATGATGCATTGAGCCTACCACAACAGAGTTGAGCAATAAACGATATATTCATTTATCCCGGAAGTCGAAGGTTACTATAGAGGGATTCGAGAATGATAGATAAAAATCTTGTAACACTACTTTATGTTAAGTATCTTTAACAAGTATCTCTCCAGCA',\n",
       " 'GGTATTTAACGGCCATTTTTAAAATCAACAATGGGTAAATCGGGGTCAAAGGTCACAGGGGCAATTTAGAGTGACAGCATTCCATACTTTTAGATTAGTAAAAAGTCCTAATATCCCAACCGTGAGTATTCGTGTTACGCTATAGACTCACGAGTCTCACGACACCCATCCTAATAATCTTGTGTTAAACGTGGCTATAT',\n",
       " 'ATGCCCACGGTGATGTGCTATTTCGATTTCCCAAATTTATTATTAAAAAAGAGTCGGGAGTGGTACTGTACGAACCCGGAAGTGTGTTCGATCTTTTTAGTTGGCAAAATAGTTCCTCTCAGACGGGTGTAAAGCCGGAAATCCATCGATCAAAATTTTGTGGGTATACAATGTTCACACCTAGGACTTCCGGGTAGCTG',\n",
       " 'TATTGTTTGTGGAATAAAAATAACCTTATTATAGTCTTGAACGAGAATACGGATGTATAACGCTTGACCTTTGACCCCCTTTTTTTTTCCCTGATTAGAACCTTAATAATTAGTAGTACCCTCGAGAAGTAACTTCTAGGTGAAAACTATGGGGTCAAAGGTCAAATAATGTCGTGTATCGATAGAGATGTAGTGGTGCA',\n",
       " 'TTCTGAGGCCGGAGGTTTCTTACGATACCAATCCTAGTGGCCGTTGACCGACTGTTAGAACCTCTGAGGCACTTTTATAATGTTTCTTCCCCCCCTAACACACGTCTGTTGACGCGTAGGGGACCCATGTAGATAAATGACTAACACCATACTGGGTAATAGAGCACACCTACTAAGTGATTGTTTCTTTAGATTTCGTT',\n",
       " 'CCCACTGTACGATCCTAATCATCTTCAGTGCTCAACATAGCTGAAGCCTTTGACGGAATTATACTTAGTCATTACTAAGGATAATGCAAAACCATATAATATAAGTGACCTTTGACCTTTTAATTTAGCCTCGGGGTCAAAGTTCATCCGAGATCGCGATTATCACATATTGTTCACCCGGGGCCACAGAAATAGTAGTA',\n",
       " 'AACAGACCTAAGGGTTCTCTTCACAAGTTTGCATCTAAGTTTTATTAAGGTGATCTACTTATCTGGAATTAGTGACGTTGGCATATCGCGCCGACGCAATGCCTAAATACAAGCTACTGTTGAACGTAATATCACCAGCAGCCCCAGATTGAATTTAACGTTTATCTTGTTATACAGCCCTATCAGTAACTGACCACACC',\n",
       " 'TAAGAGACTTTTTCTAAACCACGTTATTCTTCTGGAGTCTGCAATATTCAATTCTCAGCATTCGTTAGTACATGTACTCTGAATTTACGCAACGCACTTCCGGGGTTAACCGGAAGTCCATCTAACTACATCCGGGTCCCTTTATACGAGAATCTTGTGCTCATGTCATATATTGGTCTCTGTCTGTAAGAGGTACTCCC',\n",
       " 'TAGATTATACTATACGGCGCATGAATCCCGGAATCTATATTGTATGTGACGGAACGCTCTCCTGCACAAGCCTAAAGAGTCACTCAGACTGACGCTTAGTGTCTATGTTACATCCCTCCAGACTAGTCCTGATAGTTGATTAAGTGGTTTAAAGATACATTATTAGATGCGTATATCCTCCGAGTTTCCATATGAGAATC',\n",
       " 'TAGACCTGGTGACTTATACGACTAGACTGATTAGGCATTAAACAGTTTATTACCTTACAGTATAGGATTAAATACATAACAGGCGATACGCGCGGAGCTTACACAAGCCACGTTTTACGTGAACTCTGATGAACCCCCGGAAGTAATCGATCTGTGATCCGTATCTGTCAACAAACCTTCTGCATCTCTTCCAGAGAAGC',\n",
       " 'CTAAGCTTTGACCAGCCAGTCTAAATACATATAGCTGCCGACGCGCAAAAGAATACCCGTCAAAAGTTAGCGACGTCCTAAGGAATTATTGTTAAATCTCACTTATTTTCAATTAGTCGTTGAATAAATTTCAGAAAGATTTCGGCCTGCTATATATCTGTCTGTTTACAATGTTTCGAATTCAACCAATTAGTTGTCTG',\n",
       " 'TAAACAAATTTGTATCTAGTCATAGTAGCGTTTAGCTACAGGAGATTTCTTGTGGCAATCTAAAAAAATTCTTAAACCCAAGTCAACTCCTGCTTATTACTTGCCAGAAGCTTATGAAAGTAACTTGACCTTTGACCTCTTCTAGAATGCAAATCAAAGTTGAATGAAAGAATTTTGTTTTGACCTTTGACCCCTACGTA',\n",
       " 'GGCAAGGTGTCTGGCGTTTTAGCGGATAATTGAGCAGACGCGAATCGTTAAGGGTCGTTAAGACTATTGGTTTGCGATCGTCATTTTCTTTTACTCAGTTCTTATTAGGTGCCTTTGTTCTTGACCCTGCAGGTCACGGGCTAGTAGAGACCTTATCAGGTCATATCATCCATCTCATAAGTCATCCACTATCCTTTCTG',\n",
       " 'AAATTTTTAATAGAATAAGAGTCTGTCTACTACAACTTCCAACTTGACCTGGAGCACACTTGATGATGAATCGTGATCACCGTGACCTTTGACCTCAACTCCTATTACATGTCCGAGAGGTCAAAGGTCACCCTTATATTGTGACATCAGGCTTCCGAGGGATCACGTAGGTTCAAAGGTCATAGAGGCTGCCCTGAATC',\n",
       " 'GCTTAGTAATGACCGTTTATATGAGCGAGGGTCAAAGGTCAACCTATTGCCTTATGAGATCCGAGGTCAAAGGTCAAGCTAGTCTAGTTTAATCTGCTTTATCTACTAAACGCATAATATACTTACGTGCTTGCGCTCACGTAACCTTTGACCCATGGCTCAACTTCTCTTAAATACATTAGAGTTTATATACATAAGAC',\n",
       " 'AGATCAAGGTGGCTGTTGCCTTTGATTCGCTTTAAAGGGTCAAAGGTCAACAAAGTATCACTGCCTGTGATCACTTGCTTGCTATTAATAACTGACTTCAGACCTCTTCCTTATTATTTTCTATGATTTTCAACTATCCGGAAGCGTGGGTACTTAGAGCGCTCCAGTGATCCAAGGTCAAAGGTCACCGACGCTGATCC',\n",
       " 'TGTGTAATTACGTATGAAAAACGGAAATGATTTTAAAGAATAAAGAAGCAAAGAAAACTGGTGTAAGCTTGTAGGAATAATTGCGATTTAAAGCAGAGTTACGTAAATTACGCAGGTGACCTTTGACCTCCGCTACCATGACCTTTGACCTCACATCGCCAGCACGTAGACGTTTGTGGATCAGTCATAATCATAGCCTG',\n",
       " 'GGCTCGCCACGAACGAACTTAGACCGCAACAAGCCCGAGCATGCAGGTAGGCCCGCCCCGGAAGTCATACTACATAGGGGATTGACATGAGGCCAAAAGATTTTACGCTTCCGTGTATGGCAGGAGCACGCTTATTATACTAGATATAGATGAGCGAACCTATTTCGGCTGCCGCTCGAAGAGTCAAGACGCACTAACCG',\n",
       " 'AAATGAGGTTGATAGAAAATCTCAGGACATGTTCCCGGTAACCTGACCCGGAAGCTGTTAAGCCTTCACGTTTAAGCAAGCGACCATTTAGGACGTAAAATCTATTTGTGCACTATCTCCATGGGACTCTTGCGAATTTACCCGGAAGCCCAAGCTTGTCATAAGCTACTTTGGATTAATTTGTTGGTGCAAGGTTGATC',\n",
       " 'ACTAGATTCTCTCGAAGATGTGTATGGGCAAATATCAGTATTCCTCCTACCTGGGCTCGAATGTCAATCTAATGTGTAAGGAGAATTATCTGTCCATCAGACTAAGCTATATCAGATAGCTAAAGCAACCACGGGAGTTTCTTTTGTTACCTAGCCTCGACTCTAGTAATGCTATGATATTGATATCTCATCCGCGGAGT',\n",
       " 'GGTTCTAATCCACATAGGTTCTTACATTGCTTACTCCCGGGCTCTCGAACAGTCGAGTCGTTCCCACTAGGTCGAAGTGAGATGTTAGCACGTAATCCGGTAGTATAGATCTAGTCATTGTCATGTTATATGTTCAGCACATTTTATAAGATCATGCCATTAAGTATCTAACACATTCTAAGTTATTGCCCGCAACAAAT',\n",
       " 'ATCCTGTTGTGTTTGCTATCAGTTTCGCGGATGACCTTTGACCCCAGCTAATACAAATTCGCTCCGCTATTCTGCGCAATGTTTGGTTACGTTCCTTTTAGCGGTAATTGTTCGCTAGAACAAGATGCGGTCATTTGTTCTAGGAGTTGTTAAAAATTCCTTTCGAGGGCCTGTCTTTGACCTTTGACCTTCAGGAACGA',\n",
       " 'AAAGTACGTCATTAAAAACAGGAGGGATGGGACTCTATAACTAGTCAGTTATTTGTAGTTCTAGTACGAACGCAAAGTGATGTTTCGTACTTCCAAACCCGGAAGTGAAGTAAGGAATGACTATCTCTGCAAGTAAATCCTGTCGACGATCGGCGACTTGTACACTAACCCGGAAGTTCTCCTCTCGGTTGTTTCACCCA',\n",
       " 'GATACGAAAGTCCCCGCGACGTACTTATCTGTTACCTCAGACCTCCTTAATCGAAGAAAACTAAATACAAAATTTATATGTAGACTCTTAAGTCTGAGATGTTGATAAGGCAACCATAAGTTGCGCGCATTAAATATCTGGTATAACCGCTGGTTGCTTTTTATCAGTATAATGGGATACAATCGTTCTATCTGACGACA',\n",
       " 'ACCGGTATCTGTACGGTTGTCGGCGTAGGTACAACTCTGCAGTTTAAAATTGGCAGTTTGCGTTCACAGTTTCAACGATCTTATCGTAAATTGAGCTGTTATTACGATTATCACCGTTGTCGAGGGGTCAAAGGTCACGCGGCCTTATCAATCATGGAGGTCAAAGGTCAAGCTCAATTCTTTACTCTTCACCGTTCTTG',\n",
       " 'TGTATTGTAACGCCAACGTATTTATTTTGTCCTGGTCTGTGATATAGTAAAGGTCAAAGGTCAAAAGCTACACATTTTATGCAAAGTCACATCATATTTGACCCGCACATCCAATCTATATGACCTTTGACCCCATATAACCTCGACACTCTTTGTAACTAATTTCGGGAGCTGGTATTCACTGAATTTCCATATAGACC',\n",
       " 'ACACAATCGTCGTTCCTACTTCATCAGCTTCTCCTTGGATGACCACCACAACACCAAACTATGTACTTTTATGCGACTTCCGGGTTGCACTTCCGGGTTCTTGTCATTGATTTATTTGCCGAATTATCACAGATCTATTGCGTCTAGATGCTATCGGTAATACGGAAGTCACGAGATAGTAGGCAAAACAGACTAAGACA',\n",
       " 'AACCATATACTTCCGGGTTTGGTAACAATCGTACCTCTAAGTAGAACAGTTAATTAGTTGTACTCTACTTTAAAATTCAACCAGATATTTACGGCCCGAATGCTAGGCGGAATCTATAAAATTGTAAAGTCGTTCATCATAAGACTCATCCATATATACTTCCGGGTTAAGGTCTTCAAGCGCTGTCTTTCTTTGTAATC',\n",
       " 'AGTAAGGAATATCGATTGCCGTGAGCCTTCCAAAGGCTGAGACCCAGAAGTTGCAAAGCCAAAAGGTGTCATAAAGTACTAAGGACTACTGGTTGCAATCACGAGAGGTGAGGTCAAAGGTCAAACTATTACCGAACAAAGAAGCACCTCTGCAATAGAGGTCAGCATAATTCACAAAGATACTGTTTGAGATAAGGACA',\n",
       " 'TTATGGGTTCAATATATAAAGTACAAGCGATAACACGTATCGTGTGATACATAATCTCTATTGTTCACAGAACCCGGAAGTGTCAAAACTTGGTTATGCTAAATGACTTCCGGATTGGGTTGTACCAGTAAGCCGTCTTGCCAATTACTGTCCTTGATAGCAAGACTTAAATCCCGAGACAATGGCTAATTTAGAATAAC',\n",
       " 'TTAAATTTGACGATTGCTTTGACAATATTAAAGCGTTCTTGGACAGCCCAATACTAGAGGAGTAAAAATTACATCTATAGCGGATTAAGGGTTGACCTTTGACCTCTGTAGGCTTGTTACACTTCAGTTATACGCCTAAAAGAGGTCAAAGGTCAATTACGAGATGCTGAAAATCATTAAAGGGCGTAAATTGCCCCCCA',\n",
       " 'GCCCACGATAAGGTGGACATTCCATTGGCACGTTGTAAATCAGGATGACCTTTGAACCCTGACTCTTCTCAACAGATATGAGTTTACATAATTCTGCATTAGAATCTCCTTTCACAAAAATCCGCACAGCAGCCATACCACTTGGAGCTCAATAGGAGGTCGTTAATAGTATTCCGTAATCTACATATTGACACAAACTC',\n",
       " 'CTACTTGCCTACGATTCTTGCATCGGCTTACATTACGCGCGACCACAAACACTACTTCCGGGTTCCCTGATGTAAAGGTCAAATGTAGAATGAATAGCTGACATTGCGTCATGTAAAATTGAAAATGAGCGATATTACGCACCCGGAAGTGAGTTTATGTTACTTAATCCCAAAAAGGTATGGCACGATCGGTTGTAGAA',\n",
       " 'TTGAAATGAGGCTCGGAAGTTCGATCGGTATTTAAAAATTTGCCCTTCCTTATCATTACAGAAATGTTTCTTGCGTATTACCAGCTGTATCTCCCCAGCTGGATCTAAGTTCTACTAGTATCATGGGCTGCTTGCTGTGCCGATTTAGAACAGAGAGTCGGATTACCTGGGCAGTCTAGGCCCGATACTACACAGCGTGG',\n",
       " 'AAATTAGGGGATTCATGGACGGGCCAGGTGAGAGGTGGCCTTCTGCAGGCAATGGAATAGGGATACAAAAAGATGGGATAGGCCGATACTAATTTCTATTTTTTTGAGCGATCGTGCAGATAAGCCTCTCCTTATGAAAAAGGTTAGAGGGTCTGGCAGAGAGGGATAGCTGCTTCTCAGTAACCATGAGGAGCAAAAAA',\n",
       " 'CATTATTTGTTAACTCTTTATTGAGGGCTAGGTCGACACTCGCTAGGTTGTGTTCATTATCGCACAAGAAACTCAGCTGATGACCTTTGACCCCTAGGGACATGCCAAACTTAGAACGCTTTCGACGTTTCACTGTACTTTATCAATGTATCATCGTATAGCTAGCCAGTTGCAGTGTGTATACGGTGCGTAACCACGAA',\n",
       " 'GAGTTACATGCCTTCTCTATTTTCAACCCGGAAGTGTCGAGTTCGGTATGCACTGGAGTTTATTGTTAGATCTTTATCGCCAACACGAGTATGCGGAAATCACAGCTATCACAGGATGTCAGTGCTGCCCCTGAACCGTTACATGCATTGGGATGGGTGATAGGATAATACCAGTAGCAGGTACGTGGCAAATTCGGCAG',\n",
       " 'CAGTTGAAACTCTGCTTTCTCGTATCCTTTCACCGCGTTCTTTATTCATCCACATGTGGCATATCCTTCACCGGGCTATCTAAAATTTTCCTCGAAAAGCTCACAACTGTTTTTTCGGACCCCTACTAGATGCTAATAACCATCACTATCAAACAAGACAACGGACCTAAACAGCAGACGACACAGTATTAAAAACACGA',\n",
       " 'GTGAAATACCATATATTTCCAGATATGCATGCTGTTTTTTTTTGGTTTTTACGTCAGAGATACTATAGACATATCAATACTTGACCTGCTCGACTAATCTGATAAGATTCTAAAGTCCGCGCATTTGCTTCTGCCGCAACCACCTTTTGTTTCTCAACAACCCGTCGTCTGGTGCCTAAATTGGGGCGCACTTCCGGGTT',\n",
       " 'ATAACACTGCCTGAAGATAATGACATTATGGCCGAATACACTTTGCTGCGCTAACCATGATTCGGCTAATCGCTCCTACCCTTAGAGATAGATGCAAGTCCTGCGAACAAGGGACCATTCATTTGACAGGCTAAAGGGCTCTATGTGCGACAAATTCCGTACGTATTCACGTATTCTTTAAGATAGTTTCTAGGACTTGT',\n",
       " 'TGGTTGGAGATATGCCTTCTTACACGGGATCAGGTTGGATGTCTAATCTGGTATATGAGAATAGCAAGTACCTCATATAACTCGGTTTGTGCATGTTTTAAACAATTTCACGCTAGGTCTTGTCCTTAAAGACCTAAAGCATAAGTTATAGTTGACCTTTGACCCTGACCACGATCGTTTACAACATTAAACTTCAAAAA',\n",
       " 'AACATAGTCCCTAGACAATGTGTCATATATATGTTACTCGTTAATCGAAATTTAGACATCAATAGGATTGAGCCTTGCAATTTGTTGTTTCTTCTTTTGAGTCTTCAAACTCGATCAGTGCGGTCGCACTGAGACATTATTCAGTGGAAAGAAAGTTTATAATGGGCTGTGGAAGATAGTGACTCAAATGTCAAAGACTA',\n",
       " 'ATGAAGTATAAGATATGTCGGAAATTGGATTACGTAACCCGGAAGTCCCCAAAAATAACAAAACTCTAGGCATGTTGCCCACTATACTTCCGGGTAAAGTATGCCCTGACATTAAGAAGTTACAGAAAATGCCTAAGCAGAGAGGCTGAATTGTTTACCATTACATCGGACCCGGAAATAGAGCTAATCACCATATCATT',\n",
       " 'AATCAAGACACTTCCGGGCTAAATCCCTATAACAAAAAACCGGAAGCTCCTTATCTCAGGCCTTCGATTCCAATATTCAACTATCAGTCATCAAGGACTAATGAACAGAAGCATGACAGCGCTATTATAAGAAACCCGGAAGTTGTATGAATCCTAAAATGTTGAGTACGGCTTAAGATTCTGAAATTTGACTCCTTATC',\n",
       " 'GGTCGATTCAGATGCACTTATAACGCTCTTTAAGTATCGGGTTTACCCGGACAGAGAGGGCTCCAATCAGGGATATCAGTAGCGCGTTCATATAACAGCCTAGAACCATCATAATGGGACCAGTACAGAGTTCAGCACTCTGATTTTTATATCAGTAATTAAACGTCTATCAGGTATATTGTGCGTTAATAAACTGCTAG',\n",
       " 'TCCATCGTTGACCTTTGACCTCTAATCTCAATGTGTATTTTTGTTTAATATCAACTTGGACCGTTATCGTTCCGGGGTTTCGGTTAGGCAAACCACACTGAGATGGGGGTCAAAGGTCAACTGTATCTAGTTGTTATGTGTAGAACTCTTGACCTTTGACCTCTCCCAAAGACGGCCATGTCCGGAATTGCTCACATGGG',\n",
       " 'TTATGGAGACCGGAAACCAGTCGGATTTCATATATTCGAAAAACCATTCGGTATCTCACTGTTATCTAAATAAGTCAGTATCTTCTGATTGATTTCCACTACTGTTAATGGGATCATATACTTGCAATCCAGGTCAATAATCGAATGAAGTGATAAGGGAGCGGTAACGGGTTACCTTTATAACATTACATTCCGCCGGA',\n",
       " 'TATACCGCACATTATTACTTTTGATGACCTTTGACCCTATCGCGAATTAGACTTCTTCGTTATACTCCTCCCCGAATTTGAACACGACAGGATCTAAGACTTATCGTAAGCATATCTTATCAGACAAAGTGATAACCTCACACATCATACAGAGGTCAAAGGTCATGGTCTCACTCGACCTTTGACCCCCTACAGTGGGA',\n",
       " 'ACTAGCTGGTATAACTTACATAAATTTATACAAAATATCGTACATTGGGGGCTAACTCCAACGGGACCACTCTGTGTGGTCACTTATAGTTGACCTTTGACCTTGATTCGATGCGGTGGCAACGGAGGTCAAAGGTCACGGATCGGATAATCACGCGGGTGTCCTTACGAGATTAGCAAAGAAGGAGGGATGCTTCTTCA',\n",
       " 'TTATATTGGTTATGATACTAAGCTGGACCGTCCCTTCCGACTTGTTCTAGTCATATAGTTTTATACGGGTGATAAAGGTAAGTCCTATATGCGTCTTTCGCATTTGGGAGTGATAATATTAAATAACAGTTTGTAAAAGGACGTGGCTAATGCAAGATAGGCCGCCAGAAGTAACAACATCATTACTAGGAACCACTCTT',\n",
       " 'ACATTCTACTTGGACTACGGCTTTGAATACATGATAGCACTCCTGCCCTATCTCATAGCATTTCCACCAACTTACATGAATTACTACCATTATCTGAGTTGTAAATTATTAAGTATTGGTGTTGCTATTCATTAGTCCTAATATACTTATCTGAATTCTCACATTGAGGTTCACAACTTTAAGACTAGTATTCCTAACAG',\n",
       " 'GTCGATGTATCGCTGGATTGATTAGATCGCCTTCTTATGACAGTCGCTCCGGAACAAGACGCGCGGGCTTATCTGAAAGTACAGTATAAGGGCAAAACTTGAGGATTACCTAAGTCAGTGCTACGCACTGATAGCCCAGCAAGTTTCAAGTGTGAGCGGCTAGTTACGACTCCGTACTGTAGTATTTTCTATTGGGAAAT',\n",
       " 'CTGCGCATTATCGGCCTTCTATAAAGATTAACGCATGACGATAAATTAGGTCGAAATTATTTTTCTTGGAGGTTTTATCTCATTAATCTAGTTGGTAACCAAACTGCATGATACTCTGGGCTTAAACAGGAGTGAAAGCAATAGTCCGCCTATATAATGTTTAAGTTGACCTTTGACCCCATTCGAAAATAACAGGTACC',\n",
       " 'ACCGCCGGTCTCTCTATCCGGACATGACCTGCCACGCTTTTTTAAGGTAAACGGACAGGATACTCACTGTATGGCACAATGTATGAATTAAATTACCCGTCGTTCTCTCTCGTGCTTATCTGCTCCCATATCTGATAAATCACACGGAGTGCGATCAGGGATCTGACACAAATTTCGGATGATCTTAAGTTCCTTGTTGT',\n",
       " 'AAACTCAGGTGGGGTCAGCTATCAGCATTCACGGGTACTCAAAACCATTAAAGAAACGTACAAGAACTTACGGGACAATTATGGATCGATCTAGGTACATTCTTCACCTAAGCATCGTAGGAGACTTAAAGATTGCATCCTATCTAACGTATGCTTTATATAGCTTCATCGTCACAGAACTAATATTGTTTCTGACGGCT',\n",
       " 'ACTTCAGATGCTGTTCTTGTTTGGATTAAGACACAGTGTAGGGGAAGACAGGTCCCCAGCAACCTGATAACTGCACCGAGTAATAGTGCGTGCTTCTAAATTTGACTTCAATGCTAGATGATAAGAGGCATGTAACGTGGAGTCTCCATCTACCAGTGATATGCGAAATTAGTCCCGATCTACAGTGAACTTCCTCAATT',\n",
       " 'TACCTTTCGTTTCCGGAAATGTGTTATGGGCTCCTGAGGGGATTTTGGACTAAACTTTTTGACTATGATACTTGGCGCAGGCTACATGACAACCATGTTACGCATAAAGACACGAGGTTTGAAACGATAGCGACGAAGAACGAGGATTCCGTCTATGCGACTCATTGTTGTCACACCTTACCTCGTTACACGATGCCAGC',\n",
       " 'ATTAAGATTCCCGGGAGTAAGATAAGCTCATAGGTCAACTGAGAGATTTGTATCCCATACCCACGTTTTTAGTATTGTATTCTACACTTCCGGTAACCCCGGACTCCGTTACGAACACCTTTAATAAGCAGAAAGTAGGTTAAACATCCTCTCTTTAGGTAATTAACAGTAGAAGCTCAGAAAATCCTAGGGGGTCGGAA',\n",
       " 'CACTTGACCTTTGACCCTTGAGCCCAGAACATATTCGATTCAAAAGAAAGAACTTATAAACCTAGCTCCTGAAAAAACAACGACGTGACACCTACCGTTCCGTTCAACTCTATACTGGATAACTAAATAAGTAAATGGCCGTGTAACCAGAGTGGATTCACTGTGAGGTCAAAGGTCATCGGAGACTATAGGGTGGTTTG',\n",
       " 'TACATGTACACCTTTAAGGGTCAAAGGTCACCCATTCGCGTAAATAGTATTGCCACAATCTCTAGCCTATTTTTGGTGAGTGATACAGTCAGAGGTCAAAGGTCATCATCGCGGCTCTATTAAAAATTCGTAAGGAGGTCCTATGACAGTAGTTAAATCATCGCGTAATGTATCAATGGTCCTATATATAGCGACTGCGA',\n",
       " 'TCATCGAGCCAAGATCGTACTGTTAACAATACCATATTTAATCATTGGACTAGTTGAAACTCCACCTAAAGTGAAAGGTGTGTAGTTGTTTTTCATCTAGCCTGGGGTCAAAGGTCAACTTATATATCCTCGTGTGAATTTAGTTATTAGTTTTTGCGAACACTACCTCCCCTGGCGTGCTATCCTGTTCGTTGAGACTT',\n",
       " 'GTTGCTACCCAAACTTTCAGGTATATAGAGGGACAAGAGGTCCCTTAATAAATAGCTTGATAGCCGAATAGTTATGTAGTGATAAGCAGGTCGGGTCAAATAGATCGTTCATGGTTAGAAGATACAGTACCGCCCGCCGCGTGTACGGCCCTACAGTTCGATTATGCCTGTACAACTATCCTGGTTCTCGTACTTGTGGA',\n",
       " 'GTGTGGGCGGAAACTGTGCTATGTAGGTACATGCCCTCATTCAACCTTAATCTTCTCTATCAGTTCAGATTAGCGATCCAGTGTACAGTAGTTTCTAACTAAGTGCATTTGCTAATTAATTCGGATAATTGCTTCGCTCCTATCTAGTTTTTATAGATGGCCCTGCTCAGACGGAGATAGTTAGAAATATAGCCCAACGT',\n",
       " 'GGAAATAAAGGAAGATCCGTTGCAATTAGAACCCGAGATCAATTACTATTAATATCGTATTTATTGGACTCAATAGTGCCACTTATTCTGATCTGATCCAAAACCGGAAGTGGATCCGTCAACATATAAGAAATAACCGAGCTTACGTACTTAAAATGGCATCTGTCACTCGATCGGCTAACCTAATAGGTGAGACTTAA',\n",
       " 'AGGGAGTTGGTTAGGAATTCATGAAAGACATGAGCCTAACACCGATATCATTTTTAGAGGATCAGTCATGATTTATAGGTGATTTTTGAATACCTCCAGTCACAGTATTCCTGATAAAGAATGGAGATAGGGACTAATGGTAGAAGTATCCTCGTAGACGCGGTTATCTTTAAGACCCTGGTGACTATGAACGGCTACAC',\n",
       " 'GTACTAGGTGCGGAGCTCAATGCAAGTCGAGACCTTCGATCGATGACTTGGACTAAATGCTTGCGATAAGTGATACACCCCACGCGAAGCTTACAACTATAGCTGCGACGGATAACTGCCGGTATGAGATAAATTGTTTTATACCCCTTATCTGTCCTCACATCAGTAATCTGTACGAATAGATCTAGTAATTTGACACC',\n",
       " 'GCCTATACCCATGCTATTTGTAAAAACTTGCTTGCCCGGAATGCCCATATCTGGTGTCGTAGTATTAACGAACAAGGGGTCAGCAGATAGACCATTCGAGATATACCTCGCCCTATCTGCAATGCACGAATGGCTTCCTAGCTAAACACCTGAGGAGAAATAGCTCACATCAAGGCTACAAGAAAAGCTACGAAATGTTC',\n",
       " 'AGCATGAGGCGAATCCGAGATAATAATCACTTTACATCGCGTATTGAATAGTGAGATCAGATAAAGACCATAATCTATTCCACTGTCTGGCTTATGTGTCAGTGTTAATTTGCTCGCTGGCCGCAGACATACATAAGGGTGCTCCGTGACTGGGCAGATAAGGATCGGCGGAAGTTCCCGTCCATCAGGCAATTTGCACG',\n",
       " 'TCCCGTCGCGCGGCAGATCGGGTATAACATTCCTGATGCGTAAAAGTATAACTTGAGACGAGATGATAAGCACGACACTTCCGTGTTGTATAAACACCAAGACAAATGGCGTCATAGGTGTGCGCGGCGCATGAAGTAATTGACCTGAATCTCTATGTATGATTCTGACATATTTTAACATATAAATACATCACGCTCAA',\n",
       " 'CCCTGGGGTAGACCCAATTACTTCCGGGTGCGATATTCAGCTCCCTGACACAATAAAAATTCTCATGGTAGATCATTGACAATCATGGCCAGCTAGTAGATGCAATGGTGGATGAAAACGCGGAAGTGACTTTATAAAATAGACCATTCTAGTGTAGATCAACGAAACTCAGGCTCACGGCGGTGGGTCATATTTTGCTA',\n",
       " 'TCACTCAGGGTCAAAGGTCACCATACGTGGGTTTAATTCATAGGTGACACGTTCAAGTAACGGACGGGACTACTATTCAGATAGGCGTTGTTCCAATATCCTCTATGAATTAATAATTGACAGAGGTCAAAGGTCATATGTCCACCCTGCAGATAATAAGATGACCTTTGACCTTGGAACAAACTCGGCTGAATGCCGAT',\n",
       " 'TGGAGTTTCGGCCCAGGGTTATCAGTTTGGGCCTTTATGTACATTAAGCAGTATTAATTCTCACGTTGTCGGGTTGATAATCTTATTAACAGATAAGTGAGTCTGTTGACATGTGACGTCTTTCTTACCAAGAATCATTGAAAGTAGTACGGATCACGCAAACCACCGTACAATAGTATGATCTTTGCGTACTCGTTCTA',\n",
       " 'CCGGATTCGTGAAGAGTCTTAAGCGTTATGCCTGAAATGTCCCCACTTTGCACTCATACACATGCAGGTAGAAATTTTATGAAATTCGATTAAGGGGCCCTTGTTATCCGCCTATCCTCTGCTACATCTTACATGCGAGCATGTAAAAGTCACAAAGATCTACATTGTTCTTTGCAATTTAAGCTTAGTGCTGGTTCGGT',\n",
       " 'AATAATACCTTAGAGCTTTAATGCAACATCTACTTTGTACCGGGGGTCAAAGGTCATCGAAACTAAGTCACCGAAAGGTGAAAGGTCATGAATGTTTTTTTTTGTAAATCACGTATTATCTCAAGAGCTGGTCGACCCCATGGTATGGTAGTTGTTTGATAAGGGTGAAGGGTCATGGGAAGGAAGTGTACGATTATGGA',\n",
       " 'TAACAGTCACACCGATCGATTTTTTCGTTGTTAGTTTAGATCAATACGAGATCAGCACCCATATCTAGTATGCCCCCGGAAGTGCGGTTCCCAGTTACGCCGAGAATCAGTGGGTCTATAGGACATGTCTAGTTGTCATTTAAATAAACCTAAAAATTGGACATAGTGTAACTTCTAACCTCGAGCACTGATATAGCGAT',\n",
       " 'ACTGACCTTTATTATCAGCTAATTATTATGTGATAAATACCCATCCCGGATAAAGGAACCTCAGATATGATAAATAAAGGTGATTATAGTAGCTTTCCATAAGAATCAGATACATCTTGTTGACTGGTACAATCCTAAACCAAGTAACTCACCTAGGACGACACAAACGATCACGGCCGTAGTAACGTATAGGTTAAGCG',\n",
       " 'ACCTCAAAAAGCGTTTACTAGGTGCAAGCACTTATCTCTTTATTATCTTAGTTTTGTCTATGTGAAACTTGCTTCACAATTTATCTCTCGATAAAATCTAGAGATAAAGGGGGCTCAGATATAAGAGAATAACGACCTTTTGTAGTTTTTTTCTATGAGTGCCATGTATATCTTAGACAACCTTTGTAAATGTTTAAACT',\n",
       " 'GTACGTTATTAGAAGAGAGTCGGACTTCTGTATTTCCGGGGTCGCCAGGAACAAATGCAGGTTTGTAATCTTTGTGTGGTATTGTAGGGGATTTAATTGTGATACTAATAACCAACCATTAGGAAACCGGAAGTGGTTTATCTTAAGCCTAGTTGGTTGAACGGCATATAATTCACGTTTAGATGCACTTATAGCACATG',\n",
       " 'AGGTTTCCCGAGCTACACGGTCGAACAAGATTTCAAAATATTTGGAACTATGATTGGTACCGAAGACTCACGCAATTCTATAACAAAGCCATGGTAAAGATATCGTTATCAATAGCCTCATTTTGGTGTAAAAAGATAGTGGGGGACTTCTCCTTACCTAGTCGCCTGTAGTATTTTGGCTCCGCACCCAGATGATTTAT',\n",
       " 'CCCCATTTGGTTTGGTACAGTATTTTTCAAAATCCAGTCGTCTTATCAGATAATTTCGAATACCGACATATTCTACAGTCACACGAAGTATTTAAATTATCGAAAAATATTTGTTTATTCAGCCTCTCTTGAGAAACGTATACTAAATTCTTGGGGACACTGTACAAACAAATTCTTATCGGCAGACACTATATCATCTG',\n",
       " 'AATCCGCATCACGGAATGTCTCCGAATAATTGTAAAAGAGTACCTTTGGAGTTACCTGATGCTAGACGCCCTAACCTTGTTCGCAAGATCCTAAGCAGTTAGATAGGCTGTCGATGACCTTTGACCTTTCCGAGACCCACGCTCAGCCAAAAAGTGGTAGGAATAATTACAAGACTTGATGACCTTTGACCCCACGTAAT',\n",
       " 'TTTCCGTACTTGGCATTCAATGCACTAGCAAGTAATTACGTCATACTAGTTGATTACCATATCGTAGTCGATTTAAATTAAATATCCACTCATGAGAGGACTTATCATGTTGCCGAACCTACTCTAAAGATAGGAGTGGGCGGTAACCTGCGATGATAGCGGCTACGCACCGAGGTCAATGTTTTTTATATGACCAAAAC',\n",
       " 'GTAAGGTTACTCACAACGACAGATAGCAGTTCCAATTAATTTGAAAAAAGAGTGAGTAGTCGTCTAAAAATTGCTTCGCATATCACGTTGATTGCGTGTTGTCAGATAGGAAGCATCAAGTATCTTCTGTGATTGAAGTGTTGCCAATCTAATGATGATAATGCATTGTTGCTTCGTTACACACAACACCTAATAAGGAC',\n",
       " 'ATTATCATCGGGATTATAAGGGTCAAAGGTCAAAGATCGATATCAATGCCTCAAGTTGAACTTTCTTTATTTATTAGTTGACCTTTGACCCCCTGCATTTCAATAATTGTTGACGCAAACTTTGAGTATTGCCATCACGTCAGTCAGCAAAGACCCTACGTAAAGGGGGGTCCGCTTAAACCTATGCTACATTATTTCTC',\n",
       " 'ATTAGACCTAACCTATCGATGCACAATGATAAACAGGGCCACTTATCAGGCAGCGAAATTTATCAGGGTAAGGGACAAAATATATAAGGCCCTTATCAGGTAAAAGATTCCGCCGGCATAAAATAACAATATAAGAAGTTAACATCTTCCTATCTCATTTATTAGACAAGGAAAACAAAATAACAAATTTGTTTCAGGGT',\n",
       " 'CGTGTTCTCTGATAAACCTAATTAATAAGTTACATATTGAAGCGAATACGTTATAGATTGCCTGACTGTTTCCCGGAAATGATTCATAATTAGGGTGAATACAGACATGAGTCGTTCGTCACACTCAATATGGAGTGTTGTAGGAGAAACTAATAGAGACTGTATGTTTATAATACGGGGAAGAAGCATATTCCATCGTC',\n",
       " 'ATGTGTAACAGACTCCTCCGGTTTGCACCTCACCCGATGGGCATGATCGAGGTCAAAGGTCATTCCCCTGCACATTAAAAAAAAGGGCAATGGTCCTTCATTTCTGACACTTCTACCGGTGACCTTTGACCCTTCGCACTTCAAGCGCTCCGATTTTTTTGCGATGCGCACGAATATCGATGTATGTCGGGACTCCTTAT',\n",
       " 'AACGACTTATTTGTGTTCCACGTCAAGGACTTATCTGATATAGTAAAAACTTCCGGCTTGATTGGCCCCTCTGGGACTGAAGATTCACGTTTTATACCATCGTCACTTCTCTATGCTAATTGAATTAAACCCGGGTTGTCGATGCCATTATTGGTTCATGGTTACGAGTATAGCAGTCTATAGCGTCTTAAATGATTGTT',\n",
       " 'ACTAGAATCAGGTCAAAGGTTATCCGTCCGAAGGACGGGTCAAAGGTCAACTCAGTTATGCGTATGACCTTTGACCTCTCCAGAATTTGCCCTCGGGCTTTCCATGGTTCTGACGCTGTACTTACAATTTGTAGAGAATCGCCCCTATCAAAGTCTAAATATACGATCCTGTTGGTCCTGTAGCAACATTCTCACAGAGA',\n",
       " 'TGAGTCGACCGGCATGATAATAGAAAATGTGTATATTACTATATCGTTCAGAGTGCATCACAAACCACTTATCAGCTTATCACCGAGAATGGAAGTTATTAACGATCCCTCTATTAGTTTAAATATGCACTTGGTTCGTCTGATAGAAGGGGTCTTAGTATCTTTATATTCGAACGGGAAATGATAATATCCCCACATTG',\n",
       " 'TATGTTTTTTATCCTATCTTCATTCAAACGCCGAGCAAGACCACTGAACTGTCTTTGAAAGTACTGCGTGTTAGCTGGGAAAGGAATTAAGATCAATCGCTGTAGAGCGTTTGATAACCCGTGGTTTCTTAATAGTTGGCTGCGCTTACGCGTTAATAGCTAAATAGACCCCATGGCGAGGCTTAATATAGTACAAAACG',\n",
       " 'AAAGATTGTTGAATTAAAACCATGTGCTACGGCTAAATATAAACTAAAACCCGGAAGTCACCCTACTAGTATCGATCTTTGTACAGTCCAAAATTGATAATTGGATATTTTTTTAAGCTTTCCGTATAAATCAGAGTCTGGGTTATCATACTAGTGGTTCCAAAACAACCACTTCCGGAATTTCTGCTAAATCACTGACC',\n",
       " 'GAACTGTAGAACAATACTTCCGGTTTAGTTCCAGCGAGAGAGCTATTTATCGTCGAATCTTGAATTCGATGAATGTCAGCCTAGTAGAATAGAAAACTAGATTAGGATACTATACTTAAACTGCGCTCATGCGACACGACTGGGCCAAGTGTCATGATTTGCGTGAGGCCCGCCTTATAGCCATTAAATACCATTTCTTG',\n",
       " 'CATATTACAAAAACCTAAATTCTGTTTGAAAGATTGTGTCGAAATTCTGCCGATGCGCAACCTGGGGTGTTTGTGAGAAAATCAGATAAAGGGTCTCATCGTGCAAAATCAGATAAGCCAGTTCAGATAATCATTACGTATGCCCCAACATCATAAGATCTAATAGACAAATTGTGTTGTCATTAATGATACGAGCGTTA',\n",
       " 'AATTTTGCAATTTAGTTGATTAAGCGTTAATGAGTTATTGGTTTCTTCAAATGTGGCACGCTACATAGTTCGTTTCCGGGGTAATTCGTCGTGGGCTTTTAACTCACAGGGCTAGCTCACACTTCCGCCTTGTCTATAGATGACGATAACTTCCGGGTAAAACGTACATTGGAATTCCGTGAGAATAAAATATTGCATAG',\n",
       " 'ATTTTAGTACCTGTGCAAGAGTTCCCATGACCTTTGACCTCTTTATTAAACCCCAGATACTCCTTCATCCCAGGATTACGTGGCCCAATTCTTAGGTCAAGGGTCAAGTTTATCGAAAAGCGGAATGCGTTTACATCGGCTAAGAAAAAGGTCAAAGGTCACGGTAGTTTCTAGCCCAGAAGGATTTACCCAATACTTAT',\n",
       " 'ATCCTTATAGCAGTTTGGGTTGGATCGTGTATTATTTTAATTTAAGCGCCCTGAGACTGTGTACCAACGTCGATCACATCTTTAACCACTTGACCTTTGACCTCGGATCGTTCGTGTGTCGACTATGATAGGATTGATACATGTATGCCTATAACACATTATAATTGAAAGGATCGCAGTATGAGCCTGCAATTCTTAGA',\n",
       " 'ACATGAACAGCCCATTTGCTTTCTTACCCGGAAGTGATAGTATTGTCCTAGTGCTACGTTCGGGAAGTACTTGAAATTTTCCACCTGTGCAAGTTCCAAGGTCAGTCGCCACATCATGTTAGAGAATCTTCCGCTTGCCCTGCCATCTGGACCCGGAAGTATATTCGGGCTCCCATGGTCAATTACCAGATTAAGCTTCA',\n",
       " 'GTGCTAGAACCTGTAATTACCTCAAACATAACGCCTTATCAGTCTCCGTAGACATTTCATACTCCCTAATAATTGGAGAATCACAAATGCTCAACACAGGGGGTTACCCAGCAACCATAGGGTTTAAATTAGTTGGAAATAGAGTGGGACTGGGTACGATAAAACATGTAGCTAATAGCCCATATCTTCCTATTCTAGAT',\n",
       " 'TAGGACTGTAGGCTCGACATTATTAAAACCCGGAAGTGTAGCCTGAATAGTTAAAGTTACAATTAAATTTTGATCTGCTTTTATTAAAATCTAGTTAGAAGTCATTATTTCAAACCAATTGTATAATTATCCCACCAGTGCTATCACGCATGAACGCATAAATTTATCCGGAAGTCTGCCTCGAAAGTTAATGTCTCTCG',\n",
       " 'GTAACAGTTGTGTGAGGTCAAAGGTCATGGTTTATAAGGATGACCTTTGACCCGTGCAATTAGAAGACCGAAGTTCAACGATTTCTATACGGATACCGGTGTTTAACTATGCTAACGGACGTTTTTCACTCGCTTATCACCACGCGGATTTGTTTTGAGAATAAATGGGATGAATTGTCGAGATCATTCATTGACAATCG',\n",
       " 'TATGCCTATTTCTACTAGTCACAGCAGTAAAAGGGAATATTGTTGAAACATCCTTTTCCATCGATAGAATAATTAGGATATTGGTAATTCCATGGGTCAAAGGTCATGAGATAAAAGAATCCGACGATTATCGTTTCCCCTGGATAACCCATTGCTCCAAAGATATGTATAAAAGGTGACCCTTGACCCTCGCGAGAGTA',\n",
       " 'TTTTAAAGTGCAGATAATGTCAGCTATTCCGCCTACTTAGGGGTCAAAGGTCAAATATTTCTGAATACATAATTTACAATATTTTCGGAAGTTACGGTGTATGTATGACTGTGAACAAAGCTTTCCAGTGCTTAATCTGAGAATATCTACAATTGAAGCGATTAGCGTTCTTGAGCTTACGATTCCTCCTGCTTTTTTTC',\n",
       " 'CGATGGTATATGCACTATATGGAGTACAAATTTCTCGCGATATGCTGAAGACACTGTACGAAAACTAACTTTTAACGTTAAGAGACACCTATTTCCGGGTCCGTTACGACTGGCCTATTTTCTTCACTAAGTCGCCGCCATCAGCGAACATATTATATAGGAGGGGTGACTAGCATATTAGTATAGAATTAGAGATGAGG',\n",
       " 'TGAAGTATTATTTTCACAATATTGGAAGAGTTGCAGGACTATAGCTTTGTCCGGCATCCTGACAGATGGGTCAAAGGTCAACAGACTATCTTATCTTTCCGTAATCGCAACTTGGCAGTCGGGCAATCGGATGTGTCATGCAAGGAGACTGCAGAGATATTAGTTATGGCAATACAAATCAGGTTCCTGAAGGCATATAA',\n",
       " 'TTCGAATTACGCAAGAACTATTGTCTTCATTTCCGTGTTTGGTCGGTCGCCCCTCTTACCAGATGTCCACCGAGGTAATTATAGTGTCTTGTATTCTGGCTAAACCCGGAAGTCGCCTCGGTGCTCAGTCCTACCCTTAGTTAATTGCTCAAACGTAGCATCTTTCATCCAAACCCGTGTTTTGGTGTAACTTGAAACGG',\n",
       " 'CCTATTCAATTGTGCACCCTCTGTTGGCAACTCACTTACTCGTCTACGCCTGTTTTTATCAGACTTTATTTTGGAGCATATGTTGGAGTTGCCACTTCTCCGTATCCGCTAATGTTCTTTGTACATTAGCTCTTGACACTCCAGTCCTACATACTGTCCTACGGACTGTGATCAACAATCCACTATGAAGCCTCCAGTCA',\n",
       " 'TCTGGCCCCTTAATTGTTCACGTATTAAGCTGATTTCACGCGGCAGCGCGCACATCGAAAATCGGTCCTTATCTGTTGGTAAGAGTAGTAGTGGGGCGAGGGCATAGAGATAATGGACAATTTGTCGTATTCTACAATGCATCTATCGTGAGGAGAGATTACAATTGGATTTTTCTTGTGGCTATTTAAAGGATTCGTAC',\n",
       " 'GATCTACATCAGAAACAAACCATCGTACGAATCACCATGTATGCGTAACTAGGACGGTCCACCTAATCATGGTAACTTACAATGGTCCTGTGGGCGGCTACCTTCAGAGCAAAAGTTAGACGGGTCTTATCAAATAATAAAGCCGATTTCGACCTTGTAACCAGGCTATGATAACTAGGGTTATCAGTACTTCCATATCC',\n",
       " 'GTACATATTTCAGGGCAGGTTACACGACAATATTTGACCTTTGACCCCCAGGATTACATTTTACGCCATTCTACGATCGATAACTACTCCGATTATTAGTTTTCATATACATTGACTTTGCCACAACCAACTTGTATGACCTTTGACCCCTGGCAGCTGCTGCCCTTAAGTCTATAATTTAAAAATCGGGACCGTCGTAA',\n",
       " 'CTTTCCACGTCACGCGACGCAAAAAATTGCATCGGTATTTATCGACAGTTCATTGACACATGAGTTGTGCGGGTATTAAGGGGTACCCTTATGAGTATGAAAGAAAACCCGGAAGTCATGTCAGAGCTTTTTTGATAAAACGAATCATACGAACAAAATCCGGAAGTCAGGTGAGATAGTGGAAGTTGCCACGTAAAAAT',\n",
       " 'AACCGTTCAGGCTAGCCGGATTGCGTATTACACCTTCTCCAGTTATGTATATGGGGTCAAAGGTCAAGTAACTAATATCGCTATCTCCGCATGATTCTGTACTATTCAGAGTCGTATCAAGGATAAGGATATTCTCCACGGACAATAAATCTAGATTGTCATGAAAGCATAAGACCATCGTGACCTTTGACCCCAATGGC',\n",
       " 'CTCTTGGATTTAAGTGTGGATCGCCGTATATAACATTAGCAATTTGCTAAGGTCAAAGGTCATAGACCGAGATTCATTTATATCACCTTTCACCCCTGCCGGAATAGCATACTTAGTGGTGAGTGCAACATGACCTTTGACCTCAATCCTCTGGGACTGATATTATGTGGCTCGAAGTGTGAACGCAGATTGTCGTAATG',\n",
       " 'CAATGGGCTATTATAAGGACGGTATCCGAAGCCGTAAAAATATAGATCCGAATTCCTCTGACCAATTGATTGAGAGCTAACGCTTTAACACTCCTTTGAGGTCAAAGGTCAAAGGAATAAGATCAAAAGCAGTGACATTTAGCTCTGTTGATCATTTGCGACTTGTTAGATTCATTATATTCGAAAACGAAGAGTCAGTG',\n",
       " 'GTTTTTATGTAAATCACTTGTTGGATTACTAATAAATATTCTAACGAACTTGCTAACGATGAACCCAAGTTGACGGTCTAGACAAGTCAGTAAGTGTTCGCACAGGACACAATCACTTGAATAAAGTTACAGTAACAATATATGTTTTGTACTAATACTTCCGGTTGTGGAACACAGGACAGTGACATCGCGACAAGAAG',\n",
       " 'ACTTAAAACCGGAAGTGTTCACACGGCCATTATAATAAGCTATTTATCATAGATTGCACCAGGAAGTGAAAATTAATTTACTTCCGGTTAGATGATCGCGCGGTAAACCTTTTGGTTTTCGTTCTTTTGATTCCAGCGATATGCATAGTGCTGCTCTGAAACTGAGTCAACGTATATATATGTCCATTTACTTACTTGGC',\n",
       " 'TAGATATTGAGATCGTAAGGTCAAAGGTCACCGACAAGTGTTTTGCTTATTAACTAACTCGATTGCTTGAAGGGCGATACCTCGCACATTTCTTTTTGGAGAGTAAGGATATCATCTCGACCATCTTGATGAGCACAACTCCATGATTGCTTTGCATAAGTTTTAAGGAAGTTCTCATCAAAAATAGCACTATTCATCTC',\n",
       " 'CTCGAATTAGCCCTTATCAGAAACATTCAGAATAATACATCGTACAAAGAACAAATGGGGCAAATAGGTGATGTATGTGTTTCAATTACGCATTGAACACTGTAACTCTAATGAGGAGAGCAGTAGGAACTAATGGAAAGAGCCGATGGCTCATTTACCATAAGCGGGCGTCCTGAAATTTAAATTATTGCGTCCGCAAC',\n",
       " 'ATTTTTCTCTAGCCAGAACTTACTTCCATCTGATAGGACCAACAGGCTCACTCGTTTGACTATGTAATAATGTTGCCTTGCCATTTTGACCAGAACAGACCACAAAACGGAAACTCGTGTGGAATGAACCTATCAGTATCATAACAAGCTCGTGAGAGATTTACGACGGAGATTGTTAACAGCAAGTACGCGTTGTCAGG',\n",
       " 'GAATACGGCTATGGCTATATAATGACAAAGGTAGCTGTTTCCCGCATTGTTATTCTCCTAATGTGGATTTCCCACCAATATTTATGTCTTTACTTTTATGTTCAAAATAACTCATTGGTTGTCTTTTAACGTAGAAAAATCAGTAGGAGCGACGGATATTGAGCTTGACCTTTGACCTCTTACAGCTCTAGTAACATAAC',\n",
       " 'TCTGAACATATTGTCAGAATTTGAACACATGACTCAGCATGGTTACGATACTAACGTGCAAGAAAGACAATACTCCAAGTTCGGTTTGCATTTCACAATGTCAGAGGTATTTGGGTCAAAGGTCACGGTGAAACGTTGACCTTTGACCTCGAGGACTCCCGTAATCAGGTGAACCTGTTAATACCCAGGGGCGATCGCCA',\n",
       " 'ACTACGGTAGATTACAAAGTATGCCTCGTCTTTCATGTGTTGATGTGTCTAATCAACACACTAGTATTTATGTAGATCCTTACACAGTGAAATGATACAATATATTAATAATTGGTAGCAATATGTATGATAATACTATCCGATTGTCTAGGGATAGTATTGATAGGCGTACTCTATTCTTTAGCAGCAGCATTACCCTG',\n",
       " 'AACTGATGCATATCAGATTCCGGAAGTAGTCTAACCAGATTTGCACTGAAGGTATTTAAACAATCGCGCTGCTGTCGCCGCCTCTTATCAGCCATACGTAGTGAGTATAGCTACTCGTACTGTCCATGCCTGATATGCTTTTCTGGGCTCTTAACTGATTACACGTAAGGTTGTCCGCGGTCCCACTCTGCAGCTCATCT',\n",
       " 'TGAAGTTGTTGAAAATTAATAAGACAGATACAAAAGTGACAAAATGTTTAACTTAAAGTGCTACAAGAGCCCTCCTGATAAAGCGTTTGCAATACTTCAAATGACCTAAACGACGCACAACGTCCGAGTCAGATTGATAATCCTGAATACGAAGTAATCATATTTACGCGCTCGATCCGTATTCCTGGTAAGATAGATCA',\n",
       " 'ACTGCGTTTACGGAGTCGTGTATTGAGCAAACATTCTGATACATATTTCAGTACTGTCGGTAGGACTTCCGGCTCGCTACTTCGATACCGACAATATTGTTTAATGCGATACAGATTTGACATTAGAACTTATAAAGTTCTTCGAGCTATAACGCTCGTAGGGTACAACACGCTGTTGGACTTCCGGGTCATTAGAAATA',\n",
       " 'ACTAGCCGTTCATAACCATTAACCAAATACTACATGAACAATCGAAGCAGCATCAAGTCGGACCTACGACGCACGCAATGGTTATAAAAGCTAAGGCCCCTAGGCAGATAAAGGGGAGCATAGCTAAACGATCACGAGGTGGCACGTATGTCCAGACCAATCCCCCATATCGCAATAATCTCAGTAAATAGCACAAATTG',\n",
       " 'TGTACTAAATAATTATTAGTAACTGACATCGCACTATTGATTTCCCTAAGTTCACACTAAGTACACGCCGCGATCAATACCTATCAGGCAAACCAACACGTGTCTTATCATGATTCACCGCATTTAAGGCTCTCTTGCATTTCAGTTAACCAGCATTTGTGTATACATCCTGTCTTCAGTTCTATGTTCCAACGGGCGAA',\n",
       " 'CGCTTAAGCTACGTTGCATTTATGTAGGTCAACTCCTATTTGTATAAAAGTGATAAGAGTACGTTAAATGTAAGAAACCGGAGAGGGCGCTATCTGCTAGTCTGCACTAACGCCTTATCAGTTCTTCAAGGTAAATTTACCACGGAGTTCCGGTAGGGGGCCATCGTTTGTGAACAAATCGATTGTGCTTTCATAAAGAG',\n",
       " 'GCATATGAACGGGTGGGGTCAAAGGTCACCCTCGTAAAATCGTGTTAGACCAAATCTCTTGGAAACTTAACTATATGAATCATCAAGCCCCAGTCGGTGGCAATTCCTTTTCGGGTTGAAAAGTGGCTCTGCTGTGTCTAATACTCTAGTATATGATTTCGGGGTCAAAGGTCACCCATCGGTCAAACAAAAACGGATGG',\n",
       " 'ATGTTATGCTTAGAACACTTCCGGGGACACGCGGATGCGGATTCCTTCTCGGCGGAAGAACATTGTGTGCTCTCTCTGTATCATCTGCAAAAGTAAATGACGCCGACCGATCATGAAGCGACTACATAATCCTGACTACGAGTCTAAACAAACATCTCCACCAAGGACGCGTGTAACTTCTTCACCGTTATTCCTAGAGA',\n",
       " 'GTTTCAGCGGGCCAAGTTGTCTTTTTCCACTATCCATTTCACAAAAAACTGAAAGTTCATTAGTTAGAAGCACTTCCGGTTTAGATAGTCCTTGGCTCTAACTATTAGCAGTGGATCTCTTCTGCAGTATGGTGGACTGAGACACCACTGACTATTGGGTATGCAAGCGCGTAAAATTTTTAGAAGTAACAGTCCTTGAC',\n",
       " 'ATAAAGCTTGTGTTTGGAATTTGGAAAAAATATAGCCAATAACCTGCAATAACAAAATGCGAGTATACCTATGTCTGCCTAAAATATTGATAGTAACTAATGACATTTCAAAGCAAACTCGTTGTTCCTATCATATCAACAAATTGGTACAAATAAAAAATCCACCTAACGAGCAGCCAATGTTAGCTAAGAACTTAAAC',\n",
       " 'TAGTGTACTAACCATCCTAAAGGGTAAGGTCAAAGGTCACCGATGCTGGATTAGTGGAAAGCAAAGGGTACAAGTTCGCAAAGATCCATATAGTCTTTAGAAATGAATTCTCGGGACGCACAAATTAATAAATCTCGAAGGGTAGGTATTTAGTATTATCCCAATGTATTGAATCTCGATCGGACCGATGGAGCATCAAT',\n",
       " 'ACTTCTCCAAGTAACCCGGAAGCAAAAACGTTTGTCGATTTTAACATGAGTGAATTCTCCGGGTAATGGCGTTATCACAAAGATAAAAGGACCTCGAAACCGCTGTGCATGAATCTACGTCATCTTATTTTGGTGTCCTAGTAGGATCATTATCGATCTATTGACGGGTTCTAAGGTATTCATCAAAAAACATATAGTAA',\n",
       " 'GATGGGCATGATATCCAATTCTTGATTAAAGGCGACAAGAACCGGAAGCCGGAAGTCCTGTTTATTATATTTGCTAGTGTGTTATAACCCGGAAGTGGACTCCATTCATTTGCTGTCCCTCCATATCACACAGCGGGCACCTGGCGCACGACATAAAGTTAAATGATCTGATTAACATAGGACTCATTTCATAAGCTAGT',\n",
       " 'GTATATAATGTAAATATATAATCCGACTTCAATATATGAATATAAGCCCTCACTGTACAATGAGAGTGTTGTGCCCATGAGCTACAATACCCTAAAACAATTATGACCAGAAATTTAGACTGCATCGCCAGTCCATTACTCGTACAAACAACATTTTGAGCAACTTCCTGGTTCATTGGAAACTGAACCATCTCAGAACT',\n",
       " 'ATTTATCCCTTATCAGCCTTGTAAAAGTAATATATGCGACCGATTAATCACGGCATGCTACCCGATTATCAGAACCCCGGGGCGTGTCGTTCCCTATCTGCCAAGGTATCCACTTACACCTGAAGAATACTTAAACAATCCTCTAGCATAAAAGAATCACAGTACAATGTTTTAACCAATATGTTATGTGCGGATGCGAG',\n",
       " 'GGATTCTTATAAGTCAAGTAAACTATGGGTTCTCCAAAAGCAGATTCTAAGGGGCCACAGGTGGGATGTGATAGGTTTTTAGACATTACTTAATTTTTCATAGGGCATGTTAACTGCTCCAGATTGATATCTGCCAGATGAAATAGTGATGCGCCGTGTTAGATAAGTATTGGATTCCCTAACAGTCGCTTATCTTTTCA',\n",
       " 'TGGTAGCGTTTGAGTTGTCCCAAGTGATAATAAATAGGGCTCATGGCAGAAAGTCCCGTTACAATCGAGCGATGTAGGAAAAGTTCGCCTCAGACACTTAGGTTATGAATTAACAAGAAGCTATATGCTGTTCTTAAGGTCTTGGGTCGTAAGAAAGTTTATTAGCAAAAAGAATGTTAAAGTTGTAATGGGTTCCTGCT',\n",
       " 'AGTGAATGCTTGCTAATGTATATTTGCTTGGAAAAAAGAATTGTATGGATGGATCATTGTAGCACAAGTTCATCCACTGATATAGATTCTACCTCAAGTTGTAAGATTGACCGTGCGGCGGCGCTTCCTGGTGAGTGCTTTATATTATGGAGTGCCAACCCGGAAGTGTACATTCTCGGGATAACCCGGAAATCCCTTTT',\n",
       " 'GTCGCTTCCGGAGGGGTCAAAGGTCAACCTTAAAACTTAAAAACTAACCGCTGCCATGCTCTAGATTGAATACTCACACCTGAGCTATAACGATGACCTTTCACCCATTCCGACGGCATCGTCAAACCGGCGTACGTAGTGTTTGGTGGCTTCACGAGTCCCTTCCGTTTTGATAAAATTACCAGTTGAGCAGGAAAAAT',\n",
       " 'CTCCTCTTGACCTTTCACCTCTGATATAACGAGTTGTCGGAATGAATGCTTTTTGTTCCTTTGGTAAAGGTCAGGTCAGGGTGGAATTGTCATAGTCGAGAACATTTACGCTCCTTAGTGAGCTTACCGTTGGGGAGGTCAAAGGTCAACAGCATCCATAGAGACTGTTTAGGGACCTCGACAGTTAAATATAGGCCGCT',\n",
       " 'AGTTAGTCGTGGGAAATAACGAAAATTACCTGTTTCGCTGCATGAAAAAGAATGGTTTTTATGTAGGAAAAGGAAAGCTCTGGCCCGGTGACCTTTGACCCCTCTGACATGGGGTCAAAGGTCACCATATTAAGAGTGTTAAACTGACAGTTCAACATGTTTTTAGCTCATTTTCGAGCACTGTGACCTTTGACCTTTTA',\n",
       " 'GCATCCCGATTTCCTACGAGCTAAAGTCGGAGTATCAGGTGACCTTTGACCCCTACCTGCTAATCTAAATCCTGGGGTCAAAGTTCATCTCTCGCGCCCTTTAAGCCTTGGGTGACCTTTGACCCCTGGTCGCTTCATCCATTTAACGAATTTTTCGAAGTGTGAAAATAAAAATGCCCCGGCCGGAACAAAGAAATTAC',\n",
       " 'ATTAAGTGAGAAAATTGTGTCCAAGTTCTGATTAATGAATAGATCAGAACACTCATTAACATTGTCGTTCATGTCATCCAAAGTTAAAAATCCAAACCCTTACTGGGGTGATAAGCGGAGTGTTCCTTGAGTATGCTCATGCATATTGTCTATACCGGAGAGTATTTGCTTTTGATTTACTTAAAGGCTTGAGCCCAGAA',\n",
       " 'GCCTGGCGAGGTGTGACAAGATAAGAGGAGCACTTCCGGGTCATTGAGTTACTCTCAACTGGGCGATTTTATGTACACTCACGGTAAACATCTTTAAACCTCGCTGAGTGACAACATATGAAGGCGCCAGATTCCTAGTGCCGGCTATCGGTACGTGTCGGTATTTGAACTTACACACTTCCGGGGTAATGATAATTGTC',\n",
       " 'TGGTGCAAAAATAGTTTTGTCGCCATTTTCAAAATCAAATAAGATCGACTTGGACTGCCGGGTGAAGCTAATGCGCAAGTGGATGGTATCGCTGTTTCGATGAGAAGAAGTGTGCTTTATCTCGCCGTTCCAGGAATGATCCCACCACCATACTTTATGACAGGAGTAAACTTCCCAAGCTATCAAACAGTCTTATTAGG',\n",
       " 'GCAAGCACAAATTTTTGATCGTCCAAGGATGTACCTCCCTCCTGGCCCTCCTTCGGAAGCACCCCTTATCACTTAATCCATTGATTAAATAATCCCTAAATTTAACTAGTGGCCGAGTTGAATATACAGGCGAGTAAAATCGTCCGACACGGTTCTCAACAGATAGACACAAAGCTGTGTGGCATCAGATTGTAGAGACA',\n",
       " 'TCTTTGAGTTGTTACCTTTTCGTAAATCACACATAGAAAAGTGTTTTGGGTTTATCATACAGAAGTTAGCTTTAATTCGTGGCTCCCATCTTTATGCATCAGCTACCAATTGATATGATAGGGCACTTATCTTGAAATCAAAAATGGCTGGTGTCGGACTTAAAACAACATCAGTATACCGATAAATGAAGACTTGGGGT',\n",
       " 'AACATATATTCGTATACGTAGCCAAAACGCTACACTGTTACGCCTCGCACATTTCTTTAATCTTTATCCAGGGGTCAAAGGTCAACGGAACTTTTGACTGGGGTCAAAGTTCATAGTCACATTTTATTATTGGTTAATGTAAGTTGCCGTAGCACAGTGTGGGTATTTAAAGGACTCAAAGAGATCCCCGTTTAGCTGTG',\n",
       " 'TAATGTGTTTTAATCACCCTACGAAATTCTGGAACAGGCTGTGTGTTCGCTGTATTATCTGTCGCACTTCCGGTTGCCCCTCTAACCATACAACGGATATAACTGATATGGTAGATACTTCCGGCTTCTCTCCTATAACTCCTTGGGTACCGGACAGCAGTCGAGCGAGACTTACAGCCGAACAGCCTCACCTATCCCAA',\n",
       " 'TAAAGATATATGTATTAACGCTTCCTATGTTTTAACCGAACGGAAGGCGAGTCTACGGCTGTTATCTGGCACCAAACTGACCTTATTCACTCTAAAGTACAGTTGGAGTTGCCAAAGCTGATATGGCCGGGTAAAGCCAGGGTTGGCATCTTTCGTGTGATAAGGATTACGGAACCTAAGCATAGATGATGATCTGGAAC',\n",
       " 'CTAAGTGATAATGACGCAAGTCCGATAAGTGTCGTGTCGGCGAAAGCTAAGTGTCGAGATACTGACTATCAAGCCAGCAGAGATTGTCCAAAGCAATTATGTTACAACTGAATTATAGGCGCAATGATTTACATTGAAGTTTGTACGGATTCAAATCCCTGGTATGGATTGTTTTCGTAGCTTTTACCTGGCGGGACCCT',\n",
       " 'ATTTTGAGACTATACGGAAACTATGCACAACTAACCCGGAAGTGAGATCTCTGGGGTTTTAGTATAGGATTTAACGCTTACGGTGACTCGGCTCCTAGCATAAAAAATAATTACATACAAGTTTTCCCTTATTTTTAGAACCATGTAAGGCTTACATTTCCGGGTTTCGACAGTATCTAGCTAGGCATAAACGACCCTAT',\n",
       " 'AAATCTGCTTCATGCTAACGAAATTATATACAGAGTCTTCGTACAACTCCATTGGCCATTAATCAGAAGAATCATAAAAAATCAACTGTTGGAATTATCAAAACTGGCCACACAAGCGACAGCTGATAAATCGAACAGGGTACTCATGGTCCAACTTATTACTCAACCGCCTATGATACAGTCCAGGTGCTGGGCTTCAT',\n",
       " 'AACAAACTTTGAATTAAAAGACAGATAGATAGCTGTCTATCTCGTGTTGTAATTGTGCCGCATGGAAAGACCATTAACTTATGTATTCATAGACATAGGTTAGTGGATGTGATCTAAATATATCCAAGTCCTTAGTACGATACACAGATAACGCTTGCTGGCCGAGTGATAGGGGTAAGTGGCGGACTTCTTTCTCTACT',\n",
       " 'ACCATTGTTCTACTGGAGGTTGTTAATGATAGCTGATATAACGTGACAATCTGTTTTTTCGCGGATTTATCGGGAACCCTAATAGACGGACGACATAATTTAACGCAGGAATGTGCGTAATATAGAAATTTTTATGCGCTGCGAGCTGACAGTATGAGTGGCGAATACGCTATGTCAGGCGAGCAGGGTGATAGGTAGGC',\n",
       " 'CCACATCTTTGTTAGAAACCGGAAGTGTTTTAAGGAATGGCGACTGCTGCCGAACTTCTCAGGGGCAAGAACTGTTGCTAGCTTCCGGGTTTTTGCGGAGCTGTCGCTTTAACTAATTTAGCTGCTGGTTCACACGGAAATGGTAACCTATCCATTTGTCAAATAAGTGTGGATACATAAAGCGGATGTAGACGGGTAGA',\n",
       " 'ATATTTGCTCCTCTAACGCTAGTTACTCTTAACCCGGAAGTAACCGTTTTAGGCTGGTTAAATATCTGTGCGCTTTGAAATGCCCTGTGACGATACTTCCGGGTGTTTCTGAAGCGGAACAAAGATATCCATAATTAGTCTTCGGATGTATGCAAGGTTTCTCAAACGCGGCGGAGTGATTTATTGAGTCTTCCTAAGCT',\n",
       " 'TGAGTTAGAACCCGGAAATTTATCTTATTTATTGCTCAGGACATGAAAAAAGCTTCCTAATTCCAATTTGCTTGATGGTTTCGGAAGTAAAAATCCACTTCCGGGTAACTTGTTAGACAATTGCTTAACTATCCACGCTCGCGATCTCCGCTGTACCATTGTCCCGTGTCTTAGTTAATACAAATGCGGTTTAGCATGTG',\n",
       " 'TATACACTTCAATGAGAGTAATTCAGATGCCATCATATCCCGGAAGTAAGAGGAACTATAAGTTGTTTTAGTTAAGTATTTTTCCAGTAGACAACTTACGAAATACTTCCGGGTCCAGAATCTCGCAGTAATATATCCGCAGCAACGGGGAGTTCCACACAATTGAAGGTGAGTTACTTGGTATTAAAGCGGAAGTGCAG',\n",
       " 'ATTTCCTAAGATGTAAAGAGCAAAAATGTTTTTTTATATTGTGAGCCTCATTACCGATGACCTTTGACCCTTGTATACCACAGCTGCAGGACATCGTTCAAGGGGATATTCCTAAGGTTAGGAATGTACGCTGTCATAAAGAGGGGTCAAAGGTCACGATATGTCGTTTAATGAGAGGTCAAAGGTCATACTAACAGTAT',\n",
       " 'TAGCTGCAAAGCCGCTATTCCAGGGGTCAAAGGTCGCAAACCCCTCCATTTTGATGAAACCGTCGTCACGCGGCGGGGCAGCTCGTAAGTGCTGACATAATATGTAATGTTGTTACACACACTATTTCGTATTGACTGAGTAACTTCAGTGTTGCATTATGCAACCCACGATCTGATCATTAGAAGATAATGCATATAAG',\n",
       " 'ACAGATCTACACAACACGTGCCCAACATATTCAGCCGGGGTCAAAGGTCAAGAACTCAAGTATGTCTGATACGACTTGGTACGGGTTCGGGGAATTAGTCCTCCAGCTTGCAAAGAGTACTCTGCGTATAGTTCCGAGACAGGGGTCAAAGGTCAAATGGCTTGATAATCCAAAAGTTCCTAGTAGCAAGCACAGTACAT',\n",
       " 'GCATCTAACTTCGTAAGAAGGTTCACAAAATCACACTCCTGGTTGTCCTATTAAACCGGAAGTAAACATTTCACCTGCATTTCATAATTTAAGGGTCAGTTCCTAAAGCAACACATCCCTCTGATTACAGCGGGTTCGCTGATTGGTATAGACAGTTATACCTCCCCCTTATTCTTCATTTCCGGTTCCTGCGGAATTCC',\n",
       " 'ATCTGCTGAATGAGTCTCAGCGATCCCGGAAGTGCGAGCTGAGCCTTCCACAAGGTCACTGTGAAGTCAGATTCTCATACACGACTCGGTCCCTATTAGTATCTGCCTCTTACGTCCTATGCCGGAAGTCTTTTGGATGCAGCAAAGCATTATCTTTAAGTACGTCGTTAATAATTTAATAGAAAAGTGTAAGTCTGTTT',\n",
       " 'CCAGAATGTCTTAATCAACCGAAACCACCCGGGTCCAACGAAGAAGACCTCTTGCGACTGCAAAGTCCTAATAGTCTTCCAACGTTGATTTCCGGGAGCTTTGTACGTAGTTTACATTCTTGCTAAATACGCCATTCTCAATTCCTCGACGTTCCTTCTAAAATTAACAAATCTATGTTTACGCTAGAGGCTACGCGTAC',\n",
       " 'TGTACAGCTAAACCAGATGGTCTTTATAGACGCAGTACATTCCGCGATTTCAGTTGGAGAAGGCTTGTACATTGCCAGCCCCGGAAGTACGTACCGAAAGGTGTTACGCGGAAGTAAGTGCCTCTAGTAGTTAGGGTTATATGAGGGCTTTAATGGAAATTACCTGACTTTCTATGAACGTGCCTTGGGTTTCATAATTT',\n",
       " 'GTGCTACTTTAGAGGTCTTTCATAAACCCAACATATGGACTTATAAAGAGAGTATGCTGGTTAAGCACGAATAGCAGAAGCGAGCCTAATCTGATAAGACTGAAGTATTCGCTCGAGCTTAAACAAAGGTTCATACATATGATAAACCTTCTACTTCTCACTCGAAGAGTAACATTTTCCTACCAGGTATACGAGAACTC',\n",
       " 'TTCTAGAGGTCAAAGGTCATAACTCGGCAACAATACTGTACCATGCTAGTTGTCAAACGCCATATAAGATAAGCCAATTATGGCAGGGGTCAAAGGTCACCTATTTAGCTGAACAGGAGTCGGCAACTTATAGTATATGTTTGAACAAGTGTCAGTATGTTTTGTGTGTGTCCAATATTAAGTACATCTATTGCTCCATG',\n",
       " 'AACACTAGCTATTGTAGTTTTTAAGGTTCCGATCATTCCTAACTTTGTTATCAGGTTGCGTCAAAATAACCAACCGATGAAGAGCCGGTCCTTTTGTATTTCAGATACTCTTTTTTGGTGCCTAGCGATAATTCCATAAGAGTGAGGTGAAAGACCTTTTATATTAAGGTGCGGCTTTTATTACAGAGGGGTTACCATGG',\n",
       " 'ATCCAACTAACCTCGAAATACTGGTCCGCTCTTCTTTTTGCATGACCACCTTCACCGACCTAACATTATCTGCAACATTCAAAGGATAGAGCTTAGTCGTGAAAGCGACGTAACAATTTAACGGTGACTTGCAAATCAGAAGCGCACGAGTTACTTCCGGGTTATGGTGAACAATGAGTATTCATTATGAGCTTGGACGA',\n",
       " 'TTGCGGCTGTGTAAACCAACAATGAACTAATCTGGGATGCGTTAACACACATGGGCTTGTGCTACTAGAGTTATCAGACTTCAGGTGGTCACTGGAAGGCCAAATTATGTTAAGCGAGCTAAATATGTCTTCCGAACGAGTATATTAAGATTAGCTAATAGTGTAAGCACTTGGAGATACCGTATTCTATGTACAAATTA',\n",
       " 'TAAAGAGGCTGAACACGGAAGTCATGATCTTCTAGCTTTGAGATGATGTTTCATACCAGTGCCGTTAGCGACACAGAGCACTTCCGGGTTACGATAGAATCCCAGAGTGAGTCAAACTCACGACAATCCTGTGATAATTGAACAATCTCGTACTGATTGTGAATTTGTGCTGCAAGGCGTTAAACACTTCCGGGTAAACC',\n",
       " 'CACGCTTTAATATCTATGGGACGAACTCTTCGATAAAACAACTCGTGTCATTCATTGCGGAAGGATTCCCACGCATTCTGAAAAGAAGAGGCTACACAGATTGAACGTCCACACTCAAGTGGATATCGGTTATCTTGTAGCATTAAGCATGGTGCATGTCCAACTTATCTGCTTCTCATATGACTCTAGCTCGTATTAGA',\n",
       " 'TTTGTTCTACATAGCGCTGGATGACGATAAGCGAAAACGACACATATTCTCACACTGGACTTTATCAGGGGCAGGGGTCAAAGGTCATCTATTAGTTCGGTTAAGTTAACTTGGTTGACCTTTGAACTCTATCGATTAAACGTAACGGAAAAACTATCCTTCCTAAAGTTCGCTTAACAGGACCCTACAACAGTTTTAAA',\n",
       " 'CCTGAAGATGAATTGCACCATCCGATGACCTTTGACCCCAACCATTCATTGAGCTTTGTACATTCTTGGAAAGGTCAAAGGTCATCTGGTGAAACGTCACCAGTGCAGTTTCAGTCATCTCTTTGTAGTAGTGCATCGAGGAATTAAGGGGTCAAAGGTCAAAGGACCATGCTCGCCCAACTGAACTGTTTAAGTATTCA',\n",
       " 'TGACTTGGAGTCCCTAATTTTGGAGTTTCCATCGTTGCGGCTCGCTTCCGGAATACTTCCGGGCAGTTTATAATGGCATACAGATTCTACTAACGTCATAGCATAAACGTCCGAGCACGAATGCAGGGGCTTAACAATTATTTTTCTAACTGATCCTGAAATTCTCGACGTTCAACTCAGAACGAAGATGCCGACACATG',\n",
       " 'CCGTTTAGAATAACCGTGGTATAAAACGTAGTCAGTCACCCGGTTGTGACTACAATCCCGGAAATACACAGCAAAAGAGTAAGTTACTTTGCTACGTGCCATGGAGCAGTACTTCCGGAAAAGGCGTAAGGGTAGACCTATGCACTAAGATTGCAGATTGAACGTTCGGACCCTGACGAGGTGAGTATTATCCGGAAGCA',\n",
       " 'CTAGAAGTAAGTCAGTGTTCCTTCAACTTAGACCTGCCTAATTGTACAGAATAAAAACTATCTACCTTAAATTTGGCGAAGCGCCCGGAGCTGGGGCCGTCGTTAAATAGACAAGTCAGTCCAAGTGTTAGCTAGGAATACGCAACTTATCTGGAGAAAGAGTGAAATGGAATCAAGTAAGTTACCTGAGGTAATTAATA',\n",
       " 'CATGGAACGTCCACGTGAATATTACGTTAAAACCTGGTGATGTAAGCCACGGTAGTTCATGAGGACTTTGACCAGCTTGTACGGTCTACATGAACAACTAAGCTAGCATTGTTCATAACGGTCACGAAGCTCTCACTTCCGCGATGTCTATGTTGCAAAGTGTATCGAAATACGGGGATCGAGTGTAGATTTCCATGTAC',\n",
       " 'GCTGGAGGTCAAAGGTCATTTAAATGCACTGTATGAATGGTAAGTCCCTGTGATTGCACCGCCCATTCTCGCTTATTTTACGTAGCTGGCTAATAGAAGTCCGGGAGGGGTCAAAGGTCACCTGATCGCAATGATTAACATTGGAGCCATTTGACCTTTGACCCTTGTCTCTCGGTGCTAGCTCACGTTATCACTTTCTA',\n",
       " 'CCACTTGCTTGCTATATGACCATGAACTTTGACCCCTACTTTGTTAATTTAATGAGCCTTTGGTGGACGGGGTATATGTTCTTTACATGATATATTCCTTTACCTCCAGCAACCAAGGAATCATGGTGTTATAACACTCACCATAACAGTTTTGTCTCCAATGGAGTCTATGTGACCTTTGACCCCTAGCTGGTGCGCTT',\n",
       " 'GTTAATACATAGGTCCGCTAGAAATTCACTGATCCAATTGTTAGTGATTAGTGCAAAAACGTTTAAGGGTGACTATTATTGGTATTCCCTAATTCTTATCGCCTCTTCCATGATCACCACCCCTAAAGTAACGGAAATAGCAATTTAAGAAAAAGTTAACAGGTCTGCCGCTTATCGTTCACCCGTTACATAGCACACGT',\n",
       " 'TAGATGTACAGACGTTAAGAGAACTGGGGTCAAGGGTCATCTAACATTGGAACTTCGCACTATTAAAGGTATGTTAGTGGCAAGGGTGACGATTGTTATCAGATTTAATAGACGCACCATGAATACTACTTTCAGTTTATAGAAGAATTTGAATCGCTAAAACTTAAGTCTATAATCGAAAGCTGATGGGTATGTTGCAG',\n",
       " 'GGAAGTTCCAACCAAAATAAGAGTTCTCTGTCATTTGACCTTTGACCTGAGTTAGTCTAAGTACATACATACTAGATGACGTATAAGCTATTGTAATATGCTACTTTGGCGAAATTTTAACTATATTTCAACTGCACCTTATTCGAGTTAAATAATATTCTCCAATCCGTGTTCAAGTGATGAGAGGTGGAATAAGCCAT',\n",
       " 'GTAGTATGGGTGCATTGCGCTTCCGGGTTCTAGTGAACCCGCTTATTTACATGGCCTAAGGATAAGGCGTGTGTATCTTAAGAGGACTCGCCTTACCAGTTGGAGAGCTGCCAACGGCTCTCTTCCGCCTCTTGCGGAAAATACATTAATAAATTGTGCGCGATTTCCAGGCTAAGATTATTACTAGAAGGCAGTGCCAC',\n",
       " 'TCCTTAATTTAATGCAAAATCGCATACAAGTCGCTAGCAGGATATCCATTTATATAGGTATGGATTTCATCATAACTTCCGGGGTAAATCCTGTAAGTTGGCGCGCATCAAATAAAGGGGAGTATGAATCTGGAGCGGTCTATAACTCCGTATATGAATATTTTGAACCAAAATACTAGAATAAAATGCGTAAATAGATG',\n",
       " 'CCTCAACTATAGTATGGATGCTAAGAAATCTTATATGGATAGATCTTAGCCTGAGAAATAAATGTAGAGGGGGGTCAAAGGTCACCTTAAACAACACAGAAGATTTCGCCGCTCGACTGTCAAAATGAATTCTACTCCAAACCAAAGTCTATAACGTAAGTGGCTAAAATTATTGTTTCCATTAAGGATAATTAAAAATA',\n",
       " 'TACCTATATAGGCGAGCCCTGAAAATAGGATTAACTCATTAGTCTACATTTATGAAATCTAAAAACAAGATAAGGATTGTAACATGTAGATTGCAAGAGATATATGCATGAAGACGCTAGAACCCAAGCATTAATAGATCTGGGCATTTGAGCGTCGTATTGTCATGCTTCAAATATCCATATTTTGTTGAATTAGATGT',\n",
       " 'GATGTATGTTTTACAGTACATCTTTTCAGGGGTCAAAGGTCACAGGTCTAGCAATCGGAAACTGAAAATAAGCTGTCATAGAGCTTGGTAAAACCCTGATAAATAGAAAGGGCTGCCTATGACCTTTGACCCCTGACATTACAGGGGATTGCCACGTGAATTCATGAGCAATTGATTATGATGTTGTTACGCTTTCAATT',\n",
       " 'TTGGTAATCTTTTTCGAAATTTCCAATTCATGGATCTACGATGTAGTTCGTTTGGACCCTTATCTGAAGTTACATGAATTATCTATTGGAACGTAGGTCGGTCACAGGGGAACCGGCTCTTCCTTCTGTGGGTAACTATCTGAAAGCTCAAATATATTTCTTATTCGCAGAATTTTACGGATAATATAATTCACCCAAAT',\n",
       " 'CCAGGTAAATAATGAAAAAAATGTAGTGAATATTGACATAAATTATTTAATTGATAGGGGATCACGCATGGTGCTACAGCCTGAGTACAGGCTCCATAAATGTTGATATTGACCCCTATATCTTTAGCCTTCTACCTTACCCAATGGAATTCCACCGCCGCACAATTGAAACTTCTTAACCTATCACTACGATGCATTCT',\n",
       " 'ATTCAGTGATGGAAGGCCTCCATGTTTACGTCTGCAAAATTGTGGGGTCAAAGGTCATTCAGAAGCATTTGACCCTTGACCCTCCCGGCTACCGAAGAAGTGCCGCTATCTAGCATTCTTTTTAGGCATCGTTGATTAATATTCGTACAAAATCAATTGGTTGAGCTTGTAAAGATCGCCTGTCCGGGGACAAATCTTGT',\n",
       " 'TTAATAGTCTAGATTAGCACTATTGTTAACAGCTAAGCTGAGACTTATCAGCCGTACATGGCCCCACAAGATTCATTAAGATGGTATTCTGCAACGGACGTATCGAGTTCAGTATGAATCCAAGTTTAGTGACGCAATCGTCTCTCCTTTACTGTGGCTGAAAATATAGAAAATTAGGAGTCAGTAGGTCATTTTATATG',\n",
       " 'TACCTTTATCTGCTACTCTCATATCTTTCACCATCCCTCTAACGTTTTTTTCTCACACAGGTTAATATCTAAGAATAACCAGATATCGAGCGGAGAAATACGTTAGACATTTGGTTTGCGTGCAGATTAACACCTGCTCTATTTAAGGTTGTCAGGAGGTTTCCGCTGGGAAGGCTTTCATCAATTAGTTTAGAGTATAG',\n",
       " 'GTAGATCCGAAAATGAACGATATTTAGGCTAGCATGGCGCTTTGATAGGACAGTTTTAATCTATTTTACCTTACAAAACGCCGTATCAATAAGCTTGAATTATGTTTGAAACGAGCCCCATTAACCCGGAAGTTCCGACAATAGGACCTAAATTGATCTACAAGTTGTCGGTAATATTGAAATAAAAGCTTGGATAAGCA',\n",
       " 'TTTTCATAGACAGAATGGTGACCTTTGACCTCCGTGCTTCTTTGTGTTTAATATATTCGCCCGTGGAGAGAGCCCTTAGATATTAGGTGTTGGTAGAACATGTAACCTTTGACCTTTGGGGAACCTATAACGACGACCTTTCACCCCTGCACGTATAAGCCTTTACTTTCGATCCACGGAACTTTTGACGTTCGGTAACA',\n",
       " 'ACTATAACATGGCGGCGCTAATTCATCGAAATACCGAGAGCTTTTTGAAAAGCTGCTCGAGTCTCCCGGAAGTATCCCGCCACTTCTCTACTTCCGGGTTAAAGTGATGATATATACTACTGCCGAAAGTCATACTTCGTTAGAACCACTCTATTGCAGTTTCTCACCAGAAACCGATAGGAAAACGTGAGACAATGAAC',\n",
       " 'CGGAGTTGTCGTAGCCCAGGAGCGCAACGTTATGCGGTGGAGATCATGATAACTACACTAGAGTATAAGACCACCCTATCTCTTATACTCAACATTGTATTAAAGGGATACTACTTGATAGGTTTGGCTCGAGACTGTTAAATAAACGATAGCTGACGCTGGGGATGAGGTCAAAAACTCTATAGTTGAATTAGTATAAG',\n",
       " 'TTGTATGATACACTATTGGAAGTAGTGACAGATAAGTGGCTATCGGCTTAAGCATTTAGGATGGGGTCCGGCTTGCAGCCTATCTCATTCATTCGCATACCGGCAAAGAAGAATATTAAAACGGAGAACTGATAATTGTGCAATCAATTAAAGTTTTGTAAAAATCATCATGATACGAACGGTTGTCAAGAATGGCGTCG',\n",
       " 'ATGTCATGAATCTAAGACGGAGAGGAAATGACCGATAATAGCATTGTGTCCACCTAACGCACTTTCGCTTGTCTGGAAGTTACCTTTCGGCAGACAGTGGAATGAACTTATAATTGCCTGATAGGACCACGCTATCAACGTGATAGGACAGCACGACCAGGCTCAATTTAGTCATCATGGTTGTATGATTTAAATTCTTA',\n",
       " 'ATCCTTATCAGTATAAATGAGCTAGGCTAATATTTAATGTTCCGCGCATTTCTGATATCAACGTTAGCCACCGTCTTTAAAGAAGAATTATAGGGTACCATATAGGGATTACATTCTTATAAGTTAAGTCATGGGAGCAAGCTTATTACGCTGCTCGCTTATCTCTTGAGATATTAACGAAATCTCTTCATAAACCATGG',\n",
       " 'GGTTGTGGGGTCAAAGGTCATGCGTACCAGTCTCTCCTTGACCTTTGACCTTTCGACCTTCATCTCGGCGCTCTTAAAAATAGTCGAGACTACGAAGGTCTACGGGCTAAACCTTGAAAGAAGGGAAATATACAACATCCACTTCCAACGTTGACCTTTGACCTCTCGCATAGACGTGGAGGTTGATTACCATTATCAGT',\n",
       " 'GACCAAGTAACAGGCTTGTTGAACCCAAAGTTAAGGTAGGTGACCTTTGACCCTGGGATTGCATATTTGAACATATGGTGGTCATTCTTTCCAAAAGCTCGCTAGTTGTTTTAGGAGTAAGAGGTCAAAGGTCAACTTTAAAGTCATACGGTGGAATAAAAAGTTTGGAATTACTTATTACAAGAAGCAAGCTCGCGGTG',\n",
       " 'CGGCCTTGGTCCCCTATTTATGAGTAGGTCTCGCTTGCTTGTTCCGGAAAAGCGTAGGTTTCATGTTATATGTTTAAAATACTTATCTACCGAGTCCGCACCAAAAAGAACCCGGAAGTAAAAAAACCTTGAAACCATACATCAGGATTTACGTTATAAATTAAACCCGGAAGCGAAATACATTATTTTAGGCATCATCT',\n",
       " 'TGTTATCCATAGCATTATTATTAGTATCGTGCAACGCAACCGGAGTGGCGGAACGGGTCGGGGTCAAAGTTCAAGATAATTTGTACTCTTTATATTTGACTTATACCGGTTGTCGCATGACCTTTCACCTCTGCGCGTGACGCTTCAAGCGATGAAAGAAGCTCATACAATTCGCGATTACCTACTCTGCTGAGCAGATC',\n",
       " 'TTCAATACCGGCATAAGGGATTTTTATTTTATAGTAGCATTTTCAACGCAGCTAGCGTATGATTTATCTATCAAGAGGTAAGCCTAGTGATATCTGCACCCTCATGAGAGTTGAAAAATATTCTACCTTAAATTACCTGCGTCATTATTACACGACATCACGCCGGGAGTTCCACAGTTCACGTACCGCTTCTGCTATGG',\n",
       " 'AAGGTTTATATAATAAAAGTCATAAAATTTATTATGAAAGGGAACAACGCTGGTCATTATTAGCAGCTGTAATACATAATGCGATGGTTCATAAGATACAGTCAAGCACATGACCCGATAGCTCGCTATCGGTCACTCATGTTAAGAGTGATAAGGCGTAGACCAGATATGGTGCTGGCCTGACTCAGGATAGAACTCTC',\n",
       " 'TCTTCTACTCAAATTAATTGTCGTTGCATTTTCGACATACGGAAATTTACAATACCATTTATCACCGGTATCGTTGTTCTCCATCTTGTCCTATCAGGCCGTCAAGACTCAAATTGTATGTGGTCTTATTTAATAGATATAATGTGCTTCAATTCGCTAATCCCATTGATGCCGGCACTGTAAATTAGGAACTATACATG',\n",
       " 'TGAAAGCAAGGCCATAGTGTAGTGTGCATCTTCGCATTATATACTATTTCGAACAATTGGGGTTAGTGATAGCTTTACAATGGTACCTTGTTCGATATCTGAACCATGTCCAGTAAGGGGGTAGACAATAAGACAAAGGACTCCCAGATAGCTACAAATCAGTACCTGCCATCTGCAGGGTCCATCATCTTTCAGTTCGT',\n",
       " 'TGTAGGCTTAGGGTCAAAGGTCAAGTTACACATGACCTTTGACCTCTCGTGGAAGGCTGGGAAGCAAATAAGTAGATGACCTTTGACCTCTCGTAGCCTGAAAACTATTTCAGTAATGTAGTTGAATAAGGTCATTACCATTGGTGACATAGATGAGCGAAGACCAAGGCTTATCTAACGTGGTTCAGAAACGAACTTTG',\n",
       " 'AGTTACAAAGAGAACATAACCGGTTGGACTAATCCTCATGACTTAGAGAAGTGCTCGACCCGCAAATGAAACTTCCGGTTATTAAACTCACAGAGTCCTCTTAAGGCATATAATAAGAAGTACTTCCGCGTTGAGCCCGCCAACTGTTTTAATTAGCCCCACCCCATTCTTCTAGATTGTCTCTAAATTTGAGTAGGAGA',\n",
       " 'AGCCGTTGTCCGAAGTATACATTTTTATTGGATTACAATGATCGAACCGTAAATCTTATATTCACCTTGTGCACCGAAATACCTGGAATTATGGCTACTTCATGCTTACTTTGGCATTTGTAGAATTAAGCGCCACGAAGAAGTTCAATATCTGATCTCTTATTTGCCGTTATCTCACCCATGGTGGCGACATTGAGAAA',\n",
       " 'CGTTAAGACAGGTTACACGTTCCTGTTCTCGAACGCTGATCATGGTACGCATATCTCCACAGTCAAGCGATCAACTTTCAGGCGTATGTAGTACCACGGCAAAATTAAGATAAAACGATCTATAATATAAAACTACACATAAGATTTAATGCTATCAGTTCAATCGGAAATCAATTTAATCAAAACTGGCCGATAGTCAA',\n",
       " 'TAAAGTTATTTACGCGCGCAGTTTGCATAACAGATGTTACTTCTATGTTAACCTCTATCTTTTAAGCCCTGAGGTCAAAGGTCATCAGTTAACAGCCGATTAAACCCGTAATGAGGTCAAAGGTCATCACATCCCCTACACTTAGGTACCCTAGGTGAGTAGTTAACTGTTTCACGGCACAAAAACTCAACGGTGCGTCT',\n",
       " 'TTGCGTCAGTATTTTTAGATCTACCTGGAGTACTAGCTGACCTTTGACCTCTAGAATAATTGACTGCCTGGTAATCATTGGTCGATTCTAGCTCATTTCTTTGATTTAATCCGGTCCCGTTGATAACGCATGCAATCTGTTCAAGTAGAATTTTAGGTTCGGTACTGGGGTCAAAGGTCACCCACGCAGGTCGCGTATTT',\n",
       " 'CTCCAAATTGGTGGTCTCATGATTAATGGGGGGTAATTATGACAGATGCACCATATATTTCACGTATTACGTTCTACTTCGATGACCAACGAGATTATCCTTATAATTTCCGATCACTTGACCTTTGACCTCAACCGTCCATCCATCTGGAATGCAGAAAACTACTCTAGTTAACGACAATCAGTATTCTGAAATCAATT',\n",
       " 'TACGTAGCGAGCGTTATCACATGAATAGCTCCGATGTACTTCCGGCTCGTGAAGACTGCATTAAACCCGGAAGTATTTTCGTTCTTCACCTCCAGGATAGATTCTAAGGTGCTAATCGTATATTACGAAACAATCCATATCTTCGCCGTAGTTAACGTACATAGACGCAAAACCCGGAAGTACGTTGCCAAGATTAGATT',\n",
       " 'GAGTTCATGATGACTATCCCTTGCATTAAATCAACGAAGATCTGTATACCATATCTTGATAATGCCGCAAGTCGATTCTTTATGAGCTAAAGAGTCTTTATGATCCGATAACGACATTTAATATAGTTATCAGAATTTTAGTAATGATCAAGCAGGGCCTATCATACTGAGGTGTAATATTGTGGGTTCGGGTTTATCGA',\n",
       " 'ACAAGTCAGAATAGTATTTAGGTAACCACATAGGCCGTATGACCTTTGACCTTCTCTCACCTATGTCAACGAGAATGACCTTTGACCCTTGGTACATCCGAAGGACCGAGAATAGTAACACATTTCCTACGGGTTTACACGATTTTATCTTAGTCCTTGTGAATACTGGAAAGAACAGAGACACTTCCTTATCTGGATAA',\n",
       " 'CCTTCATAGCCGCGATATCACAAAATATTGGTAATTAAATAGATCACGATCAAAGGTTACCCTCGATTATGCGGTCTAACGAACCTATGTCCGAGATATTCATCCTGGTCAGATGTTAAAAGATTCCGGAATATATGTACTCTTCGGGTTGCGTGCTGTGTTGGGTTTACGATTGAAGTTTGCGATTGCATGAAATACAC',\n",
       " 'AAATCCGTTTCATATAAATTAATTAATATACTTTATTTGTTGCTTCCAGTTATAGAGGGCGAGTGTTAATAGCTGTTAGAAGTACCAACTTGATAGCGTGAAGGTCAATATAATCCTACAAGAGGACCGCTTCGATCTCTATACCTACTAGCCGGCTATCAGTGGATATAGTGCATTAACGAGCGACCCCTATCTGAAAG',\n",
       " 'CTTATCCCACTAGGGAGCCATTTATCGATGACCTTGATTATTTAGTATACGTCGTGTAGTTTTTGCATTGTCTCTTAGATCGCTTAAAAACAAGGCACTTCCGGGTTGTTGTGAGAGTAGCAATGCGCATACCTGCCCTTAAATTTAATATAGAAAGCTATTGATTAGCCCCCGTAATCTCGACCGGTGTAGAGTCCGGT',\n",
       " 'CTTCAAGGCCATATCTTGTCTACATAAGTGATAAGATATAGCTGAGGTTAAAATTACCTATGCACAGCATCGTTTCGGGTGGATAAATATCTCCGCCGCTACTAAATCAAGCCAGGAAGACTTATCCTGAACATTTGGATCAACAACCTAGGGTGGCTTGTCCCCGAGTTATTGGTGGATACCACAAAATTATAGAATGT',\n",
       " 'TTTAGTCTAAGGCGTTGATCTCCTATCTCATCTAATGAATTAGTGATCAGGCATTGCGCACTGGATTAGATGTGTTTTACTAAAGAGATGAAACTTATCGCTTACACATCGTAAAACATACGAAGGATGTTGCAAGATAAAATTGTGATAGGGGCCGATCTAGGTTTACCGCAGTGTGTAAAATGATACAGCAAATAAGC',\n",
       " 'CACATTGGTCTATGAACACGGAAGTGTAGGCTGGGTGTGAATTTTTTACTTCCGGGTGGCACTTCCGGGTTTACGTCCATGGATTTTATATATATGTGAAGCTGCCGCCGGGGCTTAGATAGCACTGAAAGTCATATCAAGGGTTCCTCCCACGGACTTTTCTTATAAGGGAAGTATACCGATCATAAAGAGTAGATTTG',\n",
       " 'GCGCACATAAACAAATGAGGATTATTAATCGTGTGATCTTTTAAATTGTAGTCATACAACTGAGTTACAAATTTACCTGATCGAAAAGCAACGTAAAATCCATTGGAATTCCAATATATTTTAGATTATCCCGTGACGGTATCACTTAGGGGTCAAAGGTCATTGGCGATAAATATAACAACAATCCATAAAAACCCCAC',\n",
       " 'GTGTACAGATAGACGTAGCATCGACCAGCGCTGTTATATATTTTTTACGTTACTAGTGATTGATTATCAGAGGGTAAGCTCATTAGCCATGTGCGTCTATAAAAGACAGGAGTTCGTCCAAGCAGATAGGGTCAACTCGATAGAAGGTCTTGTAATATGGTCGTATAAGCGGTTCAGAGACTACAGATAGGGGGGTTCTA',\n",
       " 'TGCCCTTACTAGGAACACACCGATCCATTTGTATCCCTTTGACCTTTGACCCCAGGATCTTCTTGGTGCAGACTTTCGCATTCTTTGTTGTGGCTCCATTAGCTGGCTGGTGTTCGGAACAGAGGTCAAAGGTCAGGAATCTGAATTTACAAAGATGGTCGAATACATAAATGGAGGACTGTTCGGGTATTGCCCATAAC',\n",
       " 'ATGTACTTTAGGGCATCACTAAACCACGCGTGACCTTTGACCTGTCGACGTGTATGAATTGGTACCCATATGAGAATTAGGCGGCGGACTCGGGTTATGTATCAGACTGTAATATAATAAAATAGGAATTAATACCAAGGTCAAAGGTCATAATTTGTCAGTAGTCTTTAAGGGCCCTGTCCGACCAAGTCTTAACGTAG',\n",
       " 'AACCATTGTTAGTTAGCCCCTCATTTATGAATACGGAAAAGTATAAGTGTCTAAATGCGCGGTTGTGAATAGCGCACATATGTAGCAGACGTGTGTAGACCCGGAAGTATGAAAAAGACCAGATCTACGATACAATATATAATTAGAGGATAAGACTCAGCAACCCGGAAGTCTCCGTTAAAGGGGACATAGTTAATCCC',\n",
       " 'CTCAATAATCATTCCTAGACCGACATCTATGGGACCCGGAAGTGTTGACATCTGACCACGATGGCCTGAAAGCGGAAGTTCAAATATGCAAACCCGGAAGTTTTGGCCACCGGTTGGTCTGTTCGACCGTTTCTGCTCAGAGCGCCAGTTACGAGTCTCATACAACGGCGGGACAATTAAGACACGCTCAAGTTTTGGAA',\n",
       " 'AGCACCTATAGTTAGGGCCTAAGTCTAGACATTGACATAAAGCCTTATCAGCTCACTAGAAATTATGCTTAATGGAATCCTCTCGTGATACTGGTTAGTCCATAAAAAGCGGACTTTGTATCATTATGTATCCCCCTGAAGGGACAGTGAAATTATGAATGACTGATAGTGGAGCTGCCCTTTCGTTTAGGCGTCGGTGT',\n",
       " 'CATCGAATTTTCCTGCCAGGGGTCAAAGGTCATATCACAACGAACACTATTAATCGGCTTTAAGGGGTCAAAGGTCACACGTACTCTAAGGCCTACCCTGACCGCGGATAGCGATATCCATGTAAAATGTACCACTACAGTGACTGATGCATAGCACGTGCCAGAGTGAACCGTGAGGGTCAGTTAAGTACAGCCTTCTG',\n",
       " 'CAAGCCTAAGACAATCTCAATTTAGGTCTACAATTGAGCAATAAATACACCTGAACATAGAGATGCGTTATAACGGACTAAAGAAGTAGGAGACTGACGATCTACTCGCATAGTTACTTCCAGTATGGACCATAAGATATGTACGTGATAATGGCAGACATGTTGTGGATATAAAATTACCAGATAAAAGGTGCCAAATA',\n",
       " 'CACGGAAAGCTGCGCAGGTAAGAGCAATCGCACTTTGCATCGATTTCCGGTTTTAGCTTTCATCTGTATCAAGACTGAATTCGGTGGGTCACTTCCGGGTTAAGTAACATAACATCCGTCAAGTGTAGAGTGCCTTACGATTGTTGCCTTATATCGTTCTGAATTTAAACTACTTCCGGGTAAACTACTGGTGCTATAAC',\n",
       " 'GGATAGATTAGAATACTTGATGCTTATGCTTATCTCCTGAAATCCGAGAACGTGGGAATTGAACTTTCATTCAGTACAACCTAGGTATATTCTCACTTGGGCTATACCACACAATATAAGGTAACGATATCGTGGGCCATACTTCTCCTACTCTTTATGGTGTGGGTCGAGATCGACTTAAAAATTTGTTCGAATAATTT',\n",
       " 'TACAATTAGATTAATATATTCGATCTATCAGATAAACCCATCACGTGAACCATCGGTCGATGTCGTCCGAGTACTCCATTTACGGTAACTGATAAGATGGTCCGAGCCCATAAAGAAAGAGATCTGAAAAAGTTAGAGCTCCATCCGTGGGACATAGCAAGTTTCATCACTTTGCGTCTTATCAGCTTCATAGTAAATCT',\n",
       " 'ACACGCTCTCTAGAGCAGTTGTATTAGCTACATAGCGGTGTAGACTAGGTCACTTTATTATATGAGAGTATTGATGAGCGCATCATATCTGTCTCTCGAGAACCGATGTGACGTGCTGTTGTACCTAGCGGTTTCTAGGGGGTCAAAGGTCACCGCAATCTGTCGCCCAACTGTAACGTACGTTCGCGCACGCCTCAGCT',\n",
       " 'CTGCTAGTTGTCTCGGTCGCCCCGTGACCTTTGACCCTCTACAGGAGGACCGAATGTGTGAAAAGATGTTACTCAGCACCTTTTGGTGACGCGGTGTATCAAGTGGAGGTTTACTTCTCCTCTCATTCTAGATTAAAGGCCCTCCTCAATTCGGCCGCCGTTAAGTCGTATCAGCTTGACCTTTGACCCCTGTCTTTCAG',\n",
       " 'CCGCGATCACAAAGGAGAGCCCAAACCAACACGTAGAGCTCTATAATTCACCGCATTTAAGGCTCTCTTGCACACTTCCGGGTAAGCATTTGTGTATACATCCTGTCTTCAGTTCTATGTTCCAACGGGCGAAGATTAGATTCGAGCGGTGAATTAAGTACGGGTCCATTCCCTCACATTAGATAATTCCACTAGTCACA',\n",
       " 'TTCAGTTACCAAACCCGGAAGTACACGTCATCAATAATAAGGTATCAATCATCCGAACTTTTCATCACTTCCGGGTACTTTCTAGCGAAGCTTCCATAAACTATCAAACTAACTGTAAGATTCCGGAAATGGAACAAACATAAGCAGAACCCAGAGCCTATAGATACTTCGGATAGGTACGGTTTAACGCATTTAGGCTA',\n",
       " 'TCAATTGACTTCCGGGTTTTAGTTTAATCTTGTATGCGTACCTTAACTAATATCGCTAAACCCGCCATTGATGACTCGCCCCGGAAGTATTTTAACGTACCATCGCAAAGTTAGAAGAATGGCCTACACCCTTGACGCACTGGAGAAATATACGATGATCTTTCGACCTAGTCTGTTAGTCCGGTGGGTGTTATGCTTTC',\n",
       " 'CGGGTTTTGTCATAAGGTAATTCCCCACTACAGGAGGCACTGTATCCGAACTTGGGCTCAGGTAAGCAAGTCAAAGCTGAACCCGGAAGTATAATAGCGCCACAAATAAGGCTTTATCCTAAGTCCGTATAATGGGCTTTCTCAAAAGCCGGAAGTACTTATTGGTCATATACAGACACTTCCGGGTTACAGCGAGGCCG',\n",
       " 'CTTTATCCATAAGAAATTTCCACACCTGCCTAACCAGAAATCAGCTCTAATAGAGAATTGTAGAACATTATGTTTAGTCTCTTTAGTTAGAGAGTCGTTACGAGGGGATCGATCGCTGATGGGTCAAAGGTCATCCAAAAAAAGCGAATCGTCGATTACTAGTCTGCATCACCTTTGACCTCTCTGGTTGCGAACTCGAG',\n",
       " 'AGTAATAAAAATCCACCTATCTGAATAATATCTTTATCTGACGACGTCTACCTTACTTCTTGTAATTTACGGCCCCTATCTAAATCACTTGGCTTCTATATTGAGGACATCTTGAGTCTGTGTCACAGTAACGTTATCCAAAAATTCAAGATCAATTGAGTCATAAAAGATCGGACCAATTATCGGATCATTGTCTCTTT',\n",
       " 'GTGTTCACTATTTAAGTAGCAATAGCTTGGTAAACTAGTAACATTATGTTTTGAAAGCTAAACTACCCGAAGATTTGACATACCGATGGTCCCACATCCCCGGAAGTACGCAATGCATCCGGATTGACACAACTTAAATTAGGAAATAGCAAAAGAGCTACCTTTTACGGCCATAACCCTTCTCTTCTTCTTCAAGTTCT',\n",
       " 'TTTGTAAAAATATAGTCAAAGGATGGACGACATTTGTCCGTCGATACCTGTAACCGAATTAGTTAGGTTATCCACCTATTTGCAACACCGGGTCAAAGGTCAACAAAGGAGGGTAAGAAAATTATAGTTGTAAGTAAAATATATACAAGCGAATTAGAGGGATTTTACGCGCCACCAGGCGTGAATTTAATGCCTAGCCT',\n",
       " 'CTTTTAGGCAAGGATGACCTTTGACCCCTGCTAAACAAGTTTCGAATATTGCTTTACTATTGTATTGCGATACCATGCATTTGTAAAAAAATTCAAGATTTTGTTGAATGGCTCTGTCTTTAAATTCATATGAGTAGCTACTTGTATATGGGAAAACATCTCACGTCGTGCCTTCCTGAAAAAAGGCAATAATCCGAAAT',\n",
       " 'CAAATTCAAAAAACGTATGAATTCTGGCCCAATGTAGGTTTTACTAGTAGCCTAAGCGTCTCCATGCCACCCATGCCGAAACGACTAACGCTGCGCCTGATGTCTACCGTAATAAATTAACAAGCATTATTTGCAGAATATGATAAACAGAACTCTAGTGGGAAGCGGTGCGAACACACTGAGCGGGTATGATGAGATCG',\n",
       " 'GGGTGAGTTGGTGTTGGATCAGTTCTTAGTGTCCGACTCATCTTTACTCTGTTCAGAAGATGCATTTGCAAGGTAAAACACCGTTCTTGCTGTTTTAACTCTACTCTTTTCACAGATTTCCGGGGTTACAAAGACAAATAGAGGTAGAGCTAAATACCAACTTCCGGTATTGTCGTTTAGCTGTAAAAATGAAAGAGAGA',\n",
       " 'AAATCGTTTAACAGACTCATAGGCAGTACTCGTAGGATGTTATCCTTAACTCTACTCATTGTAGACGATAATATTACCACTTATCAGGTCCATTTATCTCATTCTCATTCTTCGGCATCGGTGAATAAGTCTTAGGTGTCGTCGAGAAGCGACGAAACATCGCATCGTGTTAAATGAAGGCTCTAATGGAATCTAGAAGT',\n",
       " 'GTGATAAACAGCGGATCCATCGGCTCCTGTTATCTGGCCACCTGATCATTATAAGCGCAATGTCTAACAATTATTGAAATCTGCTCGGAGACATGGCGTTTGTTATATGGCAGACGCACTTGTGCTTTTATCTGGTATTTGTACATCACCACAGTGCAAGCTACTGATTGATAAGATAGAACCATCAGACTGATAGGTTT',\n",
       " 'TTTGCATAATTACGAAATTCGCTTTCGTGAGCAAAACCGATTTTGTGAACACTCGTCCAGTATTAAATGACTTGTAATATCTCTTCCTCAGTAACAATGGGAACACTTTAGTTATCACCGTTTTATCTGCCTCAAAGCCGTTATTACGTAGGTAATTGTATCTGAGTAATGCTCCCGGTTATGTGAGATAGAGACGTTTC',\n",
       " 'AAAAGATGTATATTCGATGTCTGGAGTAATATTATTGATGCGTCAAGTCAATTGCCGGAACCTTACACCTGTGCGCAAGGTTAAGGTTGAGGTCAAAGGTCATCGTGATTTTGAGGTCAAAGGTCACGATACTGTCACTTGTTACCCCTAATTAGAAGATGTCGCACAATGAATTCAATAAAGTGTGTTTACAACACCTA',\n",
       " 'TATCGGTCGTATACCACTACGTAGGTACAAATCCATTATCTTCTAGGGCATGTCTTGGAAATCATGATCGGTTAAGGATCATTCTATGGTGGCGAATTCACTCCACGGCGTTCTTAAGTTTCGTCGGGCTCGTTAAAAAACATGTCCTAATATCTATAAGTGCCAATTCGACCCCTATCTCAAATTAACAGAGGGTATGA',\n",
       " 'ACTTGATGGGCAAGGGTCAAAGGTCGCACGATATATGTGCACGACTAAGAGAGCGAATTACATAGCCCTACACCCGACAATCTTAATTTCAAACTGGAATAGTGACGGGGGCAATAGAAACCTATAATGAAACCCACACGTACGGACAGGGCCGGTGCAAAGCCTACATGAAGTAAAACTGGAGGTCAAAGGTCATAAGC',\n",
       " 'AACTTAGAAGCGCACAATACAACATTACGGTCGACCCCCACGAACACTCTAACGAATCGTGCGTTATGTATAGCAATGTGCATGTCCGAAAACCCATAGAAATTTACGCTAAATCTACTACAGGAATGGGGCGTTTGTTGATAAAATAAGATCTCTCCTCTGATATAACGATAACCTGTTGGGGTCAAAGGTCACAGGCT',\n",
       " 'ATGGTAATAAAGATGTCCTGATCAGTACAGTACGATGTGCGACGAAATGCGTTCTGAGGGTTTCCGCGGATAAGACGGCAGCACATGTTCGCCCACATCTTCAAATTATACGCGGCACATCTAGCGTAAACTCTTCAGACGCATGCAATCGTCCGAACGTTCTGTATTGGTCGCCGGTATAAGAATCACCTATGAATATA',\n",
       " 'CGTAAGATCTTACAAATAATTGGAAGTTGCATGACCTTTGACCCCTAGTCTGGTATGGTCTCTCGTTATCTTAATCCTTCATGGCCATACCTCCATGGTTTTATCGAAGCGTATGCGACCCCTAGATTATTATAATTTGTGACTCCAGATTATGCATTACTAGTAAAGAGCTGTCCAAAAAATACTAGCTCGCCTTTGAT',\n",
       " 'TCGGGTCTTCAGACTGATACACGCATGCCTTATACATTAATTATGCGCTCATGGGTTACTTTCTTTTTCCGGGCGACTTAATTATGATGACCATTTGCTACACATGACCTTTGACCCCTACCTAGGTGACCTTTGACCTCTTCGGATGCTCATGTACCAGAAAGCCACACGTTGTCACACTTGACCTTTGACCTCTGGTC',\n",
       " 'CTACTACGAAAGATCCTTTAGGAGCCGCTTACGTAAGTGCACACTTTCTGAGTTCGTGATTAGAATTGACTTCTTCTCGAAATCAGTCGTAGATAGTTCCCTTTGCAATACTAGTTACGTGTTATCAAGGCATACTCACAGAGAGGGTTAGTTTTCGAGAAGAGAGCACCACAGTGCCAAGACGGGTCAAAGGTCAAGGA',\n",
       " 'GCCCGGTCAGGCTACAGAGGTGCTGATCATTAGACTTGACACAGCTGCTTGTATTTCTAGTATGTTCACTTCCGGGTTTGAAACTCTGCTTTCTCACTTCCGTGATCCGCGTTCTTTATTCATCCACATGTGGCATATCCTTCACCTACTACACTTTAATTTTCCTCGAAAAGCTCACAACTGTTTTTTCGGACCCCTAC',\n",
       " 'TGCACATTCCTCTGTGAAAACACTGTAGGGGCTTCATTTATGATATGATTGTCTCAAAGACTTCCGGGCTTGCCTGTAGATCATGACTCCTTCTTAGTAACAGTAATCAAGTTATTAACAACAATGTCCGATATCGGACTTGCAGGTTACATTCAGATCACACTATGGCCTTAATGAACTTAAACCTTGGTAACTAGCAC',\n",
       " 'AAGAGTAAGGACCGTCCGTCTGCGCGCGGCGGAAATAATATGTATGTCTAGACCTACTTGACCCGGAAGTGTAAGGTATATAAAGGGACGATAAGTTATTTGTATATCTATGTAAAAATTGGAGACATGGAACGGTTACGCCATCGTTCCAGTCATAAATAGTACTCAGGTCGTTATATTTTAAAAAAATTACACTCTTT',\n",
       " 'GCTATCTGCTCAGGAAAAAACGTCTATGATTAAGCTGCATCGCGGCGTTGCACTATACTGCTACATGTCTCAGTACCGTGTTCTTGATACTGTACCTATCAGCATGTAAAATTTTTACCAAGATAAACCTAGCTCGACAGCCTGTGATCTTATCAGACGGCCTAAGGCAGAATTATGTTGACATTGCCTGAAACTGGTTC',\n",
       " 'GCAAAAGGGTGTACCATTGAATGAGCCACAGACTATGTGGGTATTTTGACCTTTGACCTTAGGCTAGTTGAATCTGAGTGAATGGTTCAAGTTCGATAGAATCAGCTGCCGAGCACATTTTGTCAATGACCTTTGACCCCTGAAAAGACTTGGATCCACATTTTATCCTTGTGGCAACGTTTCTGCCCTACTGGCGACTG',\n",
       " 'GTCCGTAAAACTCCTTGTGACCGATCATATTAAATAATACATCGCCGTACGTCAGATACAGTTCCAGAGCCTAATGTATGACCTTTGACCCCCCCTCTTCGAATACCCATGTAATCCTATCTCGAATGGTTTATTGACGTTGGCACAGAAGTTGATCAACGTTTTTTGAGAAATTTGTGTATCAAGTATTTTGCTTGATA',\n",
       " 'TATGTTCCATAAAGTTCTTTGTCGGTCCGTGTGTTGCTGTTTACATTAACTTAATTGGGAAATGCGACTAAAATACTTTTTCTTCATATTATTATAGATAATACTACCCGGCACTCACCTATCATATTGGCACATAGGTAACTCTCTATACCGGAAGTATCCATTTATTACTGAGTATTACTTCCGGGTTTGTATTTCCC',\n",
       " 'CAGGAAAATGAGACATACTCGGTTGACCTTTGACCTCTAGTTTTGCACAGTAGTAGTGTAAGAACATTCTTACAATCTTGGATATCGGCTCAGGTGGGTTTTGAGACCCTGGGGTCAAAGGTCATCAGGAAATTACTCGGCAAATGAGTGCTTGCATCTTAGAAATGCTTCATGGCTTGCCAAGTAGACACCCCAACAAT',\n",
       " 'AGGATTATGTTAAGTATGAGACCGTTAAGGTCTCCAGGTACTCGGCGTTGACGCGAGCAGTTACATCTTTAGCGAGGGGACCCGGAAATATGTAGGATCGCATTGATGATTGACCCTACATACCCGGAAGTGGATCGGTCTGAGCACTACTTACTAGGCGACTTTGGTCATTGCCGTTAACTCTTTCTGTCCCAACACAT',\n",
       " 'ATAGGAATGATTTTACCCAGCACTATTTTGCAAAATTTTCGAGTGACTTCCGGGTTCGCAAGCCGCACACGGATCGGCTTACACCACTTATCTATCGTCGAAGCGATAAGCATGCGTCCCGAAAGACGTACTTCCGGGTTGCGCATGGACTACTAACCCGGAAACCAAAGTTATTGAGCTTTTTAAGTATGATCTATTTC',\n",
       " 'ATCCGCGCAGCAATTCGCATATACCCGGAAGTCCGGTCATTGTGCAAAACCTACGTCTTACGGGAGATCCGGGTTAAAGATACGTTAGAAACCCGGAAGTAACTTTCATAATGAACAGTCCGTGGAAATTGCATGATTAATTATATAGGAAGGATTCTTTGAAGATTTTGTATAGCATTATCCGACATAACAGATCGCGG',\n",
       " 'AGTCCAACTCGGTGGTGACGCCTTATCTGGCACGTTGACGATATCAATGCTTACAAAATGGCTGACCGGTGGTCCACTAAGCCCACGGGGTCAAAGGTCATAGTCTGATCCGATTAAATCTTTGACCTTTGACCCTTCGAGAGAAGTTTTAGTGTTGGCTAGCATTCAGACTCCAAAACTCCATATACTTAGTTCGGTCG',\n",
       " 'GCTTAAGACTTCGAACGGAATCACCGATTCTTGTACTCTTCGCTCTCGTTATAAATGTGATAGATTTATTCCGATACAATATTTATCTGATTATGGGCTTTCGGAAGCGATGAATAATTTGTCTTTAATAGAAAAACAATTACGTGTAAGAGCGACGATAGCGAACTCTGTCGAACGAAAAACCAAGACTAGTACATATG',\n",
       " 'AAGTAGTTCGACCACTAAGGAACTCATGGGGACAAGCATAAGGGGTCAAAGGTCATAGTCAAAAATACAGCCCATTATGACCTTTGACCTCCGCTTATATATACTCCTTATTAGTACCACTGGGCATAAGTTTCATGGCCATCGTCGATCTCCTTAGGAAGCTATAAGGGACTTGTCGCTTGTCGAATTCCAACCTCGGT',\n",
       " 'GAAGGGGTATAAGAGTTATAGTGTGACAATGAATCATGTAAAGACGGAGCAACCTGTCTTGTGGCGTCCTCTTACACAGACCACCCGCTCTACGATATACTACTCAACTTCCGGGTTAGTCTAATGGGTAATATTCCTGAACCCGGAAGTAGACAATCGAACGCTACGTTTCACCCGGAAGTGGCTTCACGAATAAGTAT',\n",
       " 'TTTTTTTATCGATGGTATTAGTACAACCTGAGGCGTCAATGTATGTTTACCTGTGTATTGCTTCGCATTAGCAATATCACCTGCAAACACTCTAGTTTAATATATAACCGGCCACGTGCGTCAATAGGTGGTGTTTAAGATTACGGTGATCCCCCAAATGCTTGTCGGATCGGCATAACGTGAAAACATCTAGACTGTTC',\n",
       " 'TTATCTGTTACATATAAGTTCTTTCAGTGCCATCATCACGCAATATAGATTGTGCTGGTTTGCGATTACTAAGTCATCGGTAACCGTTTATAGAGAAGTACTCTATAATCGTCAAATCTCTATCCATGTACTTCCGGGTGATGCAGAGTCATACTCATGACGTGTACGAGCCGGAAGTTCGAATCGGTTGAGCTTTATCG',\n",
       " 'AGCCTTCTAGCACGCCGTTAATATCACAAGTAGTTCAAATCCGGAAATAACATATAAACGCATTACGATTTCCGCGTTATTAGGAATAGTGAAATATCGTTTATCCTATAAACCAAAGTTGAACCTAATGAAATATCGATGTTGCTCGGCTTTTAATGTACTTTTCCGCACATTATGGCGACATTTATCTAAGGGAGAGA',\n",
       " 'GCGGTTTCAAGCCAGCTTTATCTCCATAGAATCTGTCGCCCAACTGTAACGTACGTTCGCGCACGCCTCAGCTAGTCATGGGCGGACTCACATACATGATACGCTACAGGCCAGTCTTCGTCTATTTTTTTGATAGATAAATACGAGGTGGAAATCCTCTTTTCACTTCCGGGTTGTAGTGAAAGTCCTGCTGCCCAAAA',\n",
       " 'AAAGCTCCATTTCCGGTTATGATTATTGTTAACATAGACGCTATAGTATTTTAACGTTGTTCATAGCCTTTCTACCTCACTAGATAATATAACGTACTAGTCACTTGCGCATTGTATTATATTAAATGACACTCCTATTCATTTTGAGGTAATGTACCGGCGACAGACTTCCCTTTTACGCTACAACAACGAATAACTCA',\n",
       " 'TTAACCTTTAGGATTGCAAATAAATATATAAAATTTTCTGCAATGTAATCCATATCATGAAATTTCAATGCTGCCCAAACCGCCACAGGATAAATTTGATAATGCGGATTCCAAGAAAGCAAGTGTCGATTATAGTCAGCGTGACCTTTGACCTCTCGCGTTAACAGAACCATATAGCTCGTGACCTTTGACCCTTAAAA',\n",
       " 'TTAATTTATCTTTTATGACACTTATGAAATCATGGTCAACGGTTATGTCTTATATGCAGATAGGCCGAATATATAGAATTGAGATAAGACGAAAAAAAGTTCGAGGCTTCTTTTTTGATCAATCTTAGAACGTTTGTGGGATAAGGCGCCAGAACTCCGTACTTTTAATAGTTGCTATCACCCAGGCGGCTCATCAATAC',\n",
       " 'GCTTTGGCGAGTGGAGAAATCTTGCCTTATCTGACATTCTACACAGTTTCCTCCTTAGAGGTCAAAGGTCAACATCATACTCGGGTCAAAGGTCATGTGCGAAGACTTGAGATCCAGATGGAGCAGGCTTTGGCGGTACCGGTGTGGTCAACTTATAGATATCTGGGTAAACATAACCTCGAAATCCATAAAATGCATCA',\n",
       " 'AGGCCGCGTTTCCGCGTTTACTAGGATAGTCGTCTTAAGGCTGATAACACATCAAGCTACTGTTAATCTACTCTCTATCGGATAACCAAAGGACCCGTCTGTGAGTAGGGGGTGAATTCAGAAAGTGTATGAACGCCTATCTACACATAAACAGGAAACAAACGATTTTCCCTTTTGCAACGACGCTATTCCGGTTTTTT',\n",
       " 'GATAAGGTCAAAGGTCACACTCTAGACATCAATAGAAGAACTCCTATAAGATGCCTAACGCGATAATCCACGTTTAGCTCAGACGCGTGACCTTTGACCTCAGCAAATCTTTTTTTTAAGCCCCAGCAAAAGGGTCATCAGAATAAACCAAAATAATTAATCCATCATGACCATTCGTCGGGTTGATACATTAATCTCGG',\n",
       " 'CCATCGAGTCATCCGGAGAGTCTATGAAACCAATGGCTAGCTCGTATTGCTATCCCAATCCGGAAATGTAACCTACATCTTTAACTCGTATACTTGCTTGCTAGAAGCCGAACCCGGAAGTGAAGGTACTCTGACAATTATATAACACTTCCGGGGTCATTGCATACATTACTAAGGCTGACGAGATAGTAGTTACTCCA',\n",
       " 'CCGACTCCCTTTCACATTAAGGCATAGTTTTAAGTTGTATACTGCCCGTCCGTCATCAGTATTACTACTGAAAGGCTTCCGAAATCTTAACGTATGGCGAGACTCTAATCTCCTAAGAGATGACCTTTGACCCCATAATTAGACAGGGTTCGTGAGGTCAAAGGTCAACTGTCTAATTAGATATCATTTGTTTTAGTATT',\n",
       " 'AACTTAATGATCTGCTTACTACTAGCACCAATCTGAAGCGTGGCATCCTAGCTAATCTATGCCGAGGGGTCAAAGGTCAAATTGTAGATGCTATAGGCGAAGAAAAAAAGAACTACACCCGCCTATCAGGGCAATTCAACGTTGGGAATAGCCAAGGATTCAAAATGTGCTATTGTTACGAAAAACGTAGCTAAAACTTA',\n",
       " 'ATGCTAGCGATCGATGGAATTAAGGAATGAACTTAGGTCGATTCCCGTAGTCAGCATCGCTTCCGGCTCTCACTTCCGGTTCCTCCTCAAAAATAGCTACATTTATAATTAGGGGTTAGGACGGACCTCTGTTTATCTTGATACACGTAATTCGATTGACATATTTCTCTTTCATCATTCCCGTGGTAATCCGTATCTGT',\n",
       " 'TATGGGGAACTATCATAACAGTTCCTCGCAAGCCGTGTTGGATGAGAAGCACATAGCACTTTAACCCTTTATCATCTATAGGTTTTCGAAGAATAAGTGCTGGATTATCATTATCAGTTGAGAGATCCAATCTACCAATTGAAAGCGGAATTGAGTCATTGGAGCGTAGGCAGATAAAGAGCGCCGGAGCCTATCGAGCG',\n",
       " 'GGTAGCGAGTACTGAACAGGTTGCTTTAGAGCTGGAAGTTCAGAATAGTCATAGCACTTAATATGCGCTAGCTAGATCCAGTAAATTATTGCACACATTAGTGTTGGTACGAGTATAACCCGGAAGTGGAAATAGGATTTAGGCCAAGCAATTCCTATACCCCTGCCTCTTTATTGACAGTTCAGGGTATATGACTCTTG',\n",
       " 'TACAGGCTGATAATGATCGAAGGTACTTCTTGCTAGTTACATAAGTCACAGGCGTTCCACCTCCACACCGTTTTCAAACACATGTCCTCGTGTCTAGGTTACGCGGCCCGAAAAGCAATTTCGTCTCGGCGAAGTTACTTAACTGGCAGCCTATAAGATGAAGGGATTGATAAGAGGATATCTTATATCAAGCGTAAATG',\n",
       " 'AAATTGGTATTCAGATATCGAATCTGGCAAATTGACACTAATTGTTATTGTGATCACTGTACTGGTTATAGAGAGTCATTTCTACGAGATATTAAGTAAAGGAAATTACCCACTTATATTATATGTCTAATAACGCTCTTTACAATGTATGGTTAACCCTTATCTGCTTGCGTGCTAGAATCAAAGCGATCACGTGCCAT',\n",
       " 'CCTATCCAAAGAATTGGAAGATGGAATAATATATTTATCCACTCATGATAGATACTAACCCAATAAAGTAATAATAATGAGAAAGGGTGCCACAAGTACAAACGATAGTTGGCAGATACAGAGGACTACATACGAACGCATTTCATCTCATGGTAACACGTTTGTTGGATTCCATATTGCTAATGCCTCGTGGTATCGAA',\n",
       " 'CGCTAGACCTTACAGAGGCGTTATCACGAGGGGTAGTCCCTTGATCGGTGGTGGCCACCGCTTACAAAACCAAATGATCATGATTATTTTTACACGGAGTCCCCTAAGAAAACAGACAAGGCTTTCAGATAGGACTATTCTTATAATTGGACTCGTAACGTTATCACATGAGTCAGACTATGAAGTATATCTCTTATTAC',\n",
       " 'GAATTCACCTATCTGACAATATCATTCTTAGAAACACTGGTTCTCAGGGTCTCACGTTGTCCCGTAAGAGGTTGCTACTCAATGTAAAGAGATCCCGCTCTATAGCGTCATACCCGGAAGTATGACGTCTGAAAGATCACATATATCAACTCATACGCTACTCAACAAAAGCTAAAACAGATCATGCCTTCGTCTAACTT',\n",
       " 'TGGACTGCGGCATGTTAAAAACACTAGCATCATTTTTGAGCTTACATACATTCAACGGTTACCTTTTAGCACCCGAGTATGGCTTAGTATTGACATTTAAAATTAATTTAACTTCCGGGTTAATTAAGATATTTTAAGTGTCCGCTTTAAACTGATACACTTCCGGGTTTTCACCATAATCGTAGTTGTTTGAGTTAATC',\n",
       " 'GATGTCTTAATTGTTTGTTATCGGTGACCTTTGACCCTTGTTGGAGAGGGATTCGCCATTTCTAGACCAGTTCATTAAGAAATATCTCAGTTTAAAACACACATTAACTATCCAGGAATAAATAAAAGCGAACTTCCACAGGCGACGTCTTTAAAGGTCAAAGGTCATCGAGTTTTAGGCGCTAATTAGCACTATATTTA',\n",
       " 'ATAGTACTTTGGTCATCCTCGTATAGGGTAAACTAAGCCCATAATAGTCTAACTCGCAAAAGACGCTTAAAGTAGTATAACCCTAGGGCGGACTGATAGCGCGACCATTGCCGAAGTTGTATTTCACCTTATAACTGATATAAGACTTCTGATGTGTATATGATAAGAGAAATGACGATTCTCCAGGAGTCCCCGCTGTT',\n",
       " 'GTCATATATTGATAGGCTATCATAAACTAGTAAAGAGTTTAATGATTAATGTGGATCCTGATAACCGGTTAAGCTTTTGCCGTAGCCGCAGGTTGATTGGAAACATTTGAGGTTCGTACATAAAGGCCGAAATTCGAATTGATTTAAGCCGCAAGCTGTTTACTTTATCACGCCTAGACGAATCTATTGGGTAAATTCTT',\n",
       " 'GGTTGTCGATATGTATCTATACGTATGAATATTACCTTAATCAAGGTCCCAAATAGTACAGAGAGTTAAACTCTAAAGTAAGCTATCCTAGACATGTGATACGAGAACTCGTTAATACAATTAAGACCTAGCGTCAACTCTAATCATTACTTCCGGGTTGAACTGTTATGAAGTGGTTGTGAGTACATACTAACAAGTCC',\n",
       " 'GCGCTTGAACGGTAACTGGTATGGCGTCGCAGACATGGCCACCACATCATTTCCTTGCGATTTCATAACGACTCAAACATATCAAGCTGAGCTACGGTCTGATTAATATGTAAAAATTCACTAGCTGCATGCAAATGATTCTGTCAAGAGCATGGACCCTTGAGATAAGGCCGCTCAATTAGCCTGTCCTTGTGATGCTA',\n",
       " 'TTCGTCTCAGGTAAGACTTCTCCATGAAGAGGTCAAAGGTCACATGACATATTATCGACTTGATTAGGATTGTCGCAAAATACGGTCGTGTGCTATCTATATGATTCTACCGTTTTATATTAAAAGGGTAAACGCTAACGGGGCACTACTTGGATTTCGGGTCAAAGGTCACCTTTAAGATCAGTAACGGATATTGGGAT',\n",
       " 'TAGGGAACTCACTTCCGGGTCTCGATAACAAAGTTTGTACAACTCGAAGACGAGTACCTCGAGGCAACAGTCGTAGAGACCCGGAAGTGATTACTCTAAGATGAGACATTACTAAAGGGCCAAGATTACGGGAACCCCCGGTGATACCAGCATGGTTCTACAGATTATTTAAAAATCTCGGACAAAATTCAACATTAATC',\n",
       " 'CGGTCAAGAATGAAAAAATTTTAATCCAGATGTATCTTTATGAGGTCAAAGGTCATGAAAAAAGTGAGTGTTTTGCTAGAACTTGTCGCCAGAAAGGGGAGGTGCTTCACGGTCTAAAAATATAGTTCCACCTATTTAAAAATGTAAGCTTCGGGTGAAAGGTCAAAGGTCATACGGGCTAAAGTGAGATGTAGATTAAC',\n",
       " 'AATTTCAGTGAAGGTAATGTTTGTTTTAAAAAGTTTATACGTCACGAAGTAATTATTACACCCTATCATGAACCTAATGACAAACCCGTGTGATAGGCGCTCGTCTCTTAAGTTTTCCTGGACAGCTACGCCTCCGACACGCGACTGTTAACCTTTTATCAGCCTAGCTACGGCAACGAAGTAAAGACGTAAATCCTTGT',\n",
       " 'GTCTAATCCACATCTTATCAAACTTTCCGACTTAGAATTCCCCTTATCGGCCCTTTCTCACCTTAGTTCATCAGATGATAAGTGCATCAAACATCAGATAGATAGATAACGCGGCGAGTTAGAATACCGCCTATCCGCTTGATAAGGCGTGACGCGCCACAAGGAAGATCTATCGGACACCGCGACCACGTAGTTAAGTC',\n",
       " 'TATAGTTCCAGCACACAAGTGAGATTACAGCGAACTATCCTTACGTAGTAGTTATAAGATTCTTTAGTAAGAAATAGTTGTATCTCATATCACAGATATAACACCTCGTTAAAGACCTACAAGATGAACAAAATCCTCTGTCGTCTCGAAATTGATAATTTTAATAGATCCGCGCTTCCCAATGTATCCGGAAGCAGAAC',\n",
       " 'ATGAGTGCGGCTGTTAAGAGAAGGGCTCCAAAATAGTCCATTCGAATAGCCGTTAACTCAATTGAACAATACATCGTATAATAAACACGGCTCGACTATAGTCACTTCCGGGTTACTAAACTGCGGGTCCGTAAAACTCCTTTACTTCCGGGTTATTAAATAATACATCGCCGTACGTCAGATACAGTTCCAGAGCCTAA',\n",
       " 'AATCTGACAGTTTAGCACACCCCGTACTACGCAGCTATGAGAAGCATAAGCCATTATTCATCTATGATAATTCCGTTTTAGGCGCTCAGGATGTCTGAGAGCGCTTGTAGTGCAACTCATCCTCCTGAGTCCGGTGGATCCACACGTGAAGATGGTCAGTCGAGTTTCAGATAAGTTGTAGCCTAATATGGGGGTTCTAA',\n",
       " 'TGTTAAGTTACAAAAGTAATGGGAATGCTGCATTAGCACTTAGTGTCATTAGAGAGGGCACATGATTATAATCATTGGTGTTCGTGGGTCAGGACTAGTCAAGAGCGAGTTCCTATCTGTTACTGTATATAAGTTTGAGGCATTTTCAAATTAGGTCCATAAACCACGAACAGTTTTCTATTTTATCGAGATAGGGACGA',\n",
       " 'ACCGTCTAACGCGACCATTGGGGTCAAAGGTCAAGCGAATAGGCGTCGTGTCGTTTACGCGGCTAACGTCTGACCTTTGACCCCATGAGGAAAATTTTGCATAATGATTTTACCGTTGAAAACAAGTTCTCGGTTTGAAAAGTTTCGGGTATCAATAAGGGTCAAAGGTCACAGAGCCCCGATTGCGAAATTAAACGTAT',\n",
       " 'TTAAACCCGGAAATTGAACTCCTCCTGAAGGTGCTAAACATCCAGCCGACTTCGTGCCTAATCATGGTTTATTACAGGTCATCAGTATCTCATCAGCCTCGCACCGTATTTTCCAGAAAACCATATTTTACCTCTCGCCTTTCAGTTTTGCAGGAATGGTTAACGAAGAACCCGGAAGTCAAGCTTGATAACCAGCACAT',\n",
       " 'CAAAAAAGTCATTGCGATTTCACGTATCGTTCGTGCTCTATCGAGGGTTTGCTTGGTGGCACGCTTCAGAAGTACCGTCACTTCCGGGACGCATGACATATACAACCGATAGATTCGCATAAAGTGCGTAGAAGAGTCCCACGACGAATGCGGAATTTAGCATTCGTAAATTAGTACCCATTTCCGGGTCTGTATCATAC',\n",
       " 'TTAGATCTATTGGGCATATTCCTGAATCATTCAAATGCTCATTCTGAGAATAGCGGCCTATCTCGAACTGTCATTGTCTAGACTTATCCCGCCTTAACAATGTATAGCGAAGTCCGGGTTATTTAATTTCCTTAGTATATGTCAAAGCGTCGTTCTCACTGAATCTCCTTATTTTACGAGACCCGCGGAGATTTCAATCA',\n",
       " 'CTCCTTACTACTCTGCCTTTGACGAAGAAAACCCGATATCAGCCCTTGCGGCATACACGACATTGACGAGTTAGCGATAACATATTCCATGTTCGTAAACTAAGATTCGTGAGCAGAATATAAAACTCGTAATAGAGTGGAGCAGTACATACCCAGGTTCAAAGGTCAACGACCGTTGTATTTCGAAATAGCCTTCCAAT',\n",
       " 'ATCCCATTATGTGGCCTCTTGAGATAGGCATAAGCTAACTCGACACTAAGATTTGTTACAGACTCCAGATATATGATGGCCAAAAAGGTAACCGTGTGAGCCCGTGTACGAGTTGCTATCACACACTCATGGATTGATTTAAGGTGCGATACATAGTCAGATGAAAGGCTAAAACTCAGGTAGTAGCGGAAGTTGATTTG',\n",
       " 'ATAAGTTTTTTTATATGATTAAGAGTGCGTACTGGCACTAACACCCAACCAGCATCCTGTGAATTAGTACCCTTATAATCATCCAATACCCGACTCTGTTCTGATCCCATCAAAATGGAAACAAGTCGCAATTGTCTGTCAAATTTGTTAAAAATAGTATTTATTGAGAACTTTTGTAGTCTTATCGGCTGAAAACACGA',\n",
       " 'AACGCGCCTCATTGTACGTTAATTACTTCCGGTTTTCCGCTGGGGTCTTCAATTTTCCAATGGTTTAAAAAAGTATTGCCTACTAGCATGATCCCGGAAGCATAAAGGGAAGAGGATTTTTACTGATTTTGGTACACTTACGTTAGGGGGAAATTTACAGCTATCTGCGTTGACACAGCGGGGTCGTATTACGAGGAGGC',\n",
       " 'GCGAGGTCACAGTTAAGAGATCCTCGGTTACTGAGAAGGATAGTATGATACAAGGGTTCTGGATAATAGTCACAGTCTTTCGGGGAGTGGTAGCATTGTTCCTCCACAGCAGTGGCGTACATAAGCTAATACCATCAAAGATTCGTGACCGTTTGACCTTTGACCCTTACTTTATGGGGTCAAAGGTCAGGTAAGTCCGG',\n",
       " 'ATAGGGGGCCATCTCACACTAGTATAAACTTAAGGTCGAGATTTCAAATATGCCGCGATGTTTCAAACTTGCATACTGCCAGAGAACCCAGGGCGTGCTGTATACGATGTTCGTTTATCGCCAAGTCACTTGGTCTAACCTGATAATCAAAAAGTCAAAGTGATTGAGGTTTAATAAGATGGCCTTATACAATCTGATAC',\n",
       " 'ACAGGATAGCTCGGTTACAGGCACGGGTCAAAGGTCAACTTCTTCCCGGGAGATAGGAATTACACATCCAACATAAGGTAGTCGTTGGAGACTCTGTCGTGATTAACTTTAAGTCTCTAACCCTGCTTTGGACGTACACTAATGAGCTACGCGTGTCTTTTTATGGTAGAGGTCAAAGGTCACAGGGAATCTAAAGCCAA',\n",
       " 'TATTTATTATTGCGTGTAAGGGGTCAAAGGTCACCCGTGTATCTAACGACTCGACGTTGACCTTTGACCTTTCCACCAAAATACTCTTAATAGATGGTAATCCGAAGTCACCAATACTATATGGAGAAGAGTGCCCAATTGCAAACCACTAAGGCCAGATTTCGAATTGATGGTGGGCTTAAGAGTATGTGCTAAAGTCT',\n",
       " 'GAATACGTAACCCACAATCCATAACTATATCAACCAAGGTCCAAGGTTGTAACTGCGTTTGGATCTACTAATATTTTACACCTACCTATATATTTCATGTCCCCGTTATCAAGTTCGGATGACCTTTGACCCTTGCACAACCTCGCATAATTAGAGGTCAAAGGTCACGTGATGCTAGTTTTACGACTGAAACGATTAAA',\n",
       " 'ACGGATCTACTTCCGGGTTATTTCAATCAGAAACTGAAAGGTCGGGATCATAATCCCCCGCCTGTTCTGTCGAATAATGTATAACGAATAACGACTTCCGGGTATGTCAAAAAGAAGCGTTATGTTTATTGCCAAAGATGGTAAAATAGCCGCTTCGATCTATTAATTTCAATTATATGCTCCTTGGGGAACTTATCGCA',\n",
       " 'GCGTTCATCTTCCACATAATCGATCTGATCCGAAGTAAATCGTGGATCTTGCTGATCAAAACGCTGCGATTTATACCCTGTGTAATCCACAATTTGTCTGAGCTGTTACCAGTTCGACTCGAAAAACTTCCGGGTCAGTTGACCGGAAGTATCAAGGAATACTTCCGCATTTCCTATGCAATACGTAGTTTCGATTGAAG',\n",
       " 'AAAATGCAGTCATTTTTCCTTTCGGAGTACCTGAGCGAACCAAGACACTCATGACGGCATTGTTATCCAGATATAACTGTTTACCACATTAGCATGTAAGCGATAAGCTCTATAGGCGACTATGTATATTTAGCTGATAAGAGATAAATAAGGTATAGAAGTTATTATAGATGTTTTTAGAGCAAAAGTAAGCGATCTCT',\n",
       " 'GTCACTTGCGCATTGTATTATATTAAATGACACTCCTATTCATTTTGAGGTAATGTACCGGCGACAGACTTCCCTTTTACGCTAGCAAGGGTCAAAGGTCAACGTGCAAGTGAATACAAGGAACCTAAAGTCAGGGGTCAAAGGTCACAAGTTAACATTCCACATTTATGCGTTACTCTCATATGACCTTTGACCTCTTG',\n",
       " 'CCAGCACGCTGGAGCAGACCATCAAGTCCTATTAATTTTAGCTCGCAATTCATTCAATGGTTAGGATAAAGCAAATTAACCCGGAAGCAAAACATTTGCCAAGAATAAGAGGCATTCCATTAAGAAAACCATCTATTAATCCTAAACGTCGTTATAGCCTTAACGCTGCATAATAAAAAACTAGCTTTCGGCTCTTATTT',\n",
       " 'AATGGAGAGGCTGTTTGCCATTCATTATCTTGATTCCTAAACCTTGATACATCATTAAAAGAACTGCGCTGCATCAACCCCACGTTAAAGGATAAGAATTGTCTTAAAAGTTGACTGGGAATTAGATTAGTTTATAGCAGTAAGTAACCCGGAAGAACTTAAACCACCAAATAATTTATGGTAAGAGACGGGATCCCTTG',\n",
       " 'TCCGGCTATGACCGTCCCCATGAACTCGAATGTCCACGCTTTCGTGAAACTTACTGTACGATGAACCAATATTTATGGGCGAAATAATAGTGTATCACACCGAATACACTGGTTTAACTTTGGACACACAACTACCTTAGAAAGATACTAACTGAAAATCCCAGAGGGTCTATCTTTGACCTTTCACCCCTAGTCTGAAG',\n",
       " 'CCAGGAATACCGGAAGTAACGATGTCATGAGACTTCCGGGTTTTCAGGCAAACCTTTTTGCATAGATCCAAATGCTAAATCTAGTCCGGGAATAATCAAGATACAATGTATCTGACGCGGAAGTGTCTGGTGATATGACTGAAGAACTCAGTGTGTCTCAGCGTGTAATCCCTTTTATTACTACTCCATCACGACAGAAG',\n",
       " 'ACGGTACTACGTTACGCGAAGATAGACCAGCCATCCATGACCTTTGACCTCTCCTGCCATAAATATTGGAGGCCGCCCGGTAGGGCATAGGGGTCAAAGGTCAACCACATGTTGGAGGTCAAAGGTCATATTCCGGGGCTGAATCTTGCAGCTCCGATGATAGAAGAGAGTTAAGCTAATAGAAAGGTAGATATTGAATT',\n",
       " 'TTAGAGGGAGACGGAGGAACATCCGTTTCAATATCAATGAAGACTTCAAACACTCCGCGGAAGTGTTTGAGTTCGGGCGTCTTTCTGAAGTCAGCTCACAAGTTAGTCGTGATATAATGTTGACTCAGTAGCGTGATGACGATTTCGGCGCTAGAAGCTAGGATTAAGCTCATACACGAGGCAGCGCGATTAAGCTAGGA',\n",
       " 'ACATGATGTTACCCACAAGGGTCAAGGGTCATATAATGGTCTAGACGGAAAACTAACCACTAAACCCAGTATTTCTTGTTTAATAAGGTTATTATCGTTTAATACGTAGACCGCGTGCAGGTGGCAAGAACGAATGATTGAGCGCATGTTAAATCATAAGAAGTATCAAAGACAAGCGAAATCTGCGATTGCTTTTCCAG',\n",
       " 'TACGTACGTAGCGCTGTCCCAAAAATAACAAAACTCTAGGCATGTTGCCCACTATTTCGCTGCAACAAGTATGCCCTGACATTAAGAAGTTACAGAAAATGCCCGTTTTATCTGTCTGAATGAATGATAGATGGATCGATCGTATGGACCGAGCTAATCACCATATCATTTCAACATGGATAGTCCTGCAGTACCTATTC',\n",
       " 'TCAATATATGTTTTAATAGCGTTCCTATATCTGGCACGATGAAACAGGACAGTTTACTGACGCGCAACTTAGTGGTCAGTGAAAAAATTTACAAAAAAGGTTCTCTTTTCAGTAGTAATTGCGCCTTCCACTCGTGCTTTATTCCACCGCGTAATTCGGCTTGCATGTCATTCATCGTGTATAATTCTGATCAAGTTAAG',\n",
       " 'TATTTTGAGTGGGGCTCTAAACGCTGGCGAGGAATATTACTAAGAAATGTTGTGGTAAGCTAACGCTATTGTATACTGCCATATCCATATATAATTAGATTAAGATTTAACATGGAAATCCTCCTTTTCCTTATCATGAATGGTAGATAAAGGGTATATTTTAAGCAATAGCTGTAGGGAACAAGCGCCCTGTATTGCAT',\n",
       " 'AACTCTTCGAATAATACCGCTTCGCGTCCTATCGGCGCAGTCTTTCTCAGAACATTTGGGAGTCGATCTTTAACTCCAGATATGGTTTACGATATATAACCCGGAAGTCATGTCTATACGAAATACCAATCCGACTAGTATCATGTAAACATATTACTTAATTGGGATGATTTTGGGTTGGAATCTCTCTTTACAACTCC',\n",
       " 'TTTAAACTCAGGAGAACTGTGATAGAGTTATCTTCAAGGTAGCTATCTGATGACGACTTGTAACTTAGATGACTACTCCCTTATCAGCTATATAACGGAGAGTTAAGTCCAATTTTCACTAACATCCGAGATATCTTTGAAGGACACAATCCTTCGGACAATGGGTAGCCCCTTCTCAATCTGTGATATCTGAGATAAGG',\n",
       " 'ACGGGATCTCCGCAGGTATAATTTGCTTTTATTCCTGCTCACGGAAATTCTCACGCTCTGGCTAGTTGGGGTCAAAGGTCACCCTGGTTAACGGATGATAATTGATTCGCTTAGCGTCACGAAGGCTGTGCGGGCTGAGTACCATCTGACCTTTGACCCCTACGTCAACAATCCTTTGTCTTCAAAAATGGGTATGCTGG',\n",
       " 'TCTAACACTCGATTTCGACTGATTAAGGGGGTATTAATACCCTGAGGTTGTGATATACCGGATGTCTTGCTTGTGCCCCCGGTTTGCTAGGCAAAGTCAACTGTTTCCATATAACGTCTTATATAGTCTCCGAGGGGTCAATTTATCATTGGCCCGGGGTGACCTTTGACCCCAAGTGAATAAGGATTGTTACGTGAAAT',\n",
       " 'AATTCGGCTTCTCAAGAATCGTTGCTTAATTGCGAAGTTTTAACAGAAAACTTCCGGGATACGTTAAAGGATTACTATCCGTAAGTCATAATCCGCGGAAGTCCTTATATAACTTGAACCTTTATATGTTTCGAACGCAAAGGGGACCTCCTACTTCCGGGTTCGAACTCAAAAATGTCCTTTGTAATGCTACCACACGG',\n",
       " 'CATAGGGTTCCGGGTCAAGCTGTAGGGGTCAAAGGTCAAATTTAATCTTACCAATGTTTACTTGACTCCATGAGGTCTTCTGTTTCGATATGCTTGGTCGCTTTCCTATCAGAAAGAGCATGTCGGAGCATCTACGGTAACTCGACCTTAGGAGATCTTGCAGAGACCATGACCTTCCTTACCATCAAGGCCGGGAAAAT',\n",
       " 'CCTAAAGCGTGGTTGACTGAGGTCAAGGGTCACCCATCTGTTATCGCTGACAGGCTCGAAAGGTTTAAGAATTATAAGTATCCCAACTGTTCGTGATAACTTATGAGAGGTCCATCCGATTATACACATCTGTAAAAATATCCGACGGCTCCAAAATATAGGAATGTTTTATCCCGCGGTACACGCCGGATTAACATCGT',\n",
       " 'ATGTACGTCTCGCTCTTGCTTTATAGGAGCTCAGTTAACATGTCAAATCTTATAATTTGTGTCTTCTTGATGCCTTGAACTAATGCCCAAAGTACGACACGGCTACTTCCGGGTGAAATTACAGTTTGGTGTTGGATAAGAGACCTAATTTTGGCAAAGAATTAAAAATAAATCTTATATTAGCGAATGGCGTCAGCCAC',\n",
       " 'AGACACCTGGATGTCGTTGATAGGGTCTGCAACTAAGTAGGTCTGTTTAAACTTGAATTATTGCATGACCTTTGACCCCCACATGAGAGAGCTAGTGTCGGTTCAATGGGCTCAATTAGCAAACTATGTTCAATGGTTACGAACGCTCATGAAAAAGTTGATTCCAAGTACATATAGGTCAAAGGTCAAAAGGAGTATAT',\n",
       " 'ATAATGCAAGCCTATGTCATCACCGTCCCTCACGTCAGTGGCCTCGCACTCAATAACGAACTTAGTAGGGGATTTAAACAAAGCACACGACTGATAAGATTGAGGAAGTTATAAAGCTGCCAAACACGCGTCACTTATCTTCTATTTTTATCAGATTCTGCCCAATATCTTCAATGCGCCACTGTATCACCATAATTATA',\n",
       " 'TTGGAGTTAGACAGTGAACCACTTCTTATACACTTGTTATCTGCTAGCCATGTAGTAACTAGCGGTAATAGTAAGGACGGATGAAATATCACTCTAAGACTGACTCCACCAAGGAATCTGCAGAGGGTCTTTAGCCTTCTATGTATTTAACACTCTGAGCGATCCGTCACCCAAGAGTAAGGTCTTGAATGACCAGTAAT',\n",
       " 'TAACAATGTAAACAAGCCTCTTGACTTCAGTACACCGATCCTACCAGTTATCAGCGCTCTTACCTTGCTATCACCAACTTAAGAAATAACGCATTCTAAACGTTGAATAGCCAAACTCGTCCCAGATAAGGGTCGTCACATAGTATACATGTCCTTATCGGAACAGGTGCGAGCCGGGGATTCTCTTGGCATGCCTTCAC',\n",
       " 'TACGCGGGCACGGACTCTACACTCGCGTACGATAACCAGAACGAAGTGTAACTCAATCTATTAGTCCTATCTAAGATAATATCGTAGCACGAAGTAGCTTTATATTAAATTTATCGTCTCCACTCTGAGACCCGGAAATTCGAAGGGGGCTCAGGGAATTAGATACAAATAGTTAGCTCACTTCCGGGTGGGTCACGATT',\n",
       " 'TCCAAAGTATTGTGGACCGGCCTCTAATTAATTGAATCATGACCTTTGACCTTTAGGAAATTCCTCCACCGCCAAAGCAGGGGTCAAAGGTCAAACGGAAAACAGACAGATATCACAGGGCTTAATTAATTACCTGGTTTGCTATACAGGTTCTAAAATTTACTAGATAGGGTCAAAGGTCAAATTTTTAGATAATTGAC',\n",
       " 'AACAAGAGGGGTCAAAGGTCAAATGTATGTGGCCCTGCCAGTCATACCAATTTCATTTATACGATGAAGCCGGTATTGACACAATGCAGGGGTCAAAGGTCAACCGAAGACAAGTAAAAGCATTGACGTGAAATAAGAGGAGAGCTATAATAATGCACAGCATATAAACCTTGACCTTTGACCTCAGACGTTTATCCAAA',\n",
       " 'GCTTTTCTTATTTACTGGGCTACTGGGCCCTACTTTTGCTAGACATACGGATTACATTATCACGCATATTAAATGGCTAACATACTCAAAAAGGTTCAAATTTGCCGGTACGGATCGTAAATTCACCTTCACTGGGTTTTAGGATAATAGCTTCATAGTTGTATGTAAGATTCCCCTTTAAAACGATACATGCCTTTATT',\n",
       " 'ATGGATCCATTTGTAGCACTCGTTCTTTTTAATTATCTCGCAACCGAATCTTCGGCACATCTCTGCTACTTACTGTTCCAATAGCAGAAGCGTTCAGACAGAACTAGGCCGATCATTTGTTCTACAGCCAGGATTACGAGGTGTGAATATGCCAGGGATTCGAATGGAAGGCTTCGATTAAACCACATCCGGTTTATAAA',\n",
       " 'CATATAATGAAGCAGAATCAAATCGAGTTTGTCTATCACTCTACTATTCATTTGGGTATACCATACCGCGCGTAAAGAAGTAGGAAACGCTAACCAAGTGTTTCTAAGGTTAAATACCAGAACGAGATATCGGCCAGTTTTAAGGTAAGGTCTAGGCGTTTGCAGATAGGGCATTGAAATACATTTGATATATACTACCA',\n",
       " 'AGTTATCCAATTTCACTCACGACTTCCGGCTTAGTACGTTGCCGTATCTCAAAATATACCGGAAATAAGAAAATTGCATGCCGGAAGTACTTATCCATATAGTTGAATAGGAGGACAGCACATACTAAGGTGGTGACAAGTAAATTGTCTTAGATCTTATACTCCTAAAATGTATTTATATCTAAGCTGAATTAAATGTT',\n",
       " 'CGTCATGCTGAGCAGAAGCCTTTTGAGGAGTAATAAGCATAAGGTATTGGTCGAAAACATATATTCGATCCTTTTGACCCTAATACTGTAAGTCTCGACTGAGTTATATCAAGTAACATCTGCCTTGCGCTATCAAATTATCAAGTAGAATGTGTGAACTTTGACCCCGGAATTAAAGCGGATGACCTTTGACCTAATTC',\n",
       " 'AAAAATCTAGGGGGTAATTTAAATGAAGAGTCCGATTTGATATACGCAAAATTATAATTTATAATCGACATTGTGGCTACCTGGGCATATGAGAAGTTTGCTTAATGCCTCCCTCACGCTGACTTCTGTGATTTCAGCGCGCATAACGCGCCATATGGTAATTACAGATTTTTAAACACCCCCCGGAAGTCATGCACAGT',\n",
       " 'CGGTATTAGACTCAAACAGGCTCAACGCTTTCGGCTAGTTGCAAATATACTCTATGAAATGGGGTCAAAGGTCATCCGCCCGGGATTCGGATTACACGTAATACATACGCATCATTAATGATCGTAGCCTGAGTTCCTAAAAATGGATAAGCTAGGTCATACATCGTAACAACGAAGAATTGATATGCGATTCGGGCTCG',\n",
       " 'CCTTTCCAGCCTGATGAACTCAGCATTTGGTTGAACATAATCAAAGAACATAGAAAATCTGCCACAAACTACTGGGGTTTGTCTTTCAAATTTCCGTGGGGTCAAAGGTCATCTAATACTTAGGGTAGCAATATATGCTGCATACATGGCTGTTGGCGATTTTCTATACCTTTGTCTGAGGTCAAAGGTCACACGGATCA',\n",
       " 'AGAGGACTTCCGGGGTCGCATGTGCAGCCTGGCAAGTTGATTGACTCAAGATATACTACAAGTGACGCCAATGGATTATTCTTTCTTAACTTTTATTTACTTCCGGGATCACACTAGTACCGCAGTGAGAAGGATAAATTCGGGGCGTTGACTTCCGGGTTAGGGAAATACTTTATCTTTCTATTCTGGTTAGTCTGCTG',\n",
       " 'TTAGTAATGGATGCTTTATTACTTGTATTTCAGTGGTTACCTACAAAATGTCCAGATAGTGACGTTCGTTCTTCTCGAAGCTCGGTCGTTTGGAAACAATTAATCATGGCGTCTACATGCTTAATTCAAAATTCAGTAGTATCTGTAACGATTCGGGGTAGACTTCAGGACTCACATATTATTAGACAAAGTCCCGTTCA',\n",
       " 'GCCATCGACCAATTTTATCAGACTTAGGATTCTGGATCCTCGGGATTTAGATCATTCGCATCTGATTAGATCGTTATCATGCTGAAAGAGTCCCGGATCGTGATTCTATACGAAAAGAACCCTCGCATGACTTCCGGATATGATAACAGGATTAGAATTTATATGAGTGGATCTATCCAATTGATTGATTTCCTGGTAGT',\n",
       " 'TGCCAAGCGACCGTTAACACATAAACTGTTATAGTAAACAAAGATCGCTCTTTGGCAACCAAACTCGACTTCCGGGTTACTTCATTTTAAGTGAATGCCACTTGCTGATAAACTTAAGCTCGTGTAGCTTGACAACTATTTGTTATAGTAGTATATATATAATCATTTAGTAATATAAGGGAGTTACGCTGACTCAATGT',\n",
       " 'TGAGCCGATAGGATGAGGTTAGGGAGTGATAGCCGGTGGGAGATAAAAGTGTGCCTACGTTGCAAAATATTTCGACAGTATCGTGAATTGGTGACTCTGAGTCGGATAGAAACCGGATGATAACCAGAATATACCCTTTATACGGTTAGAGAACTATCTATTAATCTGAGGTTCTCTCGTGTCATTTGTAACGATAATGC',\n",
       " 'AGTGCGTACCAGCGTGTTCCTCTATGACCTTTGACCCTTTGATGAAAGATGCTGATGATTACGTTTTCGAAAAAATGAGAAAGATGGCACCTTCATGAGTACAATCTCTTAGCCATTCACAAAAGTACCTATCATGAAAGGGTTGTTGTCAACGAACAACTAACTATCTTATCTTGTTAGCGAAAGAAGCTACCGACCTC',\n",
       " 'AAGGGTATTGCACGTAATCCCGCTTCTTTTTGGGTGAATTTGCAGTTCTACTCGTGTTTGACCTTTGACCCCTCCTCAGACAGGCCGACTTCTTGTACTCGGAACTCGATGTTACTTCGCAAAACTAGCTCAGATAGAGGGGTAATATGACCTCAAAACAAATTTTGCTCTAGTAACAATAATGTATAAACCTGTTTATT',\n",
       " 'AACGACGATCATCTATGTTCTCATTAATGCACCGGTTCCATTGCGTTGTTCATTAGCCTAACGTAAATGCTGGACCTTACTCAGCGTCAAACGCGGAAGTCAATAGAGCAAAGAATATACCCTTAATCTCGCCCGCTAAACCCGGAAGTATTCAGATCAACATTCAGTATAATGTGCAAATACTGCTTAAGCAGAGGTTT',\n",
       " 'AATTTTTTCCACATTAGAATATTCTATAAAACTGCTGGCGCTTAATGACTTTCCTAGAGATAATGCATTGCTGACACTCTCCTAGATCGAACTATCAAGCTTGGTAGATCACTCTATTGAACACTTATCAGATTTGTAAGACAGTCATTCATTGTACTCCCCCCTGACATGTTTATCTCCAGATACTTGATAAAAAGCTT',\n",
       " 'CTCAAACTGGAGTGATTCACGTCTATCGCCCAGAGGTCAAAGGTCACCTAATTCATAGTTAATACGTTTACATCGGTGACCTTTGACCCGTAAACACAGGTGGTAAGTAGTGATTATAAGATCAAGAAATCTTTAAACCAAACATGTATCTGGAGGTTGAGCTCTGAAACAACGAGGGATGTGGACATGAATGTCTTTTA',\n",
       " 'GCTCTTAGTGTCTTTTCGAAATCTTAGACTGATAGATACGCGAGCAATCTGGCCGACTGTCAATTCAGCGACCTGTATAAATTTTCGAATCACGCGCATCTTATCTATCTCTAATTGCGAGGCATTATCTGACATACGGCATAGTTTCTGTGGTTTTTAAGATAAGGCCTCGGTTGATCCGCATGAGTAAGCCCCCGGTT',\n",
       " 'GGAAGTGAAAACTTCAGAACGCAGATATTTCAAGATTAAAAGAAGTTAGCAGTTCCATAACGTTCTCGTAAATAGCATTCTCGAATGGGTCAAAGGTGATGTTCAGGTGAAGTAACTCGCAATAACACGCTTCATCGCACTGGGTACGGTTCCCGCGAATCAGAGCTGCCTATAGATGCGGGAAAACAATCCACCGAGAT',\n",
       " 'TGACTGGAGTAACATGACGATTTTCCTACGTGGGGTCAAAGGTCACATTGTTATTTGACCTTTGACCCTTAATGAGTGCCGCAAAGCCAAGGTTCGCGCATCTATTAAGGGTCAAAGGTCAAATGGGGCGCTAGTTACGCATTCTAGTCCCGGGGATGCGCGCTAACGACAAATGGGGACCCGAGACACGTATATATAAA',\n",
       " 'TTATCGCACGCCTAGAGAGTGTAAGAAATCTTATCTGTGCCACCGACGACTCTGACGAATTTCAGTTATACAACGACTGGCAACTGTGTGCATGTCTCCAAGATTCGACTTCCGGGTTTGTTATGCCGTGATAGTCAGAAACCTTAAAAATTGAGTTCGCTCACTAGACTCACTTCCGGTTTCCATGTGTCTTTTATCGG',\n",
       " 'ATGCTTAACCCGGAAGTGATCACACACAATAACCTCAGAAACCGGAAGTGAAAACATCTTAATATACTAATAGTATGAGTAATATGAGCCGAGAAATGCCGACGTTACCAAGTTTGCGTACTATTAATGGCATAAATACTTCACATAGGTGCAGCAGCACATCATCCTAGGTATAACTAAATATACATCTAAGTGTACAT',\n",
       " 'GTAAAGTAGAGGATAGCACTATCCCGTAGGGAGACAGTCTAATTTCTGACTAGCTGAGCGCCCTATCACGTGTCCTGTTAGCAGATCTTGGTTCCATATTTCAGCCTTGAGTTAACTTGACCAAGATTTGTATCCCGGATGATTTGTTCGACACGCGTCTGGTGTGCTCACGCTCTCTCCGTGTTGATCTCGATTGATTA',\n",
       " 'TGTCCCAGTAACAAAGCGTGTTCCGATGCTACTTACTTAATAAGGATTCGGAAGGGACTCGTGATCTCGCGTATGCTCAACGTACTCTCTAAATAATAAATTACCCGGAAGTGCAGTGAGTGTTTGTACTATCAGTTGAACTTAAATTATTACCCGGAAGTGTTACTACTTCCGGGATAAAAAGTTTTATTCCTGTCCAT',\n",
       " 'TAAGACTACCAATGTGCACTACTCTGATATCAGGTTGCAAACGGTAAGTGAAGTATTCGTTGACGCACATACAATACTTCATACACAATAATTATCTATTCCGTAATAGTAAATACTCTTATATGACTTCGGCTCTGATAGTGCCGATCACAATCGATTATAGATTCCTACGCCTATCAATCTCGTAGTACGAAATTAAG',\n",
       " 'TGACCTATATAAGTGATAAACACGTACCGTCAACAATGTAGACGGAAGACCCAGCAGTCACTAATGCCTAGAATACTCTCTCGTGTTTGGACGAAGCGTACGTAGAATTGCATGGAGTGTTGTACGCCGTATGCCGATGCCACGGCCCTACTATCTTCAAGAGTGAGCCTGGCACTGAATGGATACGCCTCTCAAGTTTA',\n",
       " 'GGTATATAAGGTGGGTGATGGTCTTAACTAAGTCCGATGAAACTTCTACAAGCAATGGAGGTAAGGGTACGCAAGGGATCGCGCGCATAAGTCTACAGGTCAAAGGTCAATCTTCTATGTATTCTACTCATCCACTATACGCTGGATTGAGGTTGACCTTTGACCCCCACCATACATTTAACGACTGTATTTTCTTATCT',\n",
       " 'ATGGATTATTCGGTTTCTGTTGTTGACCTTTGACCCTACTAGATCACTTTCCACGTCACGCGACGCAAAAAATTGCATCGGTATTTATCGACAGTTCATTGACACATGAGTTGTGCGGGTATTAAGGGGTACCCTTATGAGTATGAAAGAACGGGATCATTATATGTCAGAGCTTTTTTGATAAAACGAATCATACGAAC',\n",
       " 'AAGTGACTCATCGTTCTTCGTCACAACTCTTCAAAGCCTTAGTTACAGTAGCTACCAAGATGACAGATCTCATAGTAAGACATAAGATTTAAGTATTCGGCCAACTATACCAAAAATCAATGGTGGGATTGAACGCTCCCGCTCTTTGCCCAGAGGTCAAAGGTCACCGGCAATTATATCCCCCCGATAAAAATAATTAT',\n",
       " 'ACCGCCTCGTCATGTCTCTACTTTATAATTACATGCCTGACCTTTGACCCTTAACGGGTTTGCGCAAAACCACTAGATTTCGTTTAGGGGCGGAAACGTTCAGGCGTTGAGTCATCCAGTGGAGATCTATGTTCGTTGTAAGCATTATCAAGATCAGGTGCTGAATCCTTACACCGTCTTTGAATATCCCGGCTTACCAA',\n",
       " 'TACAAGGGTGCAGTAATGTTTACGAACAGCCCCAGCAGAGGTCAAGGGTCACGGTAGGGAGCTGTTAAGTTGGCATAGACAGCAGATCCTCTGACTTACCATGACATGCTCACCGGCCGCAGTAACGCTCGGGCATTTAAGTATCTAAAGGAGTTTTCTTATGATCGATGAGTCATGTATTTACTGAGTTCTTATCATAT',\n",
       " 'CGAATATGGCCGACTTTGATTCAACTGCTCTTGCATTAAGCAATAGCTCCTATCCGACTATTATGAGTCCTGATAAGGCAGATGAAATTCGGGTTAGGTGGGAATCAGCAACTTCTTACACTGAAGTATACAATATAACGAGGACCAAATTAATTGGACCTAGGAACGCCCCTATCAGTAGTATGCTCTATCAGTTACAT',\n",
       " 'TAATTCATTGAGCTCAGATATATAGATTCCTTGGTAAATGGTCTTCAAGATTATGATTTTCTGGTATTGCACTTCCGGGTTATCATGTTCGCTTTATCTTCGAGGAATAGCTTCCTTTATTTGGTCTCTGATTATTAAAGCATTAACTACATGATTATCAAATACATTGATGGATGTAACGAGAGAAGTAGAGGATTGGA',\n",
       " 'TTGCAGCTTTAATAAAAAATGTTGTTATCGTTAGCAGTCGGTAAAAACATTATGGAAATTAGATAAGTGTATCCACGTAAAGGTTCAGCAGTAGTGCATTGCTGCTATTCGTGAAATATGATATTCACCACGCTTTCGTTAGTTGACCCGAAGTTAAGGAACATGACTTTGGCTTACCCCACGGATTAATATTAAATAAG',\n",
       " 'TACATATCTAGCAGTATTGCGCTTATTTCGCATCTCTGGCAATTTAAGATATGCACGCATTGATACTACATGCCAGAAGAAATCAATATTTGTAACTAAAAGGAAACTATATGTCGGATCACGAATAAAGAGTTGAGGTTCAAAATCATTGATCCCAATTGGAATTAGAAACCGTGCTAGTCAACCTCATGTCTAGTCCA',\n",
       " 'GAGAAAGTATCAGTGCGTCGAACCTTTGCGCACTCATACGGTCTCTCCCGTACTCCAGAATGGAGTGTTACAACTGGGATATTCACCACAACCTTTCTCGCAGCCATGACCCCTACTAGGTGGATCGGGGGTTTAGGCGAATCGGGTTCAAAGGTCAAGTGTTGACCCGTCGTTATGGTGCCTTCATCCAACGTGTCTTT',\n",
       " 'CATGAAGAGTAGAGTTGGCAAAGAAAATATATATCTGCTTCAAGTAACACCCCTTATCTTCTTTATTATATTCGTAATGGCCGTAGAAAATCGCCCTATCCGCCAGGCCGCGAGATAGGTCAACCGTTGGTTAATAGTTTAGTTTCAACTATCAGATATGTCCAGCATACAAGAGATAATTCACTTGCGGGACCGGCTGA',\n",
       " 'GTGTTTATGCTATTGGAATGATTACATCCTAGTAATCGAGTGCCGATAATGAACTAACAATAACTCAATCAAATTAAGTCATGGGTCAAAGGTCATCCGGATCGGGTATTTCAAGTCTCATAATAGCCTTTAGATCCCGGGCGGGGTCAAAGGTCACGGAAATTTCGCTTGACCTTTGACCCTGATTATGCGCTGAACAT',\n",
       " 'TCCAAGTTGAGTGACCTTTGACCTCACGGTCTGCGTACGTAGTGACCCAACGGAATGTATTGACGAGAGAGAACCATATAGTAGTGTGTAACAACTTGAGCCTTTTTATTTTGTCGCATGACCTTTGACCCCGATGCAAATTCTTACTAACATTATATCAACAATGCAAGATAACCAATTAGTGAACCGGCCCGTGACTT',\n",
       " 'CAATGCTAAATGCAACAGAAAGCCGATAGTTGTGGCTTTAGGTTTCACGATACTTCAACTTAAGATAAGCGGAAACTAGCATCGACGGTTGATAGGAAATCTAAATTGTCGTCCAAACTCCTTATCTTTCTACTTTAAGAAAAGTTTATCATGCTTTTATTTCAAATTTCGCCACATGCGTAAGTATCAATGATTTCACA',\n",
       " 'ACAGTCATTTAGATATAAATAATAGAAGGGAGTGACATGGCTTAACATTCATATGCTCATTACCGAAAAATAAGAGTTATATTTGGTCGAACGTACCTCCTTGTTGACAAGCCGGGAGTAGATAATTCTTAATTAGTTGAGTTGATAGGAAACTGGTATTAGCATAGTAGTTCGAGTGCTCGGAGATAGGAATTTCGCCA',\n",
       " 'CTTCTCTTATTAGCATTATCCGCCTATCAGAAAGGAGAGTTGACGCCTTTTCCAATCAAATCGACTATGTTGTACAGACTCTCGACTGAGTTTGAACCCGTCACCGCTTGTTGGTTATAGGCCCTCTTCAAATTTCTCTGGAGTCGTGACAACATGATAGGAGATAGTGGTTCTTGGACCCCGCCCTAGGGTCCCCCTGC',\n",
       " 'TTGCATATCCCATTTAGCGAATGTAGTTTGTACCCTTGATATCTTTAATCCATAGTCGAGGGGTCAAAGGTCATCAGTTCGTAAGATCGCAAACACCGTCCTCATCCACTACAAGATGAAGAAGAGTTACGTGTCGGATTTACGATTAATTCTTCACATTCGTGTGTTATGTTTATCAATAGGTTAAATACAAGTAGATA',\n",
       " 'AGACTGAACATAGACTCGACCGCTATCTGAAGCAGATGCCGATTTGTAACTGGAGCATGGGTTTCATCAGGTAGGCGATCGGGTTCTTGGTCCACGATCTGATAATAGGTGAAATATAGGCATTTTCGTCCTGCATGGTTTGCTTAAGTACGTTTTGTCATAAACTATTTATGAAGCGCATACTCTTTCGAGTTCCGTAA',\n",
       " 'GCCGTTGTTCCCAACCTCGTTTTACCACACCGCACATAGAGGTTATCTCTTTGAACGTAGATACAAGATAAAAAGTATCTTAGTCTGTAACTAGGTGTAGTACCGGTCTAATTCGATCATCTAGTAAACGGTTTGTTAAGAACGGCCGATACTCCGTTCCATTTTTATTAAAGTGGCTTGAGAAAGACTACAATTTCCTC',\n",
       " 'TTCTATTATTGATAATATATTACGGAAAAAGGGTCAAAGGTCACCGATATTAGTTAATCTGTCGGTAAACGCCTTGTGAAGAGACATATTGCCGTAATCTTACACCGGACGGAGATTTTATTACTTGGTATTCTTAATAATACGTAAACGAGGGGTTAGAAATTTACAATTCATCATTAGACTGGATAATGAAAGGATAA',\n",
       " 'TTATATCTCAAGCAGATAGGACGGGTATGTGTTTTAAGGGTTTAACTACTAGGATGATTGAACAAATTGATCAATTTATTCTACACCATAAATTAGGGGCTGTCACTCTACCAATGTATATCCTTGTGGCTAAATGGAAATACACGATTTCCTTATCAGTCGACAGCGTGACAGCACCTTGTTGTTCAATCCAAGCTTTC',\n",
       " 'TAGCTTTAGGAAAACCGCTGTTGTTTGTACCGTTTAATAAGATTTAGTTAGTATACTTAAATCAGGGTTATCCACTTTTATTTCGAGAAAGGAGTACCCATAACTATGTGTTAGGTACGTAAGCGAAGGTATAATCAAAATTATCCCGAGTGAATAGGTCCTATCCGCAGCTGTTGATGAGTTAAGAACTGATGGCGATC',\n",
       " 'GGCCTTCTACTAAGTATCTTCTCTGAAAACTTGGCCAAGGTGATGTCATGATCAGCATGAAAGATCTGCTCAACATTGCCTAGGGGGTCAAAGGTCACCCGAACAGGTGACCTTTGACCTTCGATATCAAATTATCAAGCTATCGGTTAAACAGAAATAGTTGTTCATTATGTGACCTTTGACCCCAGGGATCCACCGAC',\n",
       " 'ATCCTCGTATCTGGTCTCACGAACAAAGTACGTTGTAAAGAACGCTATTTTTATCTTCTACGTCTATCGAACCCCGGAAGTGAACAGCTTTACTATAAACTGTTTGTTACAGGGACGATACGCATCATCGCCTAGGTCGACTTCTTAGTCACTCTAAGTGTCAGTGGTGAGTGCTAACGTAACATGTAAACCTACTAAAG',\n",
       " 'GTAGAGACGTCAAAAATACGTATGGGGAATCCCATGTTGGACTACGCTCTGATAGACCTTGTTGGCTCGAGAGCAACATTTATGCAGACCGCGTGCTAGTGACGACTATTATCAGATAAGTGCGGGGCGCATAGTTGGACGGGTCCTATCGGCCTTGAAGGATGTTAACCTATAACTTCTTTTTCTTCAGCCGAAACTTT',\n",
       " 'ACGTTCGTCTTTAAGTTACAATGCCATCCTTTAACGGTTCTTCTAGATTGTATTTTAATAGGTTGTGATAAATGGGTACTTCCAGATAAGGGGTAACGTCCTGATGGGTTGGTTCGTATATGTGACATCCTTGAGACAAGGACGATAAGGGAGAGATCGCAGATGCACTCTGACCTCGATAAGAAAGTTGGTCATACGGG',\n",
       " 'CAGAACGATTAAAGGGGTCAAAGGTCAACATAAATCACGTATAGAGCTTACAACGTCTAGGAAATTAGCTCTTGATGTTATATGAGTTTTAGAGAGCTTCAGCTCTTCCCACTACGACTGATGACCTTTGACCTCCGAGTTGAAGCAGTACGGTCTCCAAAACGTCTTCTTCAGAGAAATTACGGAGCAGATATATCTAC',\n",
       " 'TAAATATGTTTGTATTTGATTTTTGTTTCTTGCCTTCGCTCTAAACGAAAATTCTCCTCATTTGGAGTACTTCCGGGTTTTGCGCCTAGGTCTACTTGCTCTAGTTGTTGTAGGCACTTCCGGGGCTTCGTCCCATTGACGACGCACTTCCGGGGTTTCAGTCGTCGAGGCAAGTTAGACACATTGCCGATCTACGTCTT',\n",
       " 'GAAACGCTGAAGGTTTACGACATCTTAACCACGCTATATCGTGGGGCCAATTTAGTCGACTAATAAATTTGTCGAGTCATCTTATTATGAAAATCATGTGACCTTTCACCTCCGATAAACTCTCTGAAGAAGAGTATGGCTCTCTGGTACTCGTTTCCTAATCACTCTCCCAAAGAGAATCTGGATATAGTCAAGGAGGA',\n",
       " 'CTGTCGTCGATTACGGCCTATACCGCTCGAAATTGAATACAAACCCATTAAACGGGATAATATCCCAAGCTTCTTGGGTGGTCGAAGTAATGAACCTTGCAGGAAGGTCAAAGGTCACGTGAGACTTGTTATTCAAAAACTATGGTATGGCTTCTTATTGACCTCTAATGGTTGTGGACCAGGGGTCAAAGGTCAAATAT',\n",
       " 'AATACGGCATATGCTATCTGCGAATAATTTCCAACCCGGAAGTTCGAGCTTCGGTACCCGGAAATGTTGCGCAACATCTTTTCTTGAGCTTTATAGTTAAAGTTAGAGCATAATCTAGACCAAGCAATATTGCTCTCCCACACGTTTGCAACTTCTAAAAAACGGTTCATCTGGTTGATCTGAACGATTTCCACAGCCTT',\n",
       " 'ATATATTGAAAATAGATTACATATCTGGGTTCGAACAATGTAGTTTAGTTTGACCTTTGACCTCTATTTTTCTATTATACTTTATTGAGGGTCGAATCTGTTGTTGCCACATAATCTAGAGAGGTCAAAGGTCAGGATAGAGTCCTGTTATGTGTAATTGTTGGATGACCTTTGACCCATGCACGAGTGAGTAGCCGTTT',\n",
       " 'CTATAGTATTATTTCATCAATCTTCGACCTTTGACCTCCATGGACAGTAATAAACGTTCCGACTACTCGAGCAAAGAATACCCCCAGTGAAGTCTTGGGATGCATAACGCTTTCTAAAAATCGTGCTAATCTTTAATTTTGCAACGCGGTGCTCTTACTTTTCTATGTTAATAAAGAAAATAACTATTTATATCTTTTGC',\n",
       " 'TATAATTACAAACAAAATACTAGTCCGTTAACTGTAGTATATCTAAGCCTTATCCTAATTTAACATATCCTTTGACCTTTGACCCCTGATAAGTATTGTCAAGACTAGACCTCTACAACTGAGTGCGTGCTTTTTCAATACGGAAGCTGCACGCCTTACGGCATGGAGCTACAGGTACCTCTCAATAGTTGCAGGATGAG',\n",
       " 'GCACGCGTAAGCCACGAGGCATTAGCCCTTATCTGCAGCTGTCTAACATGATTCTTATTCACTTCTTTCCTTGGTATCGGTAGCTCAATATCTGAAAACCTTATCCACCTCGCTGGGTTCCAATAAAGTGTACCATCTAGAGTACTAACATTTTACTAAATCGAGCCTAATGATTTCATTGTTACTTCAGGGAACGACTT',\n",
       " 'ATTAATGCTTCAAGCAAAAATCCGGGATATAACAACGAGATCGCTCTTGAACACCGCTTTCTCCCAGTCTGATAGATGCTTTGTAGTATATTAATTTCCCCCTGTGTTCAGGAGTGGAGGAAAGGTCCGCATTGTATGCCACTTGAGGCCATAGTTTTGGCCCCCTGCGCTTCAGATAAGGATGCCATGGCGACGTTCGA',\n",
       " 'TACTGTTAAGCACCCATCTCCGCGATGAAATCTTAGTACACTGTTATTCGTATATGCTGTGTTACCTTAGAGTGCATTCCATAACTCATGGTAAAAACAAAATATGCAACCCGGAAGTCTGCGAACTTTAATACTGATTGAAATGCATCCCTCTAGCAAAGCAGACATCTTGGACAGCCCATTTTGAATGGTCTCTTAGT',\n",
       " 'CTCTTGCGCAGCGCCACTCGGACCTGAATACCCGTAATATCAAACTTCCTAACGCGGAAGTCGGCTTGATATAGAACGGCTTGATTAATGGCTTCAGGTCACACAAAAGTTATTCCCAGTAAAAGTCAAGAATTCTGCACGATCCTTCATTCACCGATCCTCTGTTAAGCAGCCAAGAAGATTAGCCGGAAGTCGACATA',\n",
       " 'GCGGCCAAAAAGATGGAAACATACCGAAGAGTGTTGATAACGCCCTTGCTAGCACTTTACTACGATCAATTCAGACTGAAGCAACCGACTCTTGTCCATGTTCACAGCAATCGCGCAACAACATTCCTATACTATATATATTCTAGTAATAACAAAGAAACCCAACGAACCACAAAGAGTTTCAGAGTGTTGATATTACA',\n",
       " 'TGTCCGATTTGCCGACCCCGTGACCTTTGACCCCATGTGCAATAAGACCAGGTGTCCATAAATATTTTATAGACATCAAACTTCTATTTTCTAGCTAAAAATGGAACATTTTTGAGGGTCGATAGATAACTGAAAAACTCATAACTTCTATCTACCAAGGGTCAAAGGTCAACACACCTTAATTGAACAGCGTTAAAAAA',\n",
       " 'AGTGGACGCGTATGACGGTAAAAAACGATATTTTTACATTTAATTGACGTAACGGGGATCCACTACACACTACTGGAGCAAGATATCTAACTGTAATTCAGGCAGATAAGGCAAAGCGCTATCAGCCTCAGAAAAGTTGTAGGGCCCAAAGACCCGGAGATAATCCGACGCACGCTCGTACGGAAAACTGTGGCTTGGCT',\n",
       " 'GGGACAAAACTTATAAATGACGGTTATACTCCCTTAATACAGGTGGACACGCATAACAACATGGTACACAATATGCATCTGAATCTATAGATGTGATATGTTTAAGTGTATGCCGGTCAAATCAGTAATGGGAAACTAATTGCGCTTTATCTTTTACAGAGATAAACAGATTAGTTAGCTAATACAAGAAGTCCGAATGT',\n",
       " 'TAGTCTGAAAAAAAAATTAGGCTTGTCCCATTAAAAGTAATTTCAGTTACTTGTGTCACGATAAGCTCACCTGGCCTAATACCTAGCGTTAAACTGGGGCTATCACGTAATGGTTACTAGCAGGAGTTTCTTTAAATTAGCGGTTGCCGTATCAGTCTGTTGGTTTCAGTGTGAGAACAACATTGAAATCAAGGAATTGC',\n",
       " 'TCATGCAAAGGTCTGTTTCGTCATATGGGCGCCTAATACTGTCGTGTGATACGGTTTTCCACCTTAGAGGTATCGGTTACTACATCCCGGAAGTCTTTTTACCCGATTCACATTCCGCAAGAACTGAACCCACTAAGTTAATGAACCCGGAAGTTAGTTTCCATTGTATTCCACTAATTTAGCCGTCACAAGATAACTGG',\n",
       " 'GGATCAAGCTCGATTTACATCTGAAGTGAGACATCTTATCTGTTTTATAGCTCGACATTGGTGTAATCATAACAGCCCAGTGTATGGAATGTTGACATATATGATAACGGTATAATCCTCGATTAGTCCTCTGTCACGGCCGAAATAGAATTTGCGTACAGGTCCTGGATCCAGTTCCAAATTGTTGTTAGTTTGCAAAA',\n",
       " 'TTACAGGGAGTCTAAACTCGCTTCCGGGCTAGGTACCCGGAAGTGCCAGTACCGATACCTCGTGCTAGAAACCGTCCTCACGCCACAGCCATAGACATAGAGCCGTGTTATGGACCTCACCTTCCTAATTCTCCCCGTTCTGGTGATGAGAAACCATGGTGCATACTTAAGCCTCTTGTCATCACTTCCGGGTTAATTAG',\n",
       " 'CTTATCGTAGTTTTGAGCGATTTTATCCTAACGTCCAAATTTGTCCATTCGTATCATTATCTAGTGATACTGAATTATCGTTGTGGATGACGTGAGAAATAGTCATACACAATCATTTCCGGGTACTTTATACTAAAATGTGCTAAAAGCCATGCTAATAACTTCCGGGTTGTAAATTACCCGGAAATGCCTACCCATTA',\n",
       " 'TTGTATATTGGTCATATCATGATGAGCTAATTAAGCAAAATCCACGCGTTTTATAGGCCAAGACGAGGCGCCTCGGCGTACGCTCCGAATAAGACGGAAGAGTAAAAGACTTCCGGGCTCAGATAAAAGTTGTCTTGGGACCCGGAAGTAACCTATTCAGCTAAAACACGTATAATGATACTCTGACACACCCCCCTTAC',\n",
       " 'GCAAGGGTCAAAGGTCATGTATATGCTATCTTAAACCATGCTCGAATTACTAGGTATCCCGGTTCGAACAGGGCATGAGGGTCAAGGGTCATCCTGCGGTTATGTCCAATCTCATATGACCTTTGACCTATTAGCGAGTTTTTTCTTGTCAAGTTTGTCGCAACTTGATGACGAGAAGCTTATGATATAAATCAGATTGT',\n",
       " 'CGAGTTATATGGTTAGAAACCATTTCAAAACCTATCAATCTTTTACGTCTGATTCAAAAGCTTCTGCTAATCAACTTCAGTATCACATAGACCTGTAAGGATTCCTATACCCTCGGGGGTCAAAGGTCAGGTAGACGTGTGAGGATGTTCAGTATACGTTAAAAGAGGGAACTTCTGTAGGGGTCAAAGGTCACATTCCT',\n",
       " 'CTTGGAGAAGTGATGAGCTTACAACGGAATTCGTAGTATGGTAGTGGCATAGAATTCGGCCTAGTATGACCTTTGACCTCTAGGCTCCGTCATTTGGCGACAACACTTGGAGTCGATTTAGATAACCTTCGTAGTTTTGGCTTGGAGGTCAAAGGTCACCCACACAAGCAAGCTCTCACTCCAGGGTCAAAGGTCACGGG',\n",
       " 'AGAATAAACCATTTATGGCACCGATGACCTTTGACCCTACATTTACTTTTCGATAAATAGTAAATATGGCGTAAGTACTTAATTCGACTAGTATGTGACTGTTGTATGACCTTTGACCCCTGGACAGCCAAGAATACGGGGGTCAAAGGTCACACGAGGTTTTTTAAACTCTCACGTAGTACGGCGGAAAGTCGCTAGAA',\n",
       " 'GCCGAAGACCAGAAGGTCCTAAACATCCGCGAGTATAGTACTTTGGAGTCTGGAATATACGAAGTCCGAGGGTGACCTTTGACCCCAGTATACTAGTAGGAGAGACTTTATCGTTATGTTTTCTTATCGATGTAATAGTAGCGCGGTGCTCCAACGATGGTATTTTAGTGTGGGGAGTCAGTGACCTTAGGGGTCCACGA',\n",
       " 'ACATACAACGATTTATCCTTTAACTGGTAGGGAGACAGTTCGCTTCAAAGTCCTGGTGCTGTAGTGAGCTGTACAGGGCCTTACCCCCAGATAAATCGTAGGGGTCAAAGGTCAAGTTCTGAGGGACCTTATGGTATACATGTCGTCATGAGGTGTTGTGGAATATTGTATAATTTCATTGAGAATACACTCTTCGACTT',\n",
       " 'ATTGCGAAGACATGTCTGTATAGAGAAGAGGAGCGAGATCAGGTCCGGACTTACTTCCGGTTTGCGTCGATTGAAAAACCTCATTCCGGCTTCCGGGAACTTTAGGCCCGCCCGGCATTAAATTTTGCCCGTAGCATCTACAAGAGTCGCACTAGACTTTGGAGCTGTGCCATGAAGAAATTTCACACAAAGGTGACTTT',\n",
       " 'TCTATGCGATTCCAGGTACGTGATAGACACTACAGTTGACCTTTGACCTTTAAGTCCGACCATATTATAGGTGAGGCCGTGCTAATGGCGCTCACGAGTCTCCCGTGTTCATACCACATATTTCAGGTAATATTTGTTTTTGAGCTATTATTTATCCTTCGCATGACCTTTGACCTCAGGAGAATATTTCATTGCATAGT',\n",
       " 'AGTCCTGAAAAGTTTTAGAGGGCGACAGTCTCATGTCTGTCCTAGTATCTTAGAGAGATGTTCGTCTAATCGTTATTATCCGAACAAGGGAGAGTATTTTACCCTGGAATAGAATGTGACCTTTGACCCCTTTTAATCACCCAGGATTCGGCTGAGACTGAACATTCTAACTCACAGAGGGGTCAAAGGTCACCGGCCCC',\n",
       " 'CATCATGCTAGGTCAAAGGTCACAAGGTGGGCATTGTTTGCTGAACTTTGACCTTACATAGAAGTCTAGTTAAGCCGGCTTGTTTGGTGTGAAGTGATTCGTCTCCCCATTTCCGCCATATTGATTGCTGTTTCATAATGAGCGATCATTATGACTATGCCATCGCTAGGGGGTCAAAGGTCAAGTGTAAGATCAGTTGA',\n",
       " 'GCGAATTTACCTCACTTCAAGGATTTAGGCACGGATTTGTACCTTCCCAACAGGTGACCTTTGACCCCAGCGAAACCTTGATCACGTTCCGTTACATAATTCCAGACATTGTTCCCTATAAAAAAATAGTGTCACGGTGTGCGCTAATGTTAAGTCTCCGCTCATTTATAAATAATGATTCAATTGTTATTAAGATATTT',\n",
       " 'ACTATGATGCAAAAGTGCCATGAAGATCATTGTCATGCAATGGCGGAGAACATCGGAATCTCAATGATCTTCCTACTGACTTTGATATATCTCCCTATCCCGCGTATATCGAGAAAGATACCCTTATGCATCCCGAATCGAATAGTCCTTATCATATACTCGGATAATTTACAGGAGGTTTACTCTAGTCCTCAGTTACA',\n",
       " 'AACAGAGTTGAGCAATAAACGATATATTCATTTCGTATAGACATCGAAGGTTACTATAGAGGGATTCGAGAATGATAGATAAAAATCTTGTAACACTACTTTATGTTAACCCATTATCAGACGTATCTCTCCAGCACGAATTAGAAATACACTATTTACACTGTGAACGGGAGCACAGCAATCCTGTTTTGTTCTAATGC',\n",
       " 'TTAGTTTAGATGCTTAGTATTTCTATTGCAAATAGAATCTGTGCGGAACACGGAAGTGCAGAGTAAGATATTAAGTCTTATGGTGCGTTAGTCTAGGCTTTTGTGATCCTTGCGTATAGGTCTCAGGGGGCGACACTGTCTTTATAATCATAAAGTGTTTATTAACCAATACTTTGATTACTTAAGCGGAACAAAATAAC',\n",
       " 'CAACCTATCCAATTCAATGGTATGGCAACTCTGTCTCGCGTGCACTCTGGAGCAATATATTAGTGACACCTCGGATAACGTGCTATCTGCTAAACTCCTATCGCCAGACGCGATATGGCGTGTAGCGTATGGCCCGTGCTTTATATGTCGCATCCCGAATAGCATTTTTCTTAGATATCGCATGGAAAGTATCATTTCAA',\n",
       " 'CCCAGATTATGGCCGTAGTGGGGTGCGTTTATAGGAAATATACTAGTTTCTATCAAAAGTTATTTACGACAGTATAACTTATGCTCGCAATAACAGTTAATGCGTAAGGAATCCCTATATCGTAAGATAACAATTCAAAATTGTACATTTAGTCTATCCACGACAAAACGTTATAGCAGAACAAATATCGTCGATAATCG',\n",
       " 'CGAAAATTAAAAATATACATTAAGGACGGGTCGCCCGTCATAAACGAGGGTCCTTCACTTTGAGGGCTAGTTGTCGAGTCCCAACAACTATCTATCCGACGTACCCGGAAGTTGTAGGCGGACCCAACGTTTACAGGCGATAGGTTATGAAGGATAAAAACTTTGCAAACGATCTATCAAAGCAAGAACGGTCGATTGCA',\n",
       " 'ATCAATGTTGAACTTCAGACTTACTTGATAGATAACTAAGCGTACGAGTATATATATGGGACCGAGACTCAGGTTGATAAAGATAATTGTCCTGGGTTTCGAACTCAGTAAATTTACTTATCGTCTATAGTCTGATAATGCCCCATTGAATTATCGGCTGCCTGGTTCGAAAGCACATAATAGACTGACAGGCCGCTATA',\n",
       " 'ATCAAAAGCATTGCAATGACAGTCAGCTTATGACACTCGCTCTTTGGTAACGAGTCTTCTGAGTCTGTGAGACTTCCGGGGTCCGGAACTGAAGTTTTAGGATATTGATAAATATTTCGATAGGTGGCTGTGATTAAGGATGAGCCACTTGATGGCTTGTCTAGCTTTGAAAATTACCCGGAAGTGACAAATCTACGAAA',\n",
       " 'TAACTCAAAACTTGGCTAATTAATTACTAATGTAGGGAATACTCGTTCCTTCATAGCTACACAGGAGTAGATATCCACGAAGCTATCGTCTCAATTACTAGTTCCGGAAGTATCTGTATATGAAGTGATTCGTGCATTTCCGGGATCCTTAGAAATGATGCTATAACTGCTCTTATGTACATTTTCCCACTTCCGGGTAT',\n",
       " 'TAAGTAATCTATAGACCCGGAAGTTGGTTAGTACTATGGTTCACGAGCCGTCGTCTACACCTCTATCAGGGTCGGGAACCCGGAAGTGTGAATACCGTTAGCGCAAGCCGGAAGTGTAATCGATAAGCCGATTCTATGATAATTCGATTCTGAAAAATGCTTTCTCATTTTTACAACCGGCGAAATCAGCGCATAGTTTA',\n",
       " 'ATGGTTAAGGACTGTGTGGGTGGATTAGGCAACACGGAGGGGTCAAAGGTCACCAGTCTCATCCATACGAATAAAACTCACATTGCATAAAGATTAGCTCGTTGACCTTTGACCCTATTTCATCATGAATATCGAAGGGATATGAGTCTGCTGAATACTATAAAGATTAGGCAATGATAACTTAAATTGACTGTTTGACC',\n",
       " 'GATGTTCGATTAATACCTTAGTGTACCCGTGTGCCAACCGGAGAACTGCCCCAATATTGGGCATCAACTTCCACCGCAAAGGCCCGCTACACCTAAACGGAAGCGATTGTTATTTCCTGGATTAGCTTGATTGAAGGCTATAGTTGTGATACCGTAACTTACCTGTAAACATCTCACATCCGGGTTAACTACCTGTCAAT',\n",
       " 'ATGATTTTCCAATAGAGTCCAGCCGCCAGCAAATAGAAGTCACGCCGAGTAGTAAAGCTCGAATCAATAAAAACGAAACTCAGTTAATTTTCGGATACAAATCAGATAAAGGTTTTAGTACCTGTGAACGTTCCAACTATTTATTGTGCATAAGACTGATAGGTGCAGTTCTATTCTTATTAATGTTACATGTTAGGAGG',\n",
       " 'TTTTCGTTCTGGCACGAGTTACGTGGTGCAGTGCCCCTGTTGCTCCGTTCATTATCTGCACGGGCAAAGACGCCATTATCAGACAGGCGGAAGACTGTTATCACGACAACGTAATTCATCACCTTGGTGTCAACTTTGGCATCTCTAACCAGGCGATTTTACCGAGTCTGCGTAGCCGTAATTTCCCCTAATTCCTCGAT',\n",
       " 'GGAATTCCTAATTCGGTCCTTATCAGTTAAATTTACGAGGCAAATAACGGTGGGTGTCCATTACACATAATGTAAGTAGTAGGTGGGAATGTGCACTCATGTCATCGTAGTAACGCTTAGAAAAAATGCTGATGGTTACTTTTTGCAGAAGGTAATTGTTTACTGTACGGCTAAATGATTCTGCCTTTTTGAAAAGACTG',\n",
       " 'ATGCAGCCTGTTAGCAACGGTTATGACTATTTGGGGGGTCAAAGGTCGAGGTGGCAATCAACATTAACGGGCCCTTATGTCAGAACGCCTCTAAGCATAGCAGTATCTTTCAAAAGAGTTTGATTATGATGCACTCAGCGTGTGACCCTTGACCTTAGATAAACTGCTATGCTGACTATATAGAGTACTATGTAGAACCT',\n",
       " 'TCTATTGTACACCAAGTTGAACAACCGTAGTAGATATACTCTCAGACGCGAATATACCCGTGGGCTCGGATTCTCTGTAATTTACCCTTCTTGGATTACTAACAGTCCCAGCGAGTGAAGTCACTTCCGGGTTCTGTAAGCAAGCCGTATCATTGCATCGTCTCAAGGCTATCGACCCACAGTGGACGTTAATAGAAGTA',\n",
       " 'CGTATAACACATAAACTCGACAATCCAGGATTCATACCGCAGATAATAGGAAGGGGAAGCCAGACCGATAGAATGTAACCTCTCGCGAAAATAGTCGTCCCATATTAGATTTTTATAACGTACAATGATAGCCATTCGTTTTTGCTCTATAGAATGTTTTATATTTTGTATTGGAAAATGAGCATAAATAATCCATGACG',\n",
       " 'TTCCGCTCGAATGGTCAGACACAAATCTTGGTATGATATTTTCAGTCCCGTATACTGCTCACTTAACGATTATATCAACGTAGCCCTTGCGAGAAATCTGCAATTTTTGACTATTCTGCTCCTAATGTTAAAATGTAACCCCCATTTTAGAGAGAGTGTTGCGGTTGGTGAACGCCTGAACTTGAACCCCGGAAGTGCTA',\n",
       " 'TAGACATATTCTTTGTCAGGACATGCTTCCGCATTTTAAGGGTCAAAGGTCCTCTAGATTGTTCCGGGAATATAATGGTTATCAGACGGTAACTTGGTTAGCAGTGGAGTCGGTACACGACAGAGTTAGAATCTACGGCATACGAGACCCGTGACCTTTGACCCAGGATGTCCCCAAATGAATAAACAATTCCTATGTTA',\n",
       " 'CTTTAGCACTAGCGGTTTAATAAAAACAAAAGTGTAAAGAATTGACCTTATTTTTTGATCTTGTAAGACGCTCCTAATCACTTTTCGTCCATTGCTCTGATTCTTATACAGGTCTGACTTCCGCTTTGACGAAAGTTGAAACATATAGATTGACTTCCGGGGTAACACTTCCGGGTTAATATTGCTAAGTGATAGACTGT',\n",
       " 'TGGCAAACATCTCTGTTTGCATCGGTGGCCACATCAGACTTCCGGGCTCTGTGCACTTATTCGATCTAACCCGGAAGTTACCAAACTGATCCTCTAGCGGAACTGGTGCAAAAATAGTTTTGTCGCCATTTTCAAAATCAAATAAGATCGACTTGGACTGCCGGGTGAAGCTAATGCGCAAGTGGATGGTATCGCTGTTT',\n",
       " 'GCTTATTACCGTTATCTGGTCGTCATCGTTAAATTTGACCTTTGAACCAGAAACGAACCGTAACTGAAGCCGCGTATCTCCATGCACCAAGGTACGCTGAGCTGTTAATTGAGGGGAATTCATTATCCGTAAATGTAAGTTTCAGGTGGAGTGGTCTGTTGATTAATAGTGCGGACATAGTGCGAGCTTGTCATTATAAT',\n",
       " 'TTTCACAAAGGTTCCGTTTAAATTATTCGAGAATTCACATTTGAAATACTCTAATATCTCTCTAGCTATTGTGGGAAGATCATCCATATCAGTCTTACACTGTACAAATTTATTCCTTCTTTTTTGTAACGGTAGGTCATACGTCCTAGAGCTAGGACTCCCAAAATTAACTTTCTGTGAGCCTATCCGGTATACCGCAA',\n",
       " 'AACAGTACCGGTACTGTAGATCGTTGACTCTATCCTCAATGTGTCCACTCTGCACTCGTTATACACTGAAAGTATATACCATAGTCGAGCCGTCAACGCTTTCTCCTAAAGCGTGGTTGCACCCATCCTGTTGATGCTGTTCTGTTATCGCTGACAGGCTCGAAAGGCACTTCCGGTTTAAGTATCCCAACTGTTCGTGA',\n",
       " 'ATATTACAATTGAAGGTCAAAGGTCAACCATAAAGTTTAGCTCTGTGCAAATACGTTAGGAGAGTACACATCTGACCTTTGACCCTTTTATGATCTTGTAGCGTAATCGCCACAATTAGTGAGTAATGTGACCGCATTGGTACCCATCACGCATTAGATTAGACTTATTTCGACGCAGCATCTTTATATTTCTGTGACTA',\n",
       " 'GTCGCCCGTTAACCGACTAAATAGAAGCGTAGTCGACTTAGATCAGAGGAAGAATAGTAGTTATTTCAATGCGCTAGCAGATAAATCATATTTTATTGAGATTAGTAATTGCGTGAGACTTTTAAGACTCAGAGCTAGCTGCGTATTTGATGATCTATGTATTCATAACCCTACAAGGAGGAATCTAATAGCCCCTGGCT',\n",
       " 'AAAATCGAATACATTAGCCATTGGTAGTTTAGCGAACGCCCTTCATGACCTTTGACCTTGGGTTGTACCGTGTATTGATGACTAGTATAAGACACTCAACTTGAAAGAAAACCTTGACCTTTGACCCCTAGACACTCGCGTACGATAACCAGAACGAAGTGTAACTCAATCTATTAGTCCTATCTAAGATAATATCGTAG',\n",
       " 'GAAACGTAGGTTGCCGTGCATGGTAAGTTTGGGGCATTTGGCTTCTAATTTCGACTTCCGGGTTGCAGCAGGGTTGAACGTAGTCTACATCCTACTCTGGAGGTGTTACGAATCTACCCGGAAATATTCGAATTCATCGTGCCGGACTTCCGGTGGCACTATCTTAGTTAGAGGAGTTAAAGGCGAGACTTTAACATTTC',\n",
       " 'TCGAACATGCTGGATTCTACCTGAGCCATCAACTGTAATGCACTATCCGTTATTACTCTATATCAAACTTGATTAAGCATGGATTAACGGGAGCGAACAGTAAACACAAATATATAAACAGGAAATAATTTTGGCCTACATAGTCTTATATTCGGGACCGAAGGAGCAGTTCACAGTTGACAAGTAATGTTTCTCTGCAC',\n",
       " 'TCTAAATGACTCCATCTAACGTTTCAATTGGCGGATCATGACCTTTGACCTTCGAATCCACCCCAGATTAAATAGCAACTTTTTCTTTTAACTGGCTGATGCGGATTCGCATCTTTAGTGTGACCTTTGACCCCCCATGTAGCGGAAATTAAAATGCAGAATTCTGGCTAAGGGGCTTCAGGGGTGAAAGGTCATCAGTC',\n",
       " 'ATAGCGGACCGCACACCCGCAGTTAGCTAAGAAGATTTCTTACGTCGGGAAGTTTAACTCAACGTATTTCAACAAATAGCTTTGAGTCAATTGACCTAACCCGGAAGTCATCAGTTAACGCAATAACACGACCGAGATGTGCCATCTTATCATTTACTAGAAGAAGTGAATTTATCGCAAATCGTCTGACGGACGTAATT',\n",
       " 'GCTACGGAAGATATGGGGGCTTGATCAGTTTTACTGCAGACCAACTACGGTTAGATTCGTTGGCTGCGGCCCTAGATTGACACGAGGGGACTTTCGAATAGGTATACAAGACTTATGGCCTATCTTCGTAGAGCCAACTCCGATGCTTAAGAAAACCCCGCCTATCATATATCTCCTCGCTTCAATAGGACTCAAGCTAT',\n",
       " 'GTTATTCACGCGACCCACTACCTCTGGAAACCACCCGGAAGTTGCAGGGTCGTCAGGGTGTCGTCGACGGGATGTAGACCACAACTTACGTTACTACGTCATGTACTCCAAATCAACATAGGAATCAGTAATATGATACTTCCGGGTTCGGGAACGTACGTTGGGGGCTGTTATATTCTTCCGGGTGTAGGACATCGGGA',\n",
       " 'AATCAAGATGAGTGACTGTTTCAAACGTCGTGGTAATATGTGACATGGGGTGAAAAGATGTTTAAAGCGTTGTATTTGCCAGTGCATATCTCCCGTGACCTTTGACCTCAGAATTATAGCTAATAAGGTTTCATCCGATTACAGTGCATACAGGAGTAGTTCGATAGCACAGAGTCCTCACAGTTTGCGAGCAAATGAGC',\n",
       " 'GGACGTATGATTGTAATAATGGTTCCTATGAAAGAGTTAATTGCCTAGGATATGAAAAGGATAGATCTTTCCTATCGTTTCTGCCATCGCTTTAAATAATAGAGAAATCGGCACGCGTCAATTTATTAGGTGATCATTGAGATAGTCATCTATCACAATGAACACAGAAGCGAAACCCGGAAGTAGTTGCACCGTACAAA',\n",
       " 'CAAAAGTACGCTATAACCAGCAGATTCACGGATCCGGAAGTGTTAATTAATAAATTGCAAATATACGAGGCGTTTAGAGCACATAAGTTGAGAAGGCTATAGGCTGTCCTCACGAACAATTTCGACCATTTACAGCTCCAGACGCATTAAATTATGGAGCTGGTCTAAGGGAATTCAATGAATTCCGGAAATAACCCCAA',\n",
       " 'CAATGCCTACTCTAGTTATTCTTTGAGGGGTCAAAGGTCACAGACTAGGATCAAACTGGATTGACCAATACATCAGCGGTGACGAACAATAGTTGTTGAAGATGACCTTTGACCTCTCATGGCATGATTTGAACGATATCGATGCTTGGAACAGGATAAGTATTACTGGATTCTTCCTTAATGTTAAAGATATTATGAAT',\n",
       " 'AAAAAGTCATTATCATGCTGAAAGCCTGTTCGGAACCCACGTACTAGATGGCGAGACAGAATCTTACAGTTGTCTGGTCCATTGGAGTAATTACATATTGAGCAAGATAAGTCCCGTTGATAATGCCAGATAAGACTTAATTGTATTCCTAACGCATCACAGCTATTAAATTACTCTGAAAAGCGTATTCGAGCTTTTTA',\n",
       " 'CCCGGATGTTAGCAAGAAGGCATCTTATAACACGTTATTGCTCATGTTTAACATCTTATATCAATATTTACTACACATCTAGGTCTGACAAGTTGTATCCGCGACAATGTACTTAAAAGTCGGATTGAATCCGATAGACGGCTTCCTGAACGTCCCTCAATAATGATAACCCTACTCATATCGAATTGGGTGCTAAGTAA',\n",
       " 'ACTGCAAAACAACCTTGCCAGGATGATACCTCCCCATTATGCCTTATTAAATGGAAAGAGACTGATCTTATATCTCTATCTGGAGATAGCTAAGATTTAGTGAGGTGACTCATACTTCCGCTCTTATAACGTTGTAAAAATACCAGGCAAAATAACTCAACTTGGCACAATCAATTTACGCTATATTGTCATCCCTTGTC',\n",
       " 'GCTGTGTAAAAACAACCATTTATCGTAGATACTTTTGGATTACTTATTGAGATAGCTTCGAGTTGTTAATAGTTACGCTTCAAGGGTTGGGACTCTGACCTGTTATATGTCCACGGAGCTTTACAAGGCATAGTAACGGTTTCCTACGGATGAAGAGTTATCATCTGGCTGTCTGTCCTATCACCTACGAACTCTCAGTA',\n",
       " 'CTTAGTGTTGTGAATTGCAAACGAGTGCATGAGAAACTATTAATGTTTGCACGATTGCTATCCCTTGATCTGACCAGATATAATAATACATTTCTCTCTAATGACTGCTAAAGCTAATAGAGCATTCAGAAGAATTCTAAGATCTAGTCCATATCTCCTGGAACTTAATTGTTACGATACGTTACGCACAGGACGGTATA',\n",
       " 'ATTATTGTAGCGGGTAAGTTGACCTTTGACCTCTATTCCGCCCGCGGCTTAATCAGCAAGCGATTCACCTGCACAGTTGACCTTTGACCCCTGCGAACTCTTGCGTGGTTAACACAAACTAGCCGTTATAGAACATGTGATAACGAACAGACTATAGAGTCCATCATAATGGCTAGCGTTCGAGGATGTTTAAAAACGGA',\n",
       " 'GCCGCTGATCAATGAAGTAGAGATAGCAAGACCCAGTCTTATGAGCTACTGTTACAGTGACGACGATAAGTTCTCACTTTCAATCGCTTGGCTTATTGAGGATCGCTGCACTGTGAACTGAATTTCGGGAACCAAAAGGGTATCAATGCCTAGAGAAACCTCTTTATTTTGAGTGCGTATTAATTGGCCAAAGCATGGCC',\n",
       " 'ATGGATATAATTTTGACATACACTAAGCTACAATTGGATGCGGTACCCACTACGTATCAATTAATACATTTCCGGGTAACTTTTGAAATAATTTACCCTGTTATGAATAGTAGCGCATTTCCGGGTATAGAATTATATATCCCGGCGCTCGCAGTATAAAGAAGCGGCATCGACATGAGGTATTTCGTAACTAGTGTTAA',\n",
       " 'TCATTCTATCAAAGGTCAAAGTTCATGCTGGGTCCTTGACCTTTGACCCCTAGCATTGTTGCCATAGGGTAAACGTGTATCTCCTAGATATAGGTAGAATGTTTTTCCGAGAGCTTGTTCACTAAAAACTTGGTACAGATACGTAACTACATGTTATAAAGGCACTTCGGACATTCATTTGTTTTTTGCAAGTGTCATAG',\n",
       " 'AGATATTCTGGTCTTCTTCAAGGCGGCGTTCGGATAAAAACAAGGCTATGTTTCCGTTAATGACTTGCGGTCTCGATTAATTGACATTCGTGCAATCACTAGCTACAGTCACCATCGCTATGATGACCTTTGACCTTTGGGCGATTAGGTAAATCAACTGTTTGAGGTCAAAGGTCATGGAAGTGCCCTCTTGCAACATA',\n",
       " 'GTAAAAAAGTAGGGTAACGTCCATCCAAAAAAGAAAATCTGTTGAAACGCTCGACTTTGTGGTAAATTTGACCTTTGACCTCAATTAGGTAGCTGAACCAGTGATTCCTACTAGGTATTGGTTACCGGAGGTCAAAGGTCAAATTTTAGTCATAACAGTGCTCACCCTTGTTTATGTTTGTCTGTTTATATGACCTATCC',\n",
       " 'TTAGAAGATCCCCGATACTTTTTTTAGGTTAATGGGGCGCTCTATAAATTTGTAAACGACCAAGATCGACCTGGCTCACAAAGCATGTTCTGGCTTCCGGAGTATTAACAAACGCGGAAGTGCAAATGAAAGGTAGTGAAGACTCTATGTTATGTTGAGATCTTTCAGTCTGGTATATTGAACCCATTACAGCGGCCAAT',\n",
       " 'TATTAGAAACTCACTCCGGTATTTTACTACTCTTAATGTCCAGACAATGAATTCGAGCATATTTCTTCCATCAGATAAAGCACCCTTCGTTATGAAATGTTGCTGCTTTATCTGTATTAAATAGTCAGTCTTATAAGTCTAATTGAATCCTACTCGACTACTCTACACAAGACCTATCAACCAGGAGTCACGCTATCCAT',\n",
       " 'GATAGGAACGACTATCTGCAGAAACCGTGACGAGTGCGTCTTTATTCAGGCCGCACTGTCATCCATAGAAATGAATATCGGGTGATGCGTAGATAACAAAAGTGAAGTCAATCAGTACCGTGCCCGAGCTTCTGTCGCTTGGGGAATGTTTCTTGCCCGTGATTCAGATAATCTTGAATTCTGTACTGTAATAACCTTCA',\n",
       " 'TCAGTTTAGCCCTCGATCTAGATCTGATGACGTGTTTGAATCTTGGGCCTAAAAAAAATTGAGAATCTGGCATATATGACAAAGATAGGCCGTCCTTATTCGTTCAAATCTCACTAATAACTCCCCAACTATAATCGCTGTATGCACAGTGCTCTTCCGTGTGGACTTCCGGTTTTATTGCCGCATCTCCTTTCTAGGAG',\n",
       " 'AGTGCTTTTGATATGCGTCTAAAGATTCCGGCTAAACCACCTGCCACGTGACCTTTGACCCCTAGAATTGAGCAAAGGTACGAACGGAACTGTCTTGACCTTTGACCTCCGCGATAGTCTAGGTCAAAGGTCATGGATGAATTTAAAAAGTAGTATCCCTTCGGAAGACTGATACTTAGTTTATCTCTTTGAAGGATTAT',\n",
       " 'TAGACACAGTGAGACTCCACCTGATCTGAGAACTTTTAACAGGGGTCAAAGGTCATACTTGTAAGCGTCGGCATTGCAGAATCGCAAGAGTGCACTTCGATTTTAGAAAAATAAGGGTCAAAGGTCATCGGAATGCATAAGGTCAAAGGTCATATGATGCTCGATGGAGTCGTCAGGGGGCTACAGATTAGGGGATGAAT',\n",
       " 'GGTGAAGCTCCCAGCTAGGCCGAGTGCGCTACTTCGACGTAGTGGTATGTCATGACGGAATTAAATGCTAGTACCATACTATATGCAGGTCGGTTGCTGCAGCGTTAGCTTGACCTTTGACCTCAAGTCAATCCTCTTACCGGGATCATCGTTAATGTGAAGTTTTTACGTTCGACGCTTAAATCGGTTGCGGGGTTTTT',\n",
       " 'ACCTCAGTTAGGCGAAAGATCTGAGCTAAGCGTAGAAATACTTTTGGATAGAGACGTCAGGCATGCGTGTAGACTCATCACCTTCATTTCATTTGTGAACAAAAAAAAGGTTAGTTGACATAGAAAAAAAAACTTGGTATAGATGGAACCCGGAAGTGCGTCATGCCAAGCATCAATCTTTTAATTCTTAAGTAAGAAAC',\n",
       " 'CTCGTGGGAGTAACTTGACTCACCATTTCAGAAATATGCGAGTAACTTCCGGGTCTCCAATATCAAAAAACCATGTATGTTTAGCAACATTTAGAATAAGATTAAATCACTTACATTTCCGGTTTGTGTCGAGGGTCTTGAGTTAAGAAGATTAACAGGTTTAATGTATAGAATTAATCCAAGAGCCATAACTTATCGAG',\n",
       " 'ACAGACTACACCATATATGACCTTTGACCTCTACTTCGGATTCGTGTTAGAAAAGTTGATGTCTTAATGTGCTGTTGCTATGTTCGCTCTTCCGGCTCGTATGACCTTTGACCCGTCGCCTGAGTCCATCGAGCAAATGGTTAAACGAAACGTGTGACCCATGACCTTTGACCTCTGCAACTGAGAAATCGCGTACTACC',\n",
       " 'ATAGATAAGGTGACCTTTGACCTCGCGAATCCTGACTTTCCAAAGGCAAACGTTAGTCCAGGGGTCAAAGGTCATCTACTATATAAGTTGGTCAATATAATCCATCCTGAACAAGTATGTAGGGGTCACAGGTCAAACTGCCATGTAATCACACGCGATTAGATCCCATGAAACAAGGGTCAACGTAAACTCCGATATCG',\n",
       " 'TTCAACCAAGTGTGTATGTTCAAATATCTTGATTTTTTAATGTCACATCTAAATGTATATAAATTATGATCTTGATTGAGAAGCTTTACGTTGAGAGTGCCTGTTAACTTCATACTCAGTAACTTCCGGGGACCTCAAATGATTTCCGGGCTAGAGAAGAGCAGACACTTCCGGGTGTCAGTTAGAATAAGTTTCTCGAT',\n",
       " 'CCATTAACACTTCCGGATCAGTTTCACGAGAATTGGTCGTCATAAGGAACTGGTACTCCGGGACGCAGTTGTCAACCTTCATTGGATTATGTGACAGTGGCAAGTACGGACTACTTAAATAAATTTCATTGTTACCCGGAAGTGAACTAACCCGGAAGTGTAATGTAGGTCGTTCTCTCAGGGGATTCTGCACGCACGTC',\n",
       " 'GTATTGCACAATAAGAAGGCGTTAGATAAGCGTCAGATTAGGTGGAGGAAATATCGATAGATGCTGACTTCCTATCATACTTTCTGTATACTACGTTGTCGTGTAACTTGGTCTACAGCTTATGGTAATTAATATTAGCTTTTATCAGGCTCTTCAATCGCGCAAACCGAAACCAATTCCACTTACTGAATTGTGGTGTT',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltrdict = {'a':[1,0,0,0],\n",
    "           'c':[0,1,0,0],\n",
    "           'g':[0,0,1,0],\n",
    "           't':[0,0,0,1],\n",
    "           'n':[0,0,0,0],\n",
    "           'A':[1,0,0,0],\n",
    "           'C':[0,1,0,0],\n",
    "           'G':[0,0,1,0],\n",
    "           'T':[0,0,0,1],\n",
    "           'N':[0,0,0,0]}\n",
    "\n",
    "def onehot_encode(sequence):\n",
    "    return np.array([ltrdict[x] for x in sequence])\n",
    "\n",
    "x_curr = np.array([onehot_encode(curr_seq) for curr_seq in x_curr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "y_curr = np.zeros((0,3))\n",
    "with open(\"simdata.labels\", \"r\") as f: \n",
    "    reader = csv.reader(f, delimiter = \"\\t\")\n",
    "    d = list(reader)\n",
    "    curr_num = 0\n",
    "    for thing in range(1, len(d)):\n",
    "        curr = d[thing]\n",
    "        other = []\n",
    "        for x in range(1, 4): \n",
    "            other.append(float(curr[x]))\n",
    "        y_curr = np.append(y_curr, [other], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_curr, y_curr, test_size = 0.3, random_state = 1000)\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 3/7, random_state = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mutate(y, mutation_prob):\n",
    "#     np.random.seed(1234)\n",
    "#     mutated_y = []\n",
    "#     for row in y:\n",
    "#         new_labels_for_row = []\n",
    "#         for label in row:\n",
    "#             if np.random.uniform() < mutation_prob:\n",
    "#                 new_labels_for_row.append(1-label)\n",
    "#             else:\n",
    "#                 new_labels_for_row.append(label)\n",
    "#         mutated_y.append(new_labels_for_row)\n",
    "#     return np.array(mutated_y)\n",
    "\n",
    "# y_train_mutate = mutate(y_train, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np \n",
    "# import tensorflow as tf \n",
    "# import keras_genomics\n",
    "# import tensorflow as tf\n",
    "# import keras \n",
    "# import keras_genomics\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import gzip\n",
    "\n",
    "# from numpy.random import seed\n",
    "# from tensorflow import set_random_seed\n",
    "# from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import average_precision_score\n",
    "# from simdna.synthetic.core import read_simdata_file\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# def train_model_other(model_wrapper, aug, curr_seed, batch_size, x, y):\n",
    "#     np.random.seed(curr_seed)\n",
    "#     tf.set_random_seed(curr_seed)\n",
    "    \n",
    "#     model = model_wrapper.get_model()\n",
    "    \n",
    "#     if aug == \"rev_after_each\": \n",
    "#         x_train = np.asarray([val for val in x for __ in (0,1)])\n",
    "#         y_train = np.asarray([val for val in y for __ in (0,1)])\n",
    "#         for i in range(len(x_train)):\n",
    "#             if i % 2 == 1:\n",
    "#                 x_train[i] = np.flip(x_train[i])\n",
    "#     elif aug == \"rev_after_all\":\n",
    "#         x_train = np.concatenate([x,x])\n",
    "#         y_train = np.concatenate([y,y])\n",
    "#         for i in range(len(x/2) + 1, len(x)):\n",
    "#             x_train[i] = np.flip(x_train[i])\n",
    "#     else: \n",
    "#         x_train = x\n",
    "#         y_train = y\n",
    "        \n",
    "#     early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "#                               monitor='val_loss',\n",
    "#                               patience = 10,\n",
    "#                               restore_best_weights=True)\n",
    "    \n",
    "#     model.fit(x = x_train, y = y_train, validation_split = 3/7,  \n",
    "#               callbacks =[early_stopping_callback], \n",
    "#               batch_size=batch_size, epochs=200)\n",
    "    \n",
    "#     return model, early_stopping_callback\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "12000/12000 [==============================] - 4s 333us/step - loss: 0.7585 - acc: 0.6032 - val_loss: 0.6778 - val_acc: 0.6562\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6382 - acc: 0.6540 - val_loss: 0.6293 - val_acc: 0.6656\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6016 - acc: 0.6853 - val_loss: 0.6016 - val_acc: 0.6870\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5724 - acc: 0.7073 - val_loss: 0.5802 - val_acc: 0.7016\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5487 - acc: 0.7256 - val_loss: 0.5600 - val_acc: 0.7177\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5287 - acc: 0.7380 - val_loss: 0.5420 - val_acc: 0.7313\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5129 - acc: 0.7495 - val_loss: 0.5295 - val_acc: 0.7391\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4990 - acc: 0.7596 - val_loss: 0.5185 - val_acc: 0.7471\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4874 - acc: 0.7713 - val_loss: 0.5097 - val_acc: 0.7530\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4772 - acc: 0.7777 - val_loss: 0.4994 - val_acc: 0.7596\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4679 - acc: 0.7862 - val_loss: 0.4900 - val_acc: 0.7707\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4590 - acc: 0.7951 - val_loss: 0.4830 - val_acc: 0.7764\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4516 - acc: 0.8005 - val_loss: 0.4763 - val_acc: 0.7808\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4460 - acc: 0.8043 - val_loss: 0.4740 - val_acc: 0.7823\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4395 - acc: 0.8095 - val_loss: 0.4660 - val_acc: 0.7890\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4336 - acc: 0.8143 - val_loss: 0.4614 - val_acc: 0.7953\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4281 - acc: 0.8188 - val_loss: 0.4580 - val_acc: 0.7974\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4237 - acc: 0.8226 - val_loss: 0.4539 - val_acc: 0.8017\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4208 - acc: 0.8243 - val_loss: 0.4515 - val_acc: 0.8022\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4165 - acc: 0.8274 - val_loss: 0.4476 - val_acc: 0.8075\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4135 - acc: 0.8274 - val_loss: 0.4451 - val_acc: 0.8129\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4101 - acc: 0.8319 - val_loss: 0.4436 - val_acc: 0.8087\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4073 - acc: 0.8338 - val_loss: 0.4399 - val_acc: 0.8156\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4039 - acc: 0.8354 - val_loss: 0.4396 - val_acc: 0.8134\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4015 - acc: 0.8375 - val_loss: 0.4362 - val_acc: 0.8182\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3989 - acc: 0.8398 - val_loss: 0.4344 - val_acc: 0.8196\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3962 - acc: 0.8419 - val_loss: 0.4337 - val_acc: 0.8204\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3942 - acc: 0.8420 - val_loss: 0.4310 - val_acc: 0.8216\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3922 - acc: 0.8437 - val_loss: 0.4311 - val_acc: 0.8207\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3899 - acc: 0.8446 - val_loss: 0.4292 - val_acc: 0.8230\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3886 - acc: 0.8454 - val_loss: 0.4284 - val_acc: 0.8250\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3863 - acc: 0.8473 - val_loss: 0.4275 - val_acc: 0.8258\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3851 - acc: 0.8473 - val_loss: 0.4273 - val_acc: 0.8276\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3852 - acc: 0.8475 - val_loss: 0.4281 - val_acc: 0.8263\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3820 - acc: 0.8492 - val_loss: 0.4241 - val_acc: 0.8286\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3805 - acc: 0.8507 - val_loss: 0.4235 - val_acc: 0.8314\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3793 - acc: 0.8521 - val_loss: 0.4231 - val_acc: 0.8311\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3784 - acc: 0.8514 - val_loss: 0.4243 - val_acc: 0.8304\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3762 - acc: 0.8524 - val_loss: 0.4221 - val_acc: 0.8301\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3748 - acc: 0.8540 - val_loss: 0.4227 - val_acc: 0.8305\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3736 - acc: 0.8545 - val_loss: 0.4214 - val_acc: 0.8336\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3720 - acc: 0.8548 - val_loss: 0.4206 - val_acc: 0.8318\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3713 - acc: 0.8563 - val_loss: 0.4204 - val_acc: 0.8333\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3715 - acc: 0.8568 - val_loss: 0.4230 - val_acc: 0.8313\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3705 - acc: 0.8558 - val_loss: 0.4203 - val_acc: 0.8326\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3679 - acc: 0.8574 - val_loss: 0.4194 - val_acc: 0.8346\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3676 - acc: 0.8581 - val_loss: 0.4195 - val_acc: 0.8346\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3672 - acc: 0.8574 - val_loss: 0.4216 - val_acc: 0.8327\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3674 - acc: 0.8585 - val_loss: 0.4197 - val_acc: 0.8336\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3650 - acc: 0.8604 - val_loss: 0.4214 - val_acc: 0.8322\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3642 - acc: 0.8599 - val_loss: 0.4194 - val_acc: 0.8361\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3640 - acc: 0.8598 - val_loss: 0.4187 - val_acc: 0.8366\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3628 - acc: 0.8617 - val_loss: 0.4181 - val_acc: 0.8364\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3628 - acc: 0.8595 - val_loss: 0.4199 - val_acc: 0.8331\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3611 - acc: 0.8624 - val_loss: 0.4178 - val_acc: 0.8374\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3604 - acc: 0.8624 - val_loss: 0.4180 - val_acc: 0.8363\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3596 - acc: 0.8618 - val_loss: 0.4181 - val_acc: 0.8369\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3596 - acc: 0.8625 - val_loss: 0.4183 - val_acc: 0.8356\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3587 - acc: 0.8624 - val_loss: 0.4179 - val_acc: 0.8356\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3583 - acc: 0.8628 - val_loss: 0.4179 - val_acc: 0.8365\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3574 - acc: 0.8633 - val_loss: 0.4180 - val_acc: 0.8367\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3572 - acc: 0.8635 - val_loss: 0.4180 - val_acc: 0.8371\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3562 - acc: 0.8633 - val_loss: 0.4176 - val_acc: 0.8377\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3572 - acc: 0.8634 - val_loss: 0.4185 - val_acc: 0.8390\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3562 - acc: 0.8646 - val_loss: 0.4203 - val_acc: 0.8366\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3558 - acc: 0.8648 - val_loss: 0.4213 - val_acc: 0.8371\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3554 - acc: 0.8642 - val_loss: 0.4216 - val_acc: 0.8353\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3561 - acc: 0.8632 - val_loss: 0.4184 - val_acc: 0.8370\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3529 - acc: 0.8657 - val_loss: 0.4191 - val_acc: 0.8392\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3526 - acc: 0.8661 - val_loss: 0.4184 - val_acc: 0.8384\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3523 - acc: 0.8665 - val_loss: 0.4181 - val_acc: 0.8374\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3519 - acc: 0.8652 - val_loss: 0.4199 - val_acc: 0.8363\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3516 - acc: 0.8663 - val_loss: 0.4212 - val_acc: 0.8342\n",
      "auroc: 0.8721438987976567\n",
      "auprc: 0.7791948211641335\n",
      "auroc: 0.8719992663187591\n",
      "auprc: 0.7790125444036929\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 1s 66us/step - loss: 0.7013 - acc: 0.6202 - val_loss: 0.6786 - val_acc: 0.6363\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6541 - acc: 0.6474 - val_loss: 0.6543 - val_acc: 0.6504\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6289 - acc: 0.6606 - val_loss: 0.6357 - val_acc: 0.6616\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6055 - acc: 0.6811 - val_loss: 0.6134 - val_acc: 0.6741\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5802 - acc: 0.6988 - val_loss: 0.5880 - val_acc: 0.6973\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5543 - acc: 0.7187 - val_loss: 0.5649 - val_acc: 0.7078\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5301 - acc: 0.7402 - val_loss: 0.5447 - val_acc: 0.7295\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5105 - acc: 0.7548 - val_loss: 0.5261 - val_acc: 0.7441\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4937 - acc: 0.7679 - val_loss: 0.5121 - val_acc: 0.7553\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4806 - acc: 0.7782 - val_loss: 0.5012 - val_acc: 0.7614\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4682 - acc: 0.7870 - val_loss: 0.4908 - val_acc: 0.7703\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4583 - acc: 0.7954 - val_loss: 0.4831 - val_acc: 0.7776\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4500 - acc: 0.8025 - val_loss: 0.4801 - val_acc: 0.7796\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4420 - acc: 0.8070 - val_loss: 0.4700 - val_acc: 0.7881\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4348 - acc: 0.8132 - val_loss: 0.4650 - val_acc: 0.7929\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4289 - acc: 0.8168 - val_loss: 0.4598 - val_acc: 0.7977\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4244 - acc: 0.8202 - val_loss: 0.4551 - val_acc: 0.8019\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4198 - acc: 0.8236 - val_loss: 0.4533 - val_acc: 0.8028\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4154 - acc: 0.8280 - val_loss: 0.4510 - val_acc: 0.8084\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4121 - acc: 0.8314 - val_loss: 0.4478 - val_acc: 0.8065\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4085 - acc: 0.8329 - val_loss: 0.4433 - val_acc: 0.8139\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4059 - acc: 0.8353 - val_loss: 0.4428 - val_acc: 0.8134\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4028 - acc: 0.8367 - val_loss: 0.4416 - val_acc: 0.8150\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4007 - acc: 0.8378 - val_loss: 0.4393 - val_acc: 0.8174\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3981 - acc: 0.8399 - val_loss: 0.4386 - val_acc: 0.8151\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3970 - acc: 0.8408 - val_loss: 0.4358 - val_acc: 0.8183\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3948 - acc: 0.8416 - val_loss: 0.4338 - val_acc: 0.8207\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3925 - acc: 0.8423 - val_loss: 0.4344 - val_acc: 0.8201\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3909 - acc: 0.8432 - val_loss: 0.4343 - val_acc: 0.8193\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3888 - acc: 0.8458 - val_loss: 0.4326 - val_acc: 0.8209\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3874 - acc: 0.8465 - val_loss: 0.4312 - val_acc: 0.8228\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3860 - acc: 0.8479 - val_loss: 0.4306 - val_acc: 0.8246\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3840 - acc: 0.8494 - val_loss: 0.4305 - val_acc: 0.8237\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3828 - acc: 0.8494 - val_loss: 0.4295 - val_acc: 0.8240\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3823 - acc: 0.8493 - val_loss: 0.4279 - val_acc: 0.8239\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3802 - acc: 0.8512 - val_loss: 0.4272 - val_acc: 0.8261\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3792 - acc: 0.8523 - val_loss: 0.4272 - val_acc: 0.8273\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3782 - acc: 0.8525 - val_loss: 0.4291 - val_acc: 0.8253\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3779 - acc: 0.8526 - val_loss: 0.4345 - val_acc: 0.8209\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3769 - acc: 0.8541 - val_loss: 0.4269 - val_acc: 0.8265\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3749 - acc: 0.8549 - val_loss: 0.4277 - val_acc: 0.8253\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3748 - acc: 0.8544 - val_loss: 0.4271 - val_acc: 0.8269\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3730 - acc: 0.8553 - val_loss: 0.4267 - val_acc: 0.8277\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3716 - acc: 0.8564 - val_loss: 0.4245 - val_acc: 0.8302\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3708 - acc: 0.8558 - val_loss: 0.4253 - val_acc: 0.8283\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3704 - acc: 0.8567 - val_loss: 0.4258 - val_acc: 0.8272\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3686 - acc: 0.8570 - val_loss: 0.4244 - val_acc: 0.8289\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3689 - acc: 0.8569 - val_loss: 0.4247 - val_acc: 0.8288\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3677 - acc: 0.8591 - val_loss: 0.4250 - val_acc: 0.8284\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3675 - acc: 0.8576 - val_loss: 0.4264 - val_acc: 0.8280\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3658 - acc: 0.8588 - val_loss: 0.4259 - val_acc: 0.8279\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3654 - acc: 0.8590 - val_loss: 0.4263 - val_acc: 0.8287\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3641 - acc: 0.8603 - val_loss: 0.4243 - val_acc: 0.8297\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3641 - acc: 0.8585 - val_loss: 0.4241 - val_acc: 0.8295\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3642 - acc: 0.8586 - val_loss: 0.4253 - val_acc: 0.8285\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3630 - acc: 0.8604 - val_loss: 0.4243 - val_acc: 0.8300\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3617 - acc: 0.8601 - val_loss: 0.4276 - val_acc: 0.8259\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3614 - acc: 0.8604 - val_loss: 0.4248 - val_acc: 0.8307\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3600 - acc: 0.8621 - val_loss: 0.4246 - val_acc: 0.8310\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3602 - acc: 0.8622 - val_loss: 0.4249 - val_acc: 0.8306\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3591 - acc: 0.8619 - val_loss: 0.4251 - val_acc: 0.8309\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3597 - acc: 0.8621 - val_loss: 0.4281 - val_acc: 0.8293\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3590 - acc: 0.8622 - val_loss: 0.4255 - val_acc: 0.8310\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3578 - acc: 0.8633 - val_loss: 0.4245 - val_acc: 0.8308\n",
      "auroc: 0.8682301167848845\n",
      "auprc: 0.7708235917930595\n",
      "auroc: 0.8678129499832758\n",
      "auprc: 0.7695710791411093\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 1s 73us/step - loss: 0.7307 - acc: 0.6079 - val_loss: 0.6750 - val_acc: 0.6354\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6500 - acc: 0.6403 - val_loss: 0.6477 - val_acc: 0.6489\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6220 - acc: 0.6667 - val_loss: 0.6252 - val_acc: 0.6672\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5982 - acc: 0.6860 - val_loss: 0.6058 - val_acc: 0.6740\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5760 - acc: 0.7031 - val_loss: 0.5833 - val_acc: 0.6974\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5544 - acc: 0.7203 - val_loss: 0.5637 - val_acc: 0.7103\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5345 - acc: 0.7350 - val_loss: 0.5471 - val_acc: 0.7272\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5160 - acc: 0.7490 - val_loss: 0.5305 - val_acc: 0.7392\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5005 - acc: 0.7603 - val_loss: 0.5206 - val_acc: 0.7475\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4879 - acc: 0.7693 - val_loss: 0.5079 - val_acc: 0.7577\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4766 - acc: 0.7781 - val_loss: 0.5007 - val_acc: 0.7610\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4671 - acc: 0.7844 - val_loss: 0.4882 - val_acc: 0.7730\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4583 - acc: 0.7926 - val_loss: 0.4826 - val_acc: 0.7770\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4510 - acc: 0.7978 - val_loss: 0.4762 - val_acc: 0.7823\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4431 - acc: 0.8038 - val_loss: 0.4700 - val_acc: 0.7906\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4386 - acc: 0.8075 - val_loss: 0.4638 - val_acc: 0.7928\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4322 - acc: 0.8138 - val_loss: 0.4592 - val_acc: 0.7960\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4273 - acc: 0.8165 - val_loss: 0.4555 - val_acc: 0.8055\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4216 - acc: 0.8217 - val_loss: 0.4505 - val_acc: 0.8063\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4159 - acc: 0.8253 - val_loss: 0.4466 - val_acc: 0.8091\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4118 - acc: 0.8279 - val_loss: 0.4438 - val_acc: 0.8121\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4079 - acc: 0.8314 - val_loss: 0.4411 - val_acc: 0.8133\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4047 - acc: 0.8330 - val_loss: 0.4388 - val_acc: 0.8170\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4009 - acc: 0.8363 - val_loss: 0.4363 - val_acc: 0.8199\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3981 - acc: 0.8376 - val_loss: 0.4355 - val_acc: 0.8210\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3952 - acc: 0.8399 - val_loss: 0.4326 - val_acc: 0.8205\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3921 - acc: 0.8421 - val_loss: 0.4312 - val_acc: 0.8244\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3906 - acc: 0.8433 - val_loss: 0.4301 - val_acc: 0.8266\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3880 - acc: 0.8443 - val_loss: 0.4276 - val_acc: 0.8269\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3869 - acc: 0.8458 - val_loss: 0.4264 - val_acc: 0.8294\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3843 - acc: 0.8467 - val_loss: 0.4290 - val_acc: 0.8275\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3843 - acc: 0.8476 - val_loss: 0.4257 - val_acc: 0.8302\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3815 - acc: 0.8494 - val_loss: 0.4245 - val_acc: 0.8331\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3806 - acc: 0.8493 - val_loss: 0.4229 - val_acc: 0.8335\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3788 - acc: 0.8510 - val_loss: 0.4230 - val_acc: 0.8327\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3768 - acc: 0.8521 - val_loss: 0.4230 - val_acc: 0.8333\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3756 - acc: 0.8528 - val_loss: 0.4213 - val_acc: 0.8345\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3741 - acc: 0.8537 - val_loss: 0.4208 - val_acc: 0.8345\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3732 - acc: 0.8543 - val_loss: 0.4213 - val_acc: 0.8343\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3723 - acc: 0.8556 - val_loss: 0.4214 - val_acc: 0.8329\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3722 - acc: 0.8545 - val_loss: 0.4203 - val_acc: 0.8341\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3701 - acc: 0.8556 - val_loss: 0.4221 - val_acc: 0.8329\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3695 - acc: 0.8556 - val_loss: 0.4199 - val_acc: 0.8363\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3689 - acc: 0.8573 - val_loss: 0.4212 - val_acc: 0.8332\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3673 - acc: 0.8578 - val_loss: 0.4194 - val_acc: 0.8361\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3670 - acc: 0.8575 - val_loss: 0.4210 - val_acc: 0.8336\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3673 - acc: 0.8562 - val_loss: 0.4219 - val_acc: 0.8339\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3669 - acc: 0.8578 - val_loss: 0.4209 - val_acc: 0.8350\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3645 - acc: 0.8578 - val_loss: 0.4211 - val_acc: 0.8348\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3641 - acc: 0.8592 - val_loss: 0.4207 - val_acc: 0.8354\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3627 - acc: 0.8598 - val_loss: 0.4194 - val_acc: 0.8376\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3613 - acc: 0.8606 - val_loss: 0.4189 - val_acc: 0.8361\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3627 - acc: 0.8586 - val_loss: 0.4201 - val_acc: 0.8372\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3617 - acc: 0.8595 - val_loss: 0.4208 - val_acc: 0.8344\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3605 - acc: 0.8608 - val_loss: 0.4223 - val_acc: 0.8353\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3593 - acc: 0.8616 - val_loss: 0.4184 - val_acc: 0.8386\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3590 - acc: 0.8617 - val_loss: 0.4237 - val_acc: 0.8368\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3597 - acc: 0.8622 - val_loss: 0.4190 - val_acc: 0.8377\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3576 - acc: 0.8629 - val_loss: 0.4186 - val_acc: 0.8377\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3576 - acc: 0.8621 - val_loss: 0.4212 - val_acc: 0.8384\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3578 - acc: 0.8620 - val_loss: 0.4191 - val_acc: 0.8368\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3562 - acc: 0.8626 - val_loss: 0.4189 - val_acc: 0.8381\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3561 - acc: 0.8632 - val_loss: 0.4188 - val_acc: 0.8393\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3554 - acc: 0.8624 - val_loss: 0.4197 - val_acc: 0.8362\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3554 - acc: 0.8621 - val_loss: 0.4185 - val_acc: 0.8387\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3544 - acc: 0.8643 - val_loss: 0.4206 - val_acc: 0.8363\n",
      "auroc: 0.8720692511290173\n",
      "auprc: 0.7769583161279853\n",
      "auroc: 0.8715043804723129\n",
      "auprc: 0.776025482189998\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 1s 78us/step - loss: 0.7956 - acc: 0.5891 - val_loss: 0.6906 - val_acc: 0.6377\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6575 - acc: 0.6436 - val_loss: 0.6396 - val_acc: 0.6552\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6247 - acc: 0.6654 - val_loss: 0.6198 - val_acc: 0.6692\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6014 - acc: 0.6813 - val_loss: 0.6010 - val_acc: 0.6853\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5801 - acc: 0.7001 - val_loss: 0.5847 - val_acc: 0.6979\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5605 - acc: 0.7157 - val_loss: 0.5664 - val_acc: 0.7129\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5418 - acc: 0.7298 - val_loss: 0.5516 - val_acc: 0.7244\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5254 - acc: 0.7447 - val_loss: 0.5411 - val_acc: 0.7320\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5104 - acc: 0.7564 - val_loss: 0.5250 - val_acc: 0.7449\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4969 - acc: 0.7697 - val_loss: 0.5147 - val_acc: 0.7526\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4849 - acc: 0.7784 - val_loss: 0.5038 - val_acc: 0.7618\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4742 - acc: 0.7859 - val_loss: 0.4955 - val_acc: 0.7690\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4656 - acc: 0.7926 - val_loss: 0.4917 - val_acc: 0.7698\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4574 - acc: 0.7983 - val_loss: 0.4809 - val_acc: 0.7806\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4507 - acc: 0.8029 - val_loss: 0.4796 - val_acc: 0.7787\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4438 - acc: 0.8092 - val_loss: 0.4713 - val_acc: 0.7863\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4385 - acc: 0.8113 - val_loss: 0.4652 - val_acc: 0.7954\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4326 - acc: 0.8170 - val_loss: 0.4608 - val_acc: 0.7989\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4278 - acc: 0.8199 - val_loss: 0.4571 - val_acc: 0.8014\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4233 - acc: 0.8232 - val_loss: 0.4538 - val_acc: 0.8045\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4194 - acc: 0.8249 - val_loss: 0.4517 - val_acc: 0.8041\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4150 - acc: 0.8291 - val_loss: 0.4491 - val_acc: 0.8074\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4119 - acc: 0.8308 - val_loss: 0.4452 - val_acc: 0.8135\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4088 - acc: 0.8316 - val_loss: 0.4429 - val_acc: 0.8123\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4058 - acc: 0.8341 - val_loss: 0.4413 - val_acc: 0.8156\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4026 - acc: 0.8362 - val_loss: 0.4402 - val_acc: 0.8176\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4002 - acc: 0.8376 - val_loss: 0.4372 - val_acc: 0.8193\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3981 - acc: 0.8385 - val_loss: 0.4354 - val_acc: 0.8197\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3954 - acc: 0.8411 - val_loss: 0.4352 - val_acc: 0.8199\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3937 - acc: 0.8425 - val_loss: 0.4329 - val_acc: 0.8220\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3912 - acc: 0.8432 - val_loss: 0.4320 - val_acc: 0.8233\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3901 - acc: 0.8441 - val_loss: 0.4320 - val_acc: 0.8248\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3882 - acc: 0.8450 - val_loss: 0.4308 - val_acc: 0.8239\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3872 - acc: 0.8466 - val_loss: 0.4296 - val_acc: 0.8258\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3859 - acc: 0.8469 - val_loss: 0.4297 - val_acc: 0.8268\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3842 - acc: 0.8487 - val_loss: 0.4287 - val_acc: 0.8262\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3830 - acc: 0.8485 - val_loss: 0.4304 - val_acc: 0.8217\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.3820 - acc: 0.8490 - val_loss: 0.4277 - val_acc: 0.8280\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3807 - acc: 0.8494 - val_loss: 0.4275 - val_acc: 0.8290\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3804 - acc: 0.8508 - val_loss: 0.4269 - val_acc: 0.8293\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3787 - acc: 0.8515 - val_loss: 0.4270 - val_acc: 0.8294\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3776 - acc: 0.8526 - val_loss: 0.4265 - val_acc: 0.8283\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3757 - acc: 0.8538 - val_loss: 0.4267 - val_acc: 0.8278\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3757 - acc: 0.8529 - val_loss: 0.4255 - val_acc: 0.8289\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3743 - acc: 0.8543 - val_loss: 0.4250 - val_acc: 0.8300\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3733 - acc: 0.8550 - val_loss: 0.4247 - val_acc: 0.8297\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3719 - acc: 0.8563 - val_loss: 0.4257 - val_acc: 0.8303\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3715 - acc: 0.8571 - val_loss: 0.4247 - val_acc: 0.8303\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3710 - acc: 0.8572 - val_loss: 0.4249 - val_acc: 0.8309\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3697 - acc: 0.8576 - val_loss: 0.4246 - val_acc: 0.8309\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3694 - acc: 0.8573 - val_loss: 0.4264 - val_acc: 0.8293\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3690 - acc: 0.8581 - val_loss: 0.4246 - val_acc: 0.8308\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3672 - acc: 0.8575 - val_loss: 0.4247 - val_acc: 0.8305\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3675 - acc: 0.8579 - val_loss: 0.4237 - val_acc: 0.8321\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3657 - acc: 0.8586 - val_loss: 0.4230 - val_acc: 0.8336\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3645 - acc: 0.8598 - val_loss: 0.4223 - val_acc: 0.8330\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3642 - acc: 0.8599 - val_loss: 0.4232 - val_acc: 0.8329\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3640 - acc: 0.8591 - val_loss: 0.4234 - val_acc: 0.8330\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3625 - acc: 0.8611 - val_loss: 0.4224 - val_acc: 0.8333\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3625 - acc: 0.8609 - val_loss: 0.4236 - val_acc: 0.8340\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3619 - acc: 0.8609 - val_loss: 0.4239 - val_acc: 0.8330\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3609 - acc: 0.8613 - val_loss: 0.4250 - val_acc: 0.8317\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3603 - acc: 0.8613 - val_loss: 0.4253 - val_acc: 0.8326\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3602 - acc: 0.8627 - val_loss: 0.4260 - val_acc: 0.8325\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3609 - acc: 0.8607 - val_loss: 0.4251 - val_acc: 0.8313\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3586 - acc: 0.8620 - val_loss: 0.4235 - val_acc: 0.8344\n",
      "auroc: 0.8694103228800967\n",
      "auprc: 0.7726299839336694\n",
      "auroc: 0.8690331910530417\n",
      "auprc: 0.7720104303526211\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 1s 88us/step - loss: 0.8096 - acc: 0.5844 - val_loss: 0.7074 - val_acc: 0.6247\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6649 - acc: 0.6396 - val_loss: 0.6563 - val_acc: 0.6370\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6337 - acc: 0.6600 - val_loss: 0.6390 - val_acc: 0.6527\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6132 - acc: 0.6753 - val_loss: 0.6237 - val_acc: 0.6677\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5950 - acc: 0.6896 - val_loss: 0.6086 - val_acc: 0.6766\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5766 - acc: 0.7044 - val_loss: 0.5917 - val_acc: 0.6902\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5588 - acc: 0.7190 - val_loss: 0.5756 - val_acc: 0.7070\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5398 - acc: 0.7334 - val_loss: 0.5591 - val_acc: 0.7195\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5215 - acc: 0.7477 - val_loss: 0.5430 - val_acc: 0.7330\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5055 - acc: 0.7596 - val_loss: 0.5307 - val_acc: 0.7406\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4921 - acc: 0.7684 - val_loss: 0.5206 - val_acc: 0.7502\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4798 - acc: 0.7782 - val_loss: 0.5073 - val_acc: 0.7610\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4699 - acc: 0.7844 - val_loss: 0.4978 - val_acc: 0.7664\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4610 - acc: 0.7916 - val_loss: 0.4901 - val_acc: 0.7728\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4523 - acc: 0.7974 - val_loss: 0.4873 - val_acc: 0.7755\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4462 - acc: 0.8020 - val_loss: 0.4787 - val_acc: 0.7799\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4399 - acc: 0.8058 - val_loss: 0.4740 - val_acc: 0.7849\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4340 - acc: 0.8132 - val_loss: 0.4693 - val_acc: 0.7884\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4287 - acc: 0.8167 - val_loss: 0.4666 - val_acc: 0.7918\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4250 - acc: 0.8196 - val_loss: 0.4619 - val_acc: 0.7946\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4201 - acc: 0.8232 - val_loss: 0.4574 - val_acc: 0.8004\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4164 - acc: 0.8247 - val_loss: 0.4551 - val_acc: 0.8013\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4130 - acc: 0.8266 - val_loss: 0.4522 - val_acc: 0.8066\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4102 - acc: 0.8296 - val_loss: 0.4544 - val_acc: 0.8030\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4082 - acc: 0.8295 - val_loss: 0.4504 - val_acc: 0.8077\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4047 - acc: 0.8324 - val_loss: 0.4491 - val_acc: 0.8078\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4021 - acc: 0.8349 - val_loss: 0.4455 - val_acc: 0.8096\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3998 - acc: 0.8352 - val_loss: 0.4430 - val_acc: 0.8131\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3974 - acc: 0.8357 - val_loss: 0.4417 - val_acc: 0.8146\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3961 - acc: 0.8379 - val_loss: 0.4410 - val_acc: 0.8161\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3936 - acc: 0.8390 - val_loss: 0.4398 - val_acc: 0.8163\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3916 - acc: 0.8401 - val_loss: 0.4387 - val_acc: 0.8161\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3902 - acc: 0.8404 - val_loss: 0.4386 - val_acc: 0.8183\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3888 - acc: 0.8425 - val_loss: 0.4395 - val_acc: 0.8168\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3887 - acc: 0.8414 - val_loss: 0.4365 - val_acc: 0.8201\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3857 - acc: 0.8437 - val_loss: 0.4400 - val_acc: 0.8168\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3853 - acc: 0.8439 - val_loss: 0.4352 - val_acc: 0.8221\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3837 - acc: 0.8446 - val_loss: 0.4363 - val_acc: 0.8207\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3814 - acc: 0.8459 - val_loss: 0.4347 - val_acc: 0.8209\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3802 - acc: 0.8466 - val_loss: 0.4361 - val_acc: 0.8203\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3796 - acc: 0.8486 - val_loss: 0.4322 - val_acc: 0.8235\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3783 - acc: 0.8486 - val_loss: 0.4332 - val_acc: 0.8248\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3770 - acc: 0.8500 - val_loss: 0.4324 - val_acc: 0.8233\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3761 - acc: 0.8498 - val_loss: 0.4313 - val_acc: 0.8253\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3750 - acc: 0.8502 - val_loss: 0.4305 - val_acc: 0.8250\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3746 - acc: 0.8504 - val_loss: 0.4311 - val_acc: 0.8253\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3735 - acc: 0.8508 - val_loss: 0.4307 - val_acc: 0.8254\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3719 - acc: 0.8532 - val_loss: 0.4313 - val_acc: 0.8251\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3709 - acc: 0.8532 - val_loss: 0.4285 - val_acc: 0.8277\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3715 - acc: 0.8536 - val_loss: 0.4313 - val_acc: 0.8258\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3706 - acc: 0.8533 - val_loss: 0.4292 - val_acc: 0.8259\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3683 - acc: 0.8547 - val_loss: 0.4276 - val_acc: 0.8277\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3674 - acc: 0.8554 - val_loss: 0.4282 - val_acc: 0.8274\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3670 - acc: 0.8563 - val_loss: 0.4294 - val_acc: 0.8275\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3668 - acc: 0.8557 - val_loss: 0.4281 - val_acc: 0.8286\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3664 - acc: 0.8558 - val_loss: 0.4272 - val_acc: 0.8288\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3654 - acc: 0.8558 - val_loss: 0.4293 - val_acc: 0.8277\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3644 - acc: 0.8572 - val_loss: 0.4276 - val_acc: 0.8282\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3640 - acc: 0.8570 - val_loss: 0.4284 - val_acc: 0.8294\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3635 - acc: 0.8571 - val_loss: 0.4266 - val_acc: 0.8293\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3624 - acc: 0.8589 - val_loss: 0.4274 - val_acc: 0.8286\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3621 - acc: 0.8596 - val_loss: 0.4269 - val_acc: 0.8292\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3619 - acc: 0.8597 - val_loss: 0.4265 - val_acc: 0.8299\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3608 - acc: 0.8602 - val_loss: 0.4271 - val_acc: 0.8302\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3603 - acc: 0.8606 - val_loss: 0.4270 - val_acc: 0.8303\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3595 - acc: 0.8609 - val_loss: 0.4278 - val_acc: 0.8296\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3596 - acc: 0.8607 - val_loss: 0.4282 - val_acc: 0.8287\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3597 - acc: 0.8608 - val_loss: 0.4268 - val_acc: 0.8288\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3584 - acc: 0.8613 - val_loss: 0.4275 - val_acc: 0.8316\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3588 - acc: 0.8620 - val_loss: 0.4268 - val_acc: 0.8304\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3585 - acc: 0.8605 - val_loss: 0.4296 - val_acc: 0.8302\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3571 - acc: 0.8618 - val_loss: 0.4287 - val_acc: 0.8285\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3568 - acc: 0.8626 - val_loss: 0.4269 - val_acc: 0.8313\n",
      "auroc: 0.867739497075712\n",
      "auprc: 0.7699779025887801\n",
      "auroc: 0.8671052131819884\n",
      "auprc: 0.7690378814651391\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 1s 93us/step - loss: 0.7239 - acc: 0.6127 - val_loss: 0.6790 - val_acc: 0.6407\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6475 - acc: 0.6515 - val_loss: 0.6396 - val_acc: 0.6590\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6141 - acc: 0.6762 - val_loss: 0.6139 - val_acc: 0.6762\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5875 - acc: 0.6969 - val_loss: 0.5891 - val_acc: 0.6934\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5638 - acc: 0.7143 - val_loss: 0.5678 - val_acc: 0.7088\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5409 - acc: 0.7321 - val_loss: 0.5476 - val_acc: 0.7224\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5207 - acc: 0.7478 - val_loss: 0.5293 - val_acc: 0.7391\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5040 - acc: 0.7611 - val_loss: 0.5153 - val_acc: 0.7510\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4894 - acc: 0.7728 - val_loss: 0.5033 - val_acc: 0.7593\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4777 - acc: 0.7818 - val_loss: 0.4947 - val_acc: 0.7665\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4683 - acc: 0.7887 - val_loss: 0.4868 - val_acc: 0.7727\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4592 - acc: 0.7953 - val_loss: 0.4784 - val_acc: 0.7812\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4500 - acc: 0.8028 - val_loss: 0.4762 - val_acc: 0.7841\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4431 - acc: 0.8076 - val_loss: 0.4710 - val_acc: 0.7881\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4370 - acc: 0.8121 - val_loss: 0.4615 - val_acc: 0.7960\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4312 - acc: 0.8159 - val_loss: 0.4575 - val_acc: 0.7996\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4255 - acc: 0.8202 - val_loss: 0.4535 - val_acc: 0.8035\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4203 - acc: 0.8247 - val_loss: 0.4534 - val_acc: 0.8026\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4183 - acc: 0.8263 - val_loss: 0.4485 - val_acc: 0.8050\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4129 - acc: 0.8297 - val_loss: 0.4443 - val_acc: 0.8107\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4095 - acc: 0.8323 - val_loss: 0.4440 - val_acc: 0.8118\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4068 - acc: 0.8338 - val_loss: 0.4406 - val_acc: 0.8125\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4032 - acc: 0.8356 - val_loss: 0.4384 - val_acc: 0.8157\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.4006 - acc: 0.8371 - val_loss: 0.4390 - val_acc: 0.8140\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3981 - acc: 0.8396 - val_loss: 0.4381 - val_acc: 0.8164\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3961 - acc: 0.8408 - val_loss: 0.4343 - val_acc: 0.8171\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3942 - acc: 0.8419 - val_loss: 0.4336 - val_acc: 0.8196\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3919 - acc: 0.8422 - val_loss: 0.4323 - val_acc: 0.8211\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3903 - acc: 0.8434 - val_loss: 0.4309 - val_acc: 0.8202\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3879 - acc: 0.8439 - val_loss: 0.4307 - val_acc: 0.8219\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3868 - acc: 0.8457 - val_loss: 0.4291 - val_acc: 0.8243\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3852 - acc: 0.8461 - val_loss: 0.4279 - val_acc: 0.8248\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3832 - acc: 0.8481 - val_loss: 0.4286 - val_acc: 0.8240\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3819 - acc: 0.8493 - val_loss: 0.4272 - val_acc: 0.8256\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3806 - acc: 0.8492 - val_loss: 0.4270 - val_acc: 0.8273\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3796 - acc: 0.8506 - val_loss: 0.4263 - val_acc: 0.8284\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3785 - acc: 0.8502 - val_loss: 0.4251 - val_acc: 0.8279\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3780 - acc: 0.8509 - val_loss: 0.4249 - val_acc: 0.8301\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3763 - acc: 0.8520 - val_loss: 0.4258 - val_acc: 0.8289\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3748 - acc: 0.8525 - val_loss: 0.4243 - val_acc: 0.8301\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3738 - acc: 0.8545 - val_loss: 0.4237 - val_acc: 0.8300\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3734 - acc: 0.8528 - val_loss: 0.4267 - val_acc: 0.8284\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3725 - acc: 0.8544 - val_loss: 0.4225 - val_acc: 0.8332\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3709 - acc: 0.8552 - val_loss: 0.4231 - val_acc: 0.8327\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3712 - acc: 0.8549 - val_loss: 0.4249 - val_acc: 0.8291\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3700 - acc: 0.8560 - val_loss: 0.4242 - val_acc: 0.8310\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3695 - acc: 0.8563 - val_loss: 0.4220 - val_acc: 0.8328\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3673 - acc: 0.8577 - val_loss: 0.4239 - val_acc: 0.8303\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3674 - acc: 0.8573 - val_loss: 0.4216 - val_acc: 0.8335\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3661 - acc: 0.8580 - val_loss: 0.4237 - val_acc: 0.8321\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3653 - acc: 0.8587 - val_loss: 0.4223 - val_acc: 0.8326\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3645 - acc: 0.8595 - val_loss: 0.4239 - val_acc: 0.8324\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3644 - acc: 0.8596 - val_loss: 0.4210 - val_acc: 0.8342\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3633 - acc: 0.8593 - val_loss: 0.4234 - val_acc: 0.8324\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3631 - acc: 0.8595 - val_loss: 0.4227 - val_acc: 0.8325\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3635 - acc: 0.8599 - val_loss: 0.4257 - val_acc: 0.8316\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3619 - acc: 0.8600 - val_loss: 0.4199 - val_acc: 0.8346\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3610 - acc: 0.8619 - val_loss: 0.4234 - val_acc: 0.8333\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3617 - acc: 0.8600 - val_loss: 0.4202 - val_acc: 0.8347\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3594 - acc: 0.8618 - val_loss: 0.4204 - val_acc: 0.8356\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3593 - acc: 0.8623 - val_loss: 0.4219 - val_acc: 0.8340\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3581 - acc: 0.8628 - val_loss: 0.4198 - val_acc: 0.8349\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3579 - acc: 0.8629 - val_loss: 0.4204 - val_acc: 0.8347\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3572 - acc: 0.8632 - val_loss: 0.4205 - val_acc: 0.8344\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3567 - acc: 0.8634 - val_loss: 0.4202 - val_acc: 0.8363\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3566 - acc: 0.8623 - val_loss: 0.4208 - val_acc: 0.8352\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3555 - acc: 0.8636 - val_loss: 0.4208 - val_acc: 0.8347\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3559 - acc: 0.8635 - val_loss: 0.4225 - val_acc: 0.8349\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3558 - acc: 0.8634 - val_loss: 0.4215 - val_acc: 0.8358\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3560 - acc: 0.8634 - val_loss: 0.4227 - val_acc: 0.8347\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3553 - acc: 0.8640 - val_loss: 0.4211 - val_acc: 0.8341\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3542 - acc: 0.8643 - val_loss: 0.4208 - val_acc: 0.8351\n",
      "auroc: 0.8705606246142684\n",
      "auprc: 0.7744302393346105\n",
      "auroc: 0.8701366757104859\n",
      "auprc: 0.7736708658321126\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 1s 102us/step - loss: 0.7043 - acc: 0.6201 - val_loss: 0.6626 - val_acc: 0.6502\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6385 - acc: 0.6514 - val_loss: 0.6324 - val_acc: 0.6570\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6066 - acc: 0.6751 - val_loss: 0.6083 - val_acc: 0.6760\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5809 - acc: 0.6952 - val_loss: 0.5857 - val_acc: 0.6927\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5564 - acc: 0.7139 - val_loss: 0.5641 - val_acc: 0.7075\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5349 - acc: 0.7312 - val_loss: 0.5480 - val_acc: 0.7229\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5171 - acc: 0.7453 - val_loss: 0.5320 - val_acc: 0.7323\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5019 - acc: 0.7559 - val_loss: 0.5205 - val_acc: 0.7436\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4895 - acc: 0.7659 - val_loss: 0.5095 - val_acc: 0.7497\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4782 - acc: 0.7753 - val_loss: 0.5039 - val_acc: 0.7582\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4685 - acc: 0.7834 - val_loss: 0.4957 - val_acc: 0.7649\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4605 - acc: 0.7884 - val_loss: 0.4872 - val_acc: 0.7721\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4520 - acc: 0.7959 - val_loss: 0.4799 - val_acc: 0.7770\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4455 - acc: 0.8008 - val_loss: 0.4789 - val_acc: 0.7802\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4390 - acc: 0.8064 - val_loss: 0.4709 - val_acc: 0.7864\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4338 - acc: 0.8102 - val_loss: 0.4654 - val_acc: 0.7923\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4281 - acc: 0.8154 - val_loss: 0.4615 - val_acc: 0.7947\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4235 - acc: 0.8192 - val_loss: 0.4587 - val_acc: 0.7987\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4194 - acc: 0.8226 - val_loss: 0.4585 - val_acc: 0.7981\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4156 - acc: 0.8263 - val_loss: 0.4522 - val_acc: 0.8057\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4117 - acc: 0.8296 - val_loss: 0.4524 - val_acc: 0.8042\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4110 - acc: 0.8302 - val_loss: 0.4470 - val_acc: 0.8093\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4056 - acc: 0.8334 - val_loss: 0.4461 - val_acc: 0.8083\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4026 - acc: 0.8359 - val_loss: 0.4440 - val_acc: 0.8121\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4008 - acc: 0.8369 - val_loss: 0.4416 - val_acc: 0.8134\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3987 - acc: 0.8381 - val_loss: 0.4395 - val_acc: 0.8146\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3960 - acc: 0.8411 - val_loss: 0.4373 - val_acc: 0.8167\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3935 - acc: 0.8430 - val_loss: 0.4375 - val_acc: 0.8166\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3920 - acc: 0.8433 - val_loss: 0.4353 - val_acc: 0.8181\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3898 - acc: 0.8442 - val_loss: 0.4346 - val_acc: 0.8208\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3889 - acc: 0.8449 - val_loss: 0.4359 - val_acc: 0.8195\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3869 - acc: 0.8458 - val_loss: 0.4323 - val_acc: 0.8237\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3847 - acc: 0.8472 - val_loss: 0.4319 - val_acc: 0.8234\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3837 - acc: 0.8483 - val_loss: 0.4319 - val_acc: 0.8228\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3824 - acc: 0.8494 - val_loss: 0.4310 - val_acc: 0.8240\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3806 - acc: 0.8493 - val_loss: 0.4327 - val_acc: 0.8239\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3825 - acc: 0.8498 - val_loss: 0.4391 - val_acc: 0.8188\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3797 - acc: 0.8510 - val_loss: 0.4296 - val_acc: 0.8264\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3773 - acc: 0.8521 - val_loss: 0.4294 - val_acc: 0.8260\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3771 - acc: 0.8521 - val_loss: 0.4291 - val_acc: 0.8264\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3755 - acc: 0.8528 - val_loss: 0.4287 - val_acc: 0.8271\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3747 - acc: 0.8539 - val_loss: 0.4317 - val_acc: 0.8246\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3756 - acc: 0.8538 - val_loss: 0.4316 - val_acc: 0.8262\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3732 - acc: 0.8545 - val_loss: 0.4275 - val_acc: 0.8293\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3717 - acc: 0.8554 - val_loss: 0.4277 - val_acc: 0.8304\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3723 - acc: 0.8551 - val_loss: 0.4318 - val_acc: 0.8255\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3713 - acc: 0.8558 - val_loss: 0.4265 - val_acc: 0.8296\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3692 - acc: 0.8563 - val_loss: 0.4320 - val_acc: 0.8249\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3695 - acc: 0.8563 - val_loss: 0.4271 - val_acc: 0.8301\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3677 - acc: 0.8578 - val_loss: 0.4260 - val_acc: 0.8297\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3670 - acc: 0.8581 - val_loss: 0.4256 - val_acc: 0.8303\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3668 - acc: 0.8585 - val_loss: 0.4274 - val_acc: 0.8307\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3669 - acc: 0.8577 - val_loss: 0.4253 - val_acc: 0.8314\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3653 - acc: 0.8593 - val_loss: 0.4281 - val_acc: 0.8292\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3647 - acc: 0.8593 - val_loss: 0.4260 - val_acc: 0.8307\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3645 - acc: 0.8590 - val_loss: 0.4261 - val_acc: 0.8322\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3635 - acc: 0.8617 - val_loss: 0.4258 - val_acc: 0.8311\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3621 - acc: 0.8609 - val_loss: 0.4266 - val_acc: 0.8324\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3626 - acc: 0.8607 - val_loss: 0.4264 - val_acc: 0.8326\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3619 - acc: 0.8611 - val_loss: 0.4253 - val_acc: 0.8326\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3620 - acc: 0.8607 - val_loss: 0.4287 - val_acc: 0.8299\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3610 - acc: 0.8619 - val_loss: 0.4251 - val_acc: 0.8326\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3601 - acc: 0.8614 - val_loss: 0.4259 - val_acc: 0.8323\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3588 - acc: 0.8628 - val_loss: 0.4244 - val_acc: 0.8333\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3587 - acc: 0.8630 - val_loss: 0.4271 - val_acc: 0.8338\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3582 - acc: 0.8630 - val_loss: 0.4247 - val_acc: 0.8327\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3573 - acc: 0.8631 - val_loss: 0.4250 - val_acc: 0.8343\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3572 - acc: 0.8632 - val_loss: 0.4246 - val_acc: 0.8344\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3561 - acc: 0.8643 - val_loss: 0.4244 - val_acc: 0.8335\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3560 - acc: 0.8645 - val_loss: 0.4253 - val_acc: 0.8346\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3569 - acc: 0.8635 - val_loss: 0.4273 - val_acc: 0.8325\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3558 - acc: 0.8641 - val_loss: 0.4277 - val_acc: 0.8311\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3549 - acc: 0.8651 - val_loss: 0.4263 - val_acc: 0.8319\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3543 - acc: 0.8658 - val_loss: 0.4250 - val_acc: 0.8350\n",
      "auroc: 0.8686234984394021\n",
      "auprc: 0.7714663931765733\n",
      "auroc: 0.868189143211664\n",
      "auprc: 0.7709547107540847\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 1s 113us/step - loss: 0.7175 - acc: 0.6144 - val_loss: 0.6672 - val_acc: 0.6256\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6476 - acc: 0.6436 - val_loss: 0.6423 - val_acc: 0.6505\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6161 - acc: 0.6711 - val_loss: 0.6185 - val_acc: 0.6711\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5890 - acc: 0.6926 - val_loss: 0.5947 - val_acc: 0.6904\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5649 - acc: 0.7094 - val_loss: 0.5761 - val_acc: 0.7046\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5437 - acc: 0.7246 - val_loss: 0.5558 - val_acc: 0.7150\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5267 - acc: 0.7367 - val_loss: 0.5413 - val_acc: 0.7231\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5113 - acc: 0.7473 - val_loss: 0.5299 - val_acc: 0.7341\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4988 - acc: 0.7583 - val_loss: 0.5210 - val_acc: 0.7400\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4898 - acc: 0.7658 - val_loss: 0.5118 - val_acc: 0.7494\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4801 - acc: 0.7730 - val_loss: 0.5036 - val_acc: 0.7569\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4719 - acc: 0.7804 - val_loss: 0.4966 - val_acc: 0.7613\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4651 - acc: 0.7853 - val_loss: 0.4912 - val_acc: 0.7688\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4569 - acc: 0.7926 - val_loss: 0.4851 - val_acc: 0.7726\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.4503 - acc: 0.7985 - val_loss: 0.4797 - val_acc: 0.7793\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4443 - acc: 0.8050 - val_loss: 0.4756 - val_acc: 0.7845\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4383 - acc: 0.8084 - val_loss: 0.4699 - val_acc: 0.7894\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4330 - acc: 0.8131 - val_loss: 0.4673 - val_acc: 0.7916\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4283 - acc: 0.8158 - val_loss: 0.4631 - val_acc: 0.7948\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4237 - acc: 0.8203 - val_loss: 0.4577 - val_acc: 0.7994\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4203 - acc: 0.8214 - val_loss: 0.4549 - val_acc: 0.8027\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4161 - acc: 0.8253 - val_loss: 0.4532 - val_acc: 0.8026\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4124 - acc: 0.8283 - val_loss: 0.4507 - val_acc: 0.8062\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4088 - acc: 0.8306 - val_loss: 0.4499 - val_acc: 0.8063\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4059 - acc: 0.8325 - val_loss: 0.4442 - val_acc: 0.8096\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4040 - acc: 0.8332 - val_loss: 0.4424 - val_acc: 0.8127\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4014 - acc: 0.8352 - val_loss: 0.4428 - val_acc: 0.8128\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3984 - acc: 0.8372 - val_loss: 0.4399 - val_acc: 0.8146\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3971 - acc: 0.8378 - val_loss: 0.4393 - val_acc: 0.8140\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3945 - acc: 0.8403 - val_loss: 0.4379 - val_acc: 0.8145\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3921 - acc: 0.8409 - val_loss: 0.4352 - val_acc: 0.8187\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3908 - acc: 0.8421 - val_loss: 0.4342 - val_acc: 0.8197\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3899 - acc: 0.8429 - val_loss: 0.4361 - val_acc: 0.8194\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3879 - acc: 0.8443 - val_loss: 0.4347 - val_acc: 0.8191\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3879 - acc: 0.8426 - val_loss: 0.4327 - val_acc: 0.8227\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3850 - acc: 0.8451 - val_loss: 0.4307 - val_acc: 0.8232\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3839 - acc: 0.8469 - val_loss: 0.4312 - val_acc: 0.8217\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3834 - acc: 0.8459 - val_loss: 0.4322 - val_acc: 0.8241\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3807 - acc: 0.8492 - val_loss: 0.4288 - val_acc: 0.8245\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3796 - acc: 0.8495 - val_loss: 0.4282 - val_acc: 0.8263\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3794 - acc: 0.8503 - val_loss: 0.4281 - val_acc: 0.8246\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3766 - acc: 0.8528 - val_loss: 0.4293 - val_acc: 0.8232\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3762 - acc: 0.8517 - val_loss: 0.4279 - val_acc: 0.8267\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3752 - acc: 0.8526 - val_loss: 0.4273 - val_acc: 0.8254\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3748 - acc: 0.8524 - val_loss: 0.4273 - val_acc: 0.8269\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3736 - acc: 0.8538 - val_loss: 0.4267 - val_acc: 0.8273\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3723 - acc: 0.8544 - val_loss: 0.4257 - val_acc: 0.8283\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3718 - acc: 0.8539 - val_loss: 0.4309 - val_acc: 0.8248\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3719 - acc: 0.8544 - val_loss: 0.4276 - val_acc: 0.8293\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3704 - acc: 0.8559 - val_loss: 0.4258 - val_acc: 0.8290\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3698 - acc: 0.8548 - val_loss: 0.4259 - val_acc: 0.8288\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3677 - acc: 0.8566 - val_loss: 0.4268 - val_acc: 0.8291\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3677 - acc: 0.8566 - val_loss: 0.4244 - val_acc: 0.8299\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3679 - acc: 0.8555 - val_loss: 0.4244 - val_acc: 0.8301\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3666 - acc: 0.8564 - val_loss: 0.4240 - val_acc: 0.8294\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3661 - acc: 0.8579 - val_loss: 0.4248 - val_acc: 0.8310\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3658 - acc: 0.8567 - val_loss: 0.4241 - val_acc: 0.8298\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3646 - acc: 0.8584 - val_loss: 0.4277 - val_acc: 0.8276\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3641 - acc: 0.8585 - val_loss: 0.4236 - val_acc: 0.8300\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3625 - acc: 0.8590 - val_loss: 0.4237 - val_acc: 0.8319\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3620 - acc: 0.8603 - val_loss: 0.4238 - val_acc: 0.8320\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3603 - acc: 0.8603 - val_loss: 0.4250 - val_acc: 0.8300\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3599 - acc: 0.8611 - val_loss: 0.4241 - val_acc: 0.8320\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3599 - acc: 0.8613 - val_loss: 0.4235 - val_acc: 0.8321\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3586 - acc: 0.8611 - val_loss: 0.4244 - val_acc: 0.8314\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3586 - acc: 0.8606 - val_loss: 0.4240 - val_acc: 0.8316\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3583 - acc: 0.8616 - val_loss: 0.4237 - val_acc: 0.8324\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3568 - acc: 0.8627 - val_loss: 0.4244 - val_acc: 0.8314\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3563 - acc: 0.8618 - val_loss: 0.4235 - val_acc: 0.8329\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3563 - acc: 0.8622 - val_loss: 0.4254 - val_acc: 0.8316\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3559 - acc: 0.8634 - val_loss: 0.4264 - val_acc: 0.8313\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3547 - acc: 0.8638 - val_loss: 0.4230 - val_acc: 0.8332\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3555 - acc: 0.8639 - val_loss: 0.4244 - val_acc: 0.8313\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3534 - acc: 0.8641 - val_loss: 0.4236 - val_acc: 0.8338\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3527 - acc: 0.8650 - val_loss: 0.4255 - val_acc: 0.8330\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3538 - acc: 0.8635 - val_loss: 0.4245 - val_acc: 0.8337\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3522 - acc: 0.8643 - val_loss: 0.4268 - val_acc: 0.8310\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3521 - acc: 0.8634 - val_loss: 0.4294 - val_acc: 0.8323\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3521 - acc: 0.8652 - val_loss: 0.4244 - val_acc: 0.8338\n",
      "Epoch 80/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3514 - acc: 0.8656 - val_loss: 0.4250 - val_acc: 0.8319\n",
      "Epoch 81/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3495 - acc: 0.8675 - val_loss: 0.4241 - val_acc: 0.8340\n",
      "Epoch 82/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3516 - acc: 0.8643 - val_loss: 0.4240 - val_acc: 0.8329\n",
      "auroc: 0.869520486742343\n",
      "auprc: 0.7706251980683074\n",
      "auroc: 0.8691575400961944\n",
      "auprc: 0.77059951337447\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 1s 120us/step - loss: 1.0175 - acc: 0.5528 - val_loss: 0.6989 - val_acc: 0.6374\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6752 - acc: 0.6525 - val_loss: 0.6520 - val_acc: 0.6345\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6298 - acc: 0.6566 - val_loss: 0.6271 - val_acc: 0.6614\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6026 - acc: 0.6812 - val_loss: 0.6042 - val_acc: 0.6790\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5754 - acc: 0.7018 - val_loss: 0.5786 - val_acc: 0.7003\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5489 - acc: 0.7234 - val_loss: 0.5543 - val_acc: 0.7171\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5259 - acc: 0.7412 - val_loss: 0.5363 - val_acc: 0.7327\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5094 - acc: 0.7537 - val_loss: 0.5231 - val_acc: 0.7440\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4965 - acc: 0.7651 - val_loss: 0.5131 - val_acc: 0.7467\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4865 - acc: 0.7709 - val_loss: 0.5133 - val_acc: 0.7517\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4770 - acc: 0.7774 - val_loss: 0.4988 - val_acc: 0.7611\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4688 - acc: 0.7840 - val_loss: 0.4947 - val_acc: 0.7664\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4620 - acc: 0.7900 - val_loss: 0.4893 - val_acc: 0.7715\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.4563 - acc: 0.7937 - val_loss: 0.4832 - val_acc: 0.7737\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4508 - acc: 0.7984 - val_loss: 0.4794 - val_acc: 0.7773\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4463 - acc: 0.8022 - val_loss: 0.4752 - val_acc: 0.7816\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4423 - acc: 0.8068 - val_loss: 0.4718 - val_acc: 0.7824\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4375 - acc: 0.8109 - val_loss: 0.4700 - val_acc: 0.7900\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4340 - acc: 0.8135 - val_loss: 0.4662 - val_acc: 0.7906\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4305 - acc: 0.8155 - val_loss: 0.4621 - val_acc: 0.7932\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4266 - acc: 0.8190 - val_loss: 0.4595 - val_acc: 0.7977\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4229 - acc: 0.8218 - val_loss: 0.4571 - val_acc: 0.7991\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4201 - acc: 0.8242 - val_loss: 0.4561 - val_acc: 0.8010\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4166 - acc: 0.8263 - val_loss: 0.4517 - val_acc: 0.8036\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4145 - acc: 0.8289 - val_loss: 0.4495 - val_acc: 0.8079\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4123 - acc: 0.8307 - val_loss: 0.4518 - val_acc: 0.8025\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.4103 - acc: 0.8311 - val_loss: 0.4456 - val_acc: 0.8099\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4064 - acc: 0.8334 - val_loss: 0.4441 - val_acc: 0.8117\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4037 - acc: 0.8357 - val_loss: 0.4421 - val_acc: 0.8126\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.4019 - acc: 0.8374 - val_loss: 0.4417 - val_acc: 0.8152\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.4001 - acc: 0.8390 - val_loss: 0.4402 - val_acc: 0.8151\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3978 - acc: 0.8407 - val_loss: 0.4411 - val_acc: 0.8166\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3963 - acc: 0.8411 - val_loss: 0.4370 - val_acc: 0.8192\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3942 - acc: 0.8423 - val_loss: 0.4364 - val_acc: 0.8189\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3921 - acc: 0.8441 - val_loss: 0.4356 - val_acc: 0.8183\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3907 - acc: 0.8437 - val_loss: 0.4339 - val_acc: 0.8208\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3901 - acc: 0.8444 - val_loss: 0.4351 - val_acc: 0.8210\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3883 - acc: 0.8460 - val_loss: 0.4335 - val_acc: 0.8202\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3872 - acc: 0.8474 - val_loss: 0.4326 - val_acc: 0.8208\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3850 - acc: 0.8476 - val_loss: 0.4308 - val_acc: 0.8240\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3842 - acc: 0.8490 - val_loss: 0.4302 - val_acc: 0.8226\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3827 - acc: 0.8493 - val_loss: 0.4317 - val_acc: 0.8239\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3815 - acc: 0.8509 - val_loss: 0.4290 - val_acc: 0.8243\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3800 - acc: 0.8509 - val_loss: 0.4284 - val_acc: 0.8248\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3791 - acc: 0.8514 - val_loss: 0.4284 - val_acc: 0.8249\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3780 - acc: 0.8527 - val_loss: 0.4301 - val_acc: 0.8241\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3773 - acc: 0.8527 - val_loss: 0.4279 - val_acc: 0.8273\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3763 - acc: 0.8532 - val_loss: 0.4285 - val_acc: 0.8256\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3758 - acc: 0.8536 - val_loss: 0.4264 - val_acc: 0.8266\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3741 - acc: 0.8547 - val_loss: 0.4272 - val_acc: 0.8257\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3743 - acc: 0.8537 - val_loss: 0.4269 - val_acc: 0.8281\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3730 - acc: 0.8542 - val_loss: 0.4290 - val_acc: 0.8268\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3721 - acc: 0.8551 - val_loss: 0.4281 - val_acc: 0.8248\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3704 - acc: 0.8568 - val_loss: 0.4270 - val_acc: 0.8269\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3707 - acc: 0.8563 - val_loss: 0.4257 - val_acc: 0.8270\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3693 - acc: 0.8568 - val_loss: 0.4247 - val_acc: 0.8289\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3681 - acc: 0.8573 - val_loss: 0.4256 - val_acc: 0.8275\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3672 - acc: 0.8585 - val_loss: 0.4251 - val_acc: 0.8299\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3670 - acc: 0.8576 - val_loss: 0.4244 - val_acc: 0.8292\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3655 - acc: 0.8582 - val_loss: 0.4249 - val_acc: 0.8303\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3647 - acc: 0.8593 - val_loss: 0.4266 - val_acc: 0.8275\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3643 - acc: 0.8596 - val_loss: 0.4249 - val_acc: 0.8301\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3640 - acc: 0.8598 - val_loss: 0.4248 - val_acc: 0.8305\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3625 - acc: 0.8601 - val_loss: 0.4243 - val_acc: 0.8305\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3625 - acc: 0.8605 - val_loss: 0.4249 - val_acc: 0.8299\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3639 - acc: 0.8596 - val_loss: 0.4290 - val_acc: 0.8317\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3618 - acc: 0.8605 - val_loss: 0.4260 - val_acc: 0.8294\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3604 - acc: 0.8615 - val_loss: 0.4252 - val_acc: 0.8304\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3596 - acc: 0.8619 - val_loss: 0.4258 - val_acc: 0.8307\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3589 - acc: 0.8628 - val_loss: 0.4250 - val_acc: 0.8306\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3576 - acc: 0.8632 - val_loss: 0.4249 - val_acc: 0.8307\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3573 - acc: 0.8633 - val_loss: 0.4248 - val_acc: 0.8305\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3571 - acc: 0.8623 - val_loss: 0.4245 - val_acc: 0.8324\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3564 - acc: 0.8633 - val_loss: 0.4251 - val_acc: 0.8323\n",
      "auroc: 0.8682466648531059\n",
      "auprc: 0.7707926761418076\n",
      "auroc: 0.8679598508357413\n",
      "auprc: 0.7705648893334193\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 2s 127us/step - loss: 0.7170 - acc: 0.6074 - val_loss: 0.6758 - val_acc: 0.6123\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6528 - acc: 0.6434 - val_loss: 0.6452 - val_acc: 0.6427\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6265 - acc: 0.6638 - val_loss: 0.6264 - val_acc: 0.6587\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6050 - acc: 0.6813 - val_loss: 0.6089 - val_acc: 0.6755\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5841 - acc: 0.6940 - val_loss: 0.5902 - val_acc: 0.6909\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5633 - acc: 0.7099 - val_loss: 0.5723 - val_acc: 0.7052\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5447 - acc: 0.7233 - val_loss: 0.5547 - val_acc: 0.7173\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5280 - acc: 0.7348 - val_loss: 0.5412 - val_acc: 0.7259\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5112 - acc: 0.7477 - val_loss: 0.5290 - val_acc: 0.7370\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4977 - acc: 0.7579 - val_loss: 0.5170 - val_acc: 0.7441\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4861 - acc: 0.7669 - val_loss: 0.5072 - val_acc: 0.7525\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4752 - acc: 0.7768 - val_loss: 0.5008 - val_acc: 0.7603\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4655 - acc: 0.7851 - val_loss: 0.4905 - val_acc: 0.7676\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4567 - acc: 0.7903 - val_loss: 0.4832 - val_acc: 0.7750\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4492 - acc: 0.7968 - val_loss: 0.4797 - val_acc: 0.7793\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4421 - acc: 0.8034 - val_loss: 0.4722 - val_acc: 0.7859\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4354 - acc: 0.8081 - val_loss: 0.4676 - val_acc: 0.7903\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4299 - acc: 0.8123 - val_loss: 0.4620 - val_acc: 0.7969\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4246 - acc: 0.8162 - val_loss: 0.4620 - val_acc: 0.7948\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4203 - acc: 0.8193 - val_loss: 0.4549 - val_acc: 0.8035\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4152 - acc: 0.8249 - val_loss: 0.4507 - val_acc: 0.8070\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4117 - acc: 0.8267 - val_loss: 0.4474 - val_acc: 0.8100\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4089 - acc: 0.8293 - val_loss: 0.4513 - val_acc: 0.8076\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4048 - acc: 0.8331 - val_loss: 0.4423 - val_acc: 0.8141\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4014 - acc: 0.8350 - val_loss: 0.4397 - val_acc: 0.8164\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3993 - acc: 0.8359 - val_loss: 0.4399 - val_acc: 0.8171\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3951 - acc: 0.8395 - val_loss: 0.4359 - val_acc: 0.8198\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3931 - acc: 0.8409 - val_loss: 0.4343 - val_acc: 0.8215\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3902 - acc: 0.8427 - val_loss: 0.4335 - val_acc: 0.8219\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3886 - acc: 0.8438 - val_loss: 0.4335 - val_acc: 0.8221\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3865 - acc: 0.8457 - val_loss: 0.4303 - val_acc: 0.8252\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3845 - acc: 0.8468 - val_loss: 0.4301 - val_acc: 0.8249\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3827 - acc: 0.8481 - val_loss: 0.4295 - val_acc: 0.8255\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3816 - acc: 0.8494 - val_loss: 0.4279 - val_acc: 0.8275\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3799 - acc: 0.8507 - val_loss: 0.4277 - val_acc: 0.8277\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3791 - acc: 0.8509 - val_loss: 0.4280 - val_acc: 0.8261\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3781 - acc: 0.8516 - val_loss: 0.4269 - val_acc: 0.8272\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3768 - acc: 0.8511 - val_loss: 0.4283 - val_acc: 0.8273\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3749 - acc: 0.8515 - val_loss: 0.4260 - val_acc: 0.8285\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3740 - acc: 0.8534 - val_loss: 0.4269 - val_acc: 0.8290\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3724 - acc: 0.8541 - val_loss: 0.4244 - val_acc: 0.8301\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3723 - acc: 0.8547 - val_loss: 0.4242 - val_acc: 0.8313\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3713 - acc: 0.8542 - val_loss: 0.4245 - val_acc: 0.8322\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3693 - acc: 0.8560 - val_loss: 0.4231 - val_acc: 0.8320\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3680 - acc: 0.8568 - val_loss: 0.4229 - val_acc: 0.8323\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3670 - acc: 0.8579 - val_loss: 0.4256 - val_acc: 0.8327\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3667 - acc: 0.8575 - val_loss: 0.4223 - val_acc: 0.8326\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3658 - acc: 0.8589 - val_loss: 0.4221 - val_acc: 0.8331\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3646 - acc: 0.8601 - val_loss: 0.4216 - val_acc: 0.8334\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3643 - acc: 0.8594 - val_loss: 0.4228 - val_acc: 0.8317\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3643 - acc: 0.8595 - val_loss: 0.4216 - val_acc: 0.8341\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3627 - acc: 0.8605 - val_loss: 0.4218 - val_acc: 0.8319\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3618 - acc: 0.8609 - val_loss: 0.4229 - val_acc: 0.8324\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3611 - acc: 0.8612 - val_loss: 0.4208 - val_acc: 0.8349\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3606 - acc: 0.8621 - val_loss: 0.4214 - val_acc: 0.8331\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3604 - acc: 0.8624 - val_loss: 0.4221 - val_acc: 0.8329\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3596 - acc: 0.8618 - val_loss: 0.4214 - val_acc: 0.8334\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3600 - acc: 0.8616 - val_loss: 0.4230 - val_acc: 0.8343\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3594 - acc: 0.8616 - val_loss: 0.4230 - val_acc: 0.8350\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3580 - acc: 0.8624 - val_loss: 0.4218 - val_acc: 0.8337\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3580 - acc: 0.8620 - val_loss: 0.4218 - val_acc: 0.8348\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3576 - acc: 0.8634 - val_loss: 0.4216 - val_acc: 0.8339\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3562 - acc: 0.8635 - val_loss: 0.4223 - val_acc: 0.8334\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3566 - acc: 0.8639 - val_loss: 0.4236 - val_acc: 0.8347\n",
      "auroc: 0.8701155312127072\n",
      "auprc: 0.772648671123347\n",
      "auroc: 0.8700883343891875\n",
      "auprc: 0.7725255067136082\n"
     ]
    }
   ],
   "source": [
    "for curr_seed in all_seeds: \n",
    "    model, early_stopping_callback, auroc_callback = train_model(model_wrapper = RC_WRAPPER, \n",
    "                                                                 aug = None, \n",
    "                                                                 curr_seed = curr_seed,\n",
    "                                                                 batch_size = 500,\n",
    "                                                                 x = x_train, \n",
    "                                                                 y = y_train, \n",
    "                                                                 val_data = (x_val, y_val))\n",
    "    save_all(filepath = \"general_results_again\", model_arch = \"exact_RC_200_auROC\", curr_seed = curr_seed,\n",
    "             callback = auroc_callback, model = model, val_data = (x_val, y_val))\n",
    "    save_all(filepath = \"general_results_again\", model_arch = \"exact_RC_200_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 2s 137us/step - loss: 0.7585 - acc: 0.6032 - val_loss: 0.6778 - val_acc: 0.6562\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6382 - acc: 0.6540 - val_loss: 0.6293 - val_acc: 0.6656\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6016 - acc: 0.6853 - val_loss: 0.6016 - val_acc: 0.6871\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5724 - acc: 0.7073 - val_loss: 0.5802 - val_acc: 0.7014\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5487 - acc: 0.7256 - val_loss: 0.5600 - val_acc: 0.7177\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5287 - acc: 0.7380 - val_loss: 0.5420 - val_acc: 0.7312\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5129 - acc: 0.7494 - val_loss: 0.5295 - val_acc: 0.7393\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4990 - acc: 0.7593 - val_loss: 0.5185 - val_acc: 0.7473\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4874 - acc: 0.7714 - val_loss: 0.5097 - val_acc: 0.7533\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4772 - acc: 0.7781 - val_loss: 0.4994 - val_acc: 0.7603\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4678 - acc: 0.7864 - val_loss: 0.4900 - val_acc: 0.7712\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4590 - acc: 0.7949 - val_loss: 0.4829 - val_acc: 0.7768\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4516 - acc: 0.8008 - val_loss: 0.4763 - val_acc: 0.7818\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4459 - acc: 0.8050 - val_loss: 0.4740 - val_acc: 0.7822\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4393 - acc: 0.8105 - val_loss: 0.4660 - val_acc: 0.7889\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.4334 - acc: 0.8149 - val_loss: 0.4615 - val_acc: 0.7955\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4279 - acc: 0.8190 - val_loss: 0.4581 - val_acc: 0.7973\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4235 - acc: 0.8231 - val_loss: 0.4541 - val_acc: 0.8007\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4207 - acc: 0.8250 - val_loss: 0.4515 - val_acc: 0.8030\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4164 - acc: 0.8272 - val_loss: 0.4477 - val_acc: 0.8069\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4135 - acc: 0.8276 - val_loss: 0.4453 - val_acc: 0.8129\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4100 - acc: 0.8318 - val_loss: 0.4435 - val_acc: 0.8093\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4073 - acc: 0.8338 - val_loss: 0.4399 - val_acc: 0.8154\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4038 - acc: 0.8353 - val_loss: 0.4393 - val_acc: 0.8131\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.4014 - acc: 0.8372 - val_loss: 0.4360 - val_acc: 0.8180\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3988 - acc: 0.8392 - val_loss: 0.4342 - val_acc: 0.8201\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3962 - acc: 0.8411 - val_loss: 0.4336 - val_acc: 0.8194\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3943 - acc: 0.8414 - val_loss: 0.4310 - val_acc: 0.8219\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3923 - acc: 0.8436 - val_loss: 0.4310 - val_acc: 0.8213\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3900 - acc: 0.8452 - val_loss: 0.4290 - val_acc: 0.8229\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3888 - acc: 0.8457 - val_loss: 0.4281 - val_acc: 0.8248\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3864 - acc: 0.8471 - val_loss: 0.4269 - val_acc: 0.8266\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3852 - acc: 0.8473 - val_loss: 0.4266 - val_acc: 0.8269\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3853 - acc: 0.8473 - val_loss: 0.4276 - val_acc: 0.8263\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3821 - acc: 0.8494 - val_loss: 0.4237 - val_acc: 0.8288\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3807 - acc: 0.8496 - val_loss: 0.4231 - val_acc: 0.8309\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3793 - acc: 0.8517 - val_loss: 0.4230 - val_acc: 0.8311\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3786 - acc: 0.8508 - val_loss: 0.4241 - val_acc: 0.8306\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3763 - acc: 0.8535 - val_loss: 0.4218 - val_acc: 0.8301\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3750 - acc: 0.8542 - val_loss: 0.4224 - val_acc: 0.8298\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3738 - acc: 0.8545 - val_loss: 0.4209 - val_acc: 0.8326\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3721 - acc: 0.8561 - val_loss: 0.4204 - val_acc: 0.8312\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3713 - acc: 0.8555 - val_loss: 0.4198 - val_acc: 0.8334\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3714 - acc: 0.8563 - val_loss: 0.4219 - val_acc: 0.8313\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3703 - acc: 0.8559 - val_loss: 0.4195 - val_acc: 0.8330\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3678 - acc: 0.8582 - val_loss: 0.4187 - val_acc: 0.8349\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3674 - acc: 0.8584 - val_loss: 0.4193 - val_acc: 0.8343\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3670 - acc: 0.8572 - val_loss: 0.4208 - val_acc: 0.8328\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3671 - acc: 0.8582 - val_loss: 0.4196 - val_acc: 0.8337\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3649 - acc: 0.8599 - val_loss: 0.4216 - val_acc: 0.8319\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3640 - acc: 0.8590 - val_loss: 0.4194 - val_acc: 0.8373\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3637 - acc: 0.8598 - val_loss: 0.4189 - val_acc: 0.8363\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3627 - acc: 0.8602 - val_loss: 0.4182 - val_acc: 0.8359\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3626 - acc: 0.8599 - val_loss: 0.4196 - val_acc: 0.8339\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3609 - acc: 0.8619 - val_loss: 0.4178 - val_acc: 0.8373\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3603 - acc: 0.8619 - val_loss: 0.4183 - val_acc: 0.8361\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3594 - acc: 0.8612 - val_loss: 0.4179 - val_acc: 0.8374\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3594 - acc: 0.8622 - val_loss: 0.4185 - val_acc: 0.8352\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3585 - acc: 0.8616 - val_loss: 0.4176 - val_acc: 0.8359\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3580 - acc: 0.8627 - val_loss: 0.4179 - val_acc: 0.8369\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3570 - acc: 0.8628 - val_loss: 0.4181 - val_acc: 0.8370\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3567 - acc: 0.8629 - val_loss: 0.4180 - val_acc: 0.8369\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3558 - acc: 0.8641 - val_loss: 0.4177 - val_acc: 0.8366\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3567 - acc: 0.8639 - val_loss: 0.4185 - val_acc: 0.8378\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.3559 - acc: 0.8648 - val_loss: 0.4197 - val_acc: 0.8368\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3552 - acc: 0.8645 - val_loss: 0.4210 - val_acc: 0.8365\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3549 - acc: 0.8643 - val_loss: 0.4209 - val_acc: 0.8353\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3556 - acc: 0.8634 - val_loss: 0.4183 - val_acc: 0.8363\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3523 - acc: 0.8661 - val_loss: 0.4186 - val_acc: 0.8377\n",
      "auroc: 0.8721292065161217\n",
      "auprc: 0.7790163471385032\n",
      "auroc: 0.8717778209640924\n",
      "auprc: 0.7781365488303527\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 2s 143us/step - loss: 0.7013 - acc: 0.6202 - val_loss: 0.6786 - val_acc: 0.6363\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6541 - acc: 0.6474 - val_loss: 0.6543 - val_acc: 0.6504\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6289 - acc: 0.6606 - val_loss: 0.6357 - val_acc: 0.6617\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6055 - acc: 0.6811 - val_loss: 0.6134 - val_acc: 0.6741\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5802 - acc: 0.6988 - val_loss: 0.5880 - val_acc: 0.6971\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5543 - acc: 0.7184 - val_loss: 0.5649 - val_acc: 0.7073\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5301 - acc: 0.7401 - val_loss: 0.5447 - val_acc: 0.7297\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5105 - acc: 0.7549 - val_loss: 0.5261 - val_acc: 0.7437\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4938 - acc: 0.7679 - val_loss: 0.5121 - val_acc: 0.7553\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4806 - acc: 0.7781 - val_loss: 0.5012 - val_acc: 0.7618\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4683 - acc: 0.7873 - val_loss: 0.4908 - val_acc: 0.7699\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.4584 - acc: 0.7953 - val_loss: 0.4833 - val_acc: 0.7771\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.4501 - acc: 0.8023 - val_loss: 0.4803 - val_acc: 0.7793\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.4421 - acc: 0.8069 - val_loss: 0.4701 - val_acc: 0.7882\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4349 - acc: 0.8128 - val_loss: 0.4649 - val_acc: 0.7926\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4290 - acc: 0.8161 - val_loss: 0.4598 - val_acc: 0.7977\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4245 - acc: 0.8204 - val_loss: 0.4549 - val_acc: 0.8016\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4197 - acc: 0.8238 - val_loss: 0.4531 - val_acc: 0.8030\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4155 - acc: 0.8282 - val_loss: 0.4510 - val_acc: 0.8087\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4122 - acc: 0.8311 - val_loss: 0.4477 - val_acc: 0.8066\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.4086 - acc: 0.8327 - val_loss: 0.4433 - val_acc: 0.8133\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4061 - acc: 0.8353 - val_loss: 0.4429 - val_acc: 0.8130\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4029 - acc: 0.8361 - val_loss: 0.4415 - val_acc: 0.8143\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.4008 - acc: 0.8374 - val_loss: 0.4392 - val_acc: 0.8176\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3982 - acc: 0.8394 - val_loss: 0.4386 - val_acc: 0.8157\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3971 - acc: 0.8403 - val_loss: 0.4358 - val_acc: 0.8175\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3950 - acc: 0.8416 - val_loss: 0.4339 - val_acc: 0.8204\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3925 - acc: 0.8429 - val_loss: 0.4345 - val_acc: 0.8200\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3909 - acc: 0.8434 - val_loss: 0.4343 - val_acc: 0.8185\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3888 - acc: 0.8461 - val_loss: 0.4322 - val_acc: 0.8216\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3874 - acc: 0.8463 - val_loss: 0.4309 - val_acc: 0.8234\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3860 - acc: 0.8479 - val_loss: 0.4304 - val_acc: 0.8249\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3840 - acc: 0.8496 - val_loss: 0.4303 - val_acc: 0.8251\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3828 - acc: 0.8496 - val_loss: 0.4292 - val_acc: 0.8243\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3821 - acc: 0.8497 - val_loss: 0.4276 - val_acc: 0.8256\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3801 - acc: 0.8522 - val_loss: 0.4270 - val_acc: 0.8261\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3791 - acc: 0.8512 - val_loss: 0.4269 - val_acc: 0.8270\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3782 - acc: 0.8529 - val_loss: 0.4283 - val_acc: 0.8258\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3776 - acc: 0.8526 - val_loss: 0.4333 - val_acc: 0.8216\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3767 - acc: 0.8535 - val_loss: 0.4261 - val_acc: 0.8267\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3746 - acc: 0.8549 - val_loss: 0.4270 - val_acc: 0.8260\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3744 - acc: 0.8548 - val_loss: 0.4266 - val_acc: 0.8274\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3728 - acc: 0.8563 - val_loss: 0.4266 - val_acc: 0.8270\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3713 - acc: 0.8575 - val_loss: 0.4243 - val_acc: 0.8287\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3704 - acc: 0.8568 - val_loss: 0.4257 - val_acc: 0.8274\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3701 - acc: 0.8582 - val_loss: 0.4263 - val_acc: 0.8267\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3683 - acc: 0.8590 - val_loss: 0.4246 - val_acc: 0.8277\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3686 - acc: 0.8580 - val_loss: 0.4249 - val_acc: 0.8284\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3672 - acc: 0.8588 - val_loss: 0.4256 - val_acc: 0.8269\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3672 - acc: 0.8586 - val_loss: 0.4263 - val_acc: 0.8283\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3654 - acc: 0.8594 - val_loss: 0.4264 - val_acc: 0.8281\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3652 - acc: 0.8594 - val_loss: 0.4271 - val_acc: 0.8279\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3639 - acc: 0.8611 - val_loss: 0.4250 - val_acc: 0.8280\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3640 - acc: 0.8599 - val_loss: 0.4243 - val_acc: 0.8293\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3640 - acc: 0.8606 - val_loss: 0.4254 - val_acc: 0.8281\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3629 - acc: 0.8615 - val_loss: 0.4243 - val_acc: 0.8298\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3618 - acc: 0.8611 - val_loss: 0.4274 - val_acc: 0.8265\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3615 - acc: 0.8616 - val_loss: 0.4245 - val_acc: 0.8303\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3600 - acc: 0.8616 - val_loss: 0.4243 - val_acc: 0.8308\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3603 - acc: 0.8616 - val_loss: 0.4247 - val_acc: 0.8290\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.3592 - acc: 0.8632 - val_loss: 0.4248 - val_acc: 0.8303\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3597 - acc: 0.8625 - val_loss: 0.4273 - val_acc: 0.8302\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3586 - acc: 0.8635 - val_loss: 0.4259 - val_acc: 0.8284\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3577 - acc: 0.8633 - val_loss: 0.4246 - val_acc: 0.8302\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3568 - acc: 0.8633 - val_loss: 0.4259 - val_acc: 0.8293\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3560 - acc: 0.8648 - val_loss: 0.4266 - val_acc: 0.8294\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3568 - acc: 0.8629 - val_loss: 0.4284 - val_acc: 0.8276\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3562 - acc: 0.8640 - val_loss: 0.4281 - val_acc: 0.8289\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3553 - acc: 0.8646 - val_loss: 0.4256 - val_acc: 0.8320\n",
      "auroc: 0.8682539247934367\n",
      "auprc: 0.7707838225596984\n",
      "auroc: 0.8682155133482731\n",
      "auprc: 0.7705380772498268\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 2s 153us/step - loss: 0.7241 - acc: 0.6109 - val_loss: 0.6716 - val_acc: 0.6330\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6486 - acc: 0.6424 - val_loss: 0.6468 - val_acc: 0.6517\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6211 - acc: 0.6678 - val_loss: 0.6241 - val_acc: 0.6680\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5971 - acc: 0.6855 - val_loss: 0.6043 - val_acc: 0.6766\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5744 - acc: 0.7039 - val_loss: 0.5813 - val_acc: 0.6976\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5524 - acc: 0.7211 - val_loss: 0.5616 - val_acc: 0.7112\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5324 - acc: 0.7367 - val_loss: 0.5449 - val_acc: 0.7286\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5141 - acc: 0.7497 - val_loss: 0.5282 - val_acc: 0.7402\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4987 - acc: 0.7608 - val_loss: 0.5185 - val_acc: 0.7484\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4863 - acc: 0.7710 - val_loss: 0.5058 - val_acc: 0.7595\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4749 - acc: 0.7794 - val_loss: 0.4986 - val_acc: 0.7636\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4654 - acc: 0.7853 - val_loss: 0.4862 - val_acc: 0.7750\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4565 - acc: 0.7947 - val_loss: 0.4803 - val_acc: 0.7791\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4493 - acc: 0.7998 - val_loss: 0.4743 - val_acc: 0.7855\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4414 - acc: 0.8062 - val_loss: 0.4684 - val_acc: 0.7919\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4371 - acc: 0.8097 - val_loss: 0.4621 - val_acc: 0.7940\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4308 - acc: 0.8152 - val_loss: 0.4580 - val_acc: 0.7978\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4260 - acc: 0.8168 - val_loss: 0.4545 - val_acc: 0.8057\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4202 - acc: 0.8221 - val_loss: 0.4493 - val_acc: 0.8068\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4145 - acc: 0.8273 - val_loss: 0.4456 - val_acc: 0.8099\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4104 - acc: 0.8296 - val_loss: 0.4426 - val_acc: 0.8141\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4066 - acc: 0.8323 - val_loss: 0.4404 - val_acc: 0.8146\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.4034 - acc: 0.8354 - val_loss: 0.4376 - val_acc: 0.8179\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3996 - acc: 0.8380 - val_loss: 0.4352 - val_acc: 0.8202\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3970 - acc: 0.8379 - val_loss: 0.4349 - val_acc: 0.8207\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3942 - acc: 0.8402 - val_loss: 0.4318 - val_acc: 0.8218\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3910 - acc: 0.8428 - val_loss: 0.4308 - val_acc: 0.8246\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3895 - acc: 0.8446 - val_loss: 0.4296 - val_acc: 0.8273\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3871 - acc: 0.8451 - val_loss: 0.4271 - val_acc: 0.8274\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3861 - acc: 0.8468 - val_loss: 0.4262 - val_acc: 0.8290\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3833 - acc: 0.8471 - val_loss: 0.4287 - val_acc: 0.8283\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3832 - acc: 0.8478 - val_loss: 0.4257 - val_acc: 0.8291\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3805 - acc: 0.8495 - val_loss: 0.4249 - val_acc: 0.8324\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3795 - acc: 0.8492 - val_loss: 0.4228 - val_acc: 0.8336\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3777 - acc: 0.8513 - val_loss: 0.4228 - val_acc: 0.8334\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3759 - acc: 0.8526 - val_loss: 0.4230 - val_acc: 0.8328\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3748 - acc: 0.8538 - val_loss: 0.4213 - val_acc: 0.8339\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3733 - acc: 0.8549 - val_loss: 0.4206 - val_acc: 0.8353\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.3725 - acc: 0.8548 - val_loss: 0.4209 - val_acc: 0.8350\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-f185ca777ee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                                  \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                                  \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                                  val_data = (x_val, y_val))\n\u001b[0m\u001b[1;32m      9\u001b[0m     save_all(filepath = \"general_results_again\", model_arch = \"exact_Reg_200_auROC\", curr_seed = curr_seed,\n\u001b[1;32m     10\u001b[0m              callback = auroc_callback, model = model, val_data = (x_val, y_val))\n",
      "\u001b[0;32m~/revcomp_simulated/train_models.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_wrapper, aug, curr_seed, batch_size, x, y, val_data)\u001b[0m\n\u001b[1;32m    143\u001b[0m     model.fit(x = x_train, y = y_train, validation_data = val_data,  \n\u001b[1;32m    144\u001b[0m               \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauroc_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m               batch_size=batch_size, epochs=200)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauroc_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/revcomp_simulated/train_models.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_epoch_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mauroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mauroc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_auroc_sofar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for curr_seed in all_seeds: \n",
    "    model, early_stopping_callback, auroc_callback = train_model(model_wrapper = REG_WRAPPER, \n",
    "                                                                 aug = None, \n",
    "                                                                 curr_seed = curr_seed,\n",
    "                                                                 batch_size = 500,\n",
    "                                                                 x = x_train, \n",
    "                                                                 y = y_train, \n",
    "                                                                 val_data = (x_val, y_val))\n",
    "    save_all(filepath = \"general_results_again\", model_arch = \"exact_Reg_200_auROC\", curr_seed = curr_seed,\n",
    "             callback = auroc_callback, model = model, val_data = (x_val, y_val))\n",
    "    save_all(filepath = \"general_results_again\", model_arch = \"exact_Reg_200_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 1s 67us/step - loss: 0.7956 - acc: 0.5416 - val_loss: 0.7209 - val_acc: 0.5772\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6997 - acc: 0.5719 - val_loss: 0.6967 - val_acc: 0.5867\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6758 - acc: 0.5984 - val_loss: 0.6824 - val_acc: 0.5953\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 17us/step - loss: 0.6598 - acc: 0.6157 - val_loss: 0.6722 - val_acc: 0.6018\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6479 - acc: 0.6277 - val_loss: 0.6655 - val_acc: 0.6115\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6384 - acc: 0.6397 - val_loss: 0.6589 - val_acc: 0.6219\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 17us/step - loss: 0.6304 - acc: 0.6499 - val_loss: 0.6522 - val_acc: 0.6299\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6235 - acc: 0.6581 - val_loss: 0.6482 - val_acc: 0.6341\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6174 - acc: 0.6664 - val_loss: 0.6431 - val_acc: 0.6371\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6120 - acc: 0.6716 - val_loss: 0.6391 - val_acc: 0.6472\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 17us/step - loss: 0.6071 - acc: 0.6770 - val_loss: 0.6345 - val_acc: 0.6469\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6022 - acc: 0.6828 - val_loss: 0.6304 - val_acc: 0.6563\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5981 - acc: 0.6887 - val_loss: 0.6274 - val_acc: 0.6606\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5945 - acc: 0.6921 - val_loss: 0.6258 - val_acc: 0.6615\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5907 - acc: 0.6958 - val_loss: 0.6221 - val_acc: 0.6686\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5877 - acc: 0.7004 - val_loss: 0.6202 - val_acc: 0.6708\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5853 - acc: 0.7001 - val_loss: 0.6186 - val_acc: 0.6723\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5822 - acc: 0.7055 - val_loss: 0.6160 - val_acc: 0.6778\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5806 - acc: 0.7070 - val_loss: 0.6132 - val_acc: 0.6797\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5775 - acc: 0.7108 - val_loss: 0.6115 - val_acc: 0.6834\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5755 - acc: 0.7126 - val_loss: 0.6103 - val_acc: 0.6850\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 17us/step - loss: 0.5742 - acc: 0.7148 - val_loss: 0.6098 - val_acc: 0.6865\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5712 - acc: 0.7162 - val_loss: 0.6080 - val_acc: 0.6901\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5695 - acc: 0.7188 - val_loss: 0.6066 - val_acc: 0.6898\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5675 - acc: 0.7197 - val_loss: 0.6050 - val_acc: 0.6934\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5661 - acc: 0.7224 - val_loss: 0.6051 - val_acc: 0.6901\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5640 - acc: 0.7239 - val_loss: 0.6028 - val_acc: 0.6945\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 17us/step - loss: 0.5629 - acc: 0.7256 - val_loss: 0.6047 - val_acc: 0.6939\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5620 - acc: 0.7261 - val_loss: 0.6029 - val_acc: 0.6925\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5609 - acc: 0.7276 - val_loss: 0.6026 - val_acc: 0.6960\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5597 - acc: 0.7279 - val_loss: 0.6009 - val_acc: 0.6998\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5569 - acc: 0.7307 - val_loss: 0.6027 - val_acc: 0.6956\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5561 - acc: 0.7308 - val_loss: 0.6013 - val_acc: 0.7003\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5548 - acc: 0.7325 - val_loss: 0.5990 - val_acc: 0.7025\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5536 - acc: 0.7326 - val_loss: 0.5977 - val_acc: 0.7028\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5526 - acc: 0.7354 - val_loss: 0.5977 - val_acc: 0.7040\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5519 - acc: 0.7361 - val_loss: 0.5966 - val_acc: 0.7054\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5502 - acc: 0.7368 - val_loss: 0.5976 - val_acc: 0.7076\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5510 - acc: 0.7354 - val_loss: 0.5980 - val_acc: 0.7040\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 17us/step - loss: 0.5500 - acc: 0.7379 - val_loss: 0.5974 - val_acc: 0.7025\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5482 - acc: 0.7399 - val_loss: 0.5957 - val_acc: 0.7065\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5478 - acc: 0.7389 - val_loss: 0.5946 - val_acc: 0.7100\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5462 - acc: 0.7407 - val_loss: 0.5949 - val_acc: 0.7094\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5469 - acc: 0.7401 - val_loss: 0.5947 - val_acc: 0.7093\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5452 - acc: 0.7419 - val_loss: 0.5946 - val_acc: 0.7106\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5442 - acc: 0.7426 - val_loss: 0.5940 - val_acc: 0.7099\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5439 - acc: 0.7432 - val_loss: 0.5952 - val_acc: 0.7068\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5434 - acc: 0.7433 - val_loss: 0.5944 - val_acc: 0.7102\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 17us/step - loss: 0.5428 - acc: 0.7441 - val_loss: 0.5939 - val_acc: 0.7101\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5431 - acc: 0.7445 - val_loss: 0.5956 - val_acc: 0.7075\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5420 - acc: 0.7452 - val_loss: 0.5938 - val_acc: 0.7123\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5405 - acc: 0.7458 - val_loss: 0.5936 - val_acc: 0.7133\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5400 - acc: 0.7474 - val_loss: 0.5951 - val_acc: 0.7099\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5405 - acc: 0.7455 - val_loss: 0.5963 - val_acc: 0.7079\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 17us/step - loss: 0.5395 - acc: 0.7484 - val_loss: 0.5937 - val_acc: 0.7114\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5378 - acc: 0.7491 - val_loss: 0.5942 - val_acc: 0.7113\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5391 - acc: 0.7484 - val_loss: 0.5941 - val_acc: 0.7119\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 17us/step - loss: 0.5393 - acc: 0.7468 - val_loss: 0.5936 - val_acc: 0.7109\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5376 - acc: 0.7491 - val_loss: 0.5933 - val_acc: 0.7114\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5367 - acc: 0.7498 - val_loss: 0.5938 - val_acc: 0.7128\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5360 - acc: 0.7514 - val_loss: 0.5931 - val_acc: 0.7130\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5357 - acc: 0.7507 - val_loss: 0.5989 - val_acc: 0.7072\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 17us/step - loss: 0.5357 - acc: 0.7513 - val_loss: 0.5936 - val_acc: 0.7118\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5358 - acc: 0.7499 - val_loss: 0.5957 - val_acc: 0.7108\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5363 - acc: 0.7503 - val_loss: 0.5946 - val_acc: 0.7111\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5344 - acc: 0.7507 - val_loss: 0.5939 - val_acc: 0.7131\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5339 - acc: 0.7512 - val_loss: 0.5957 - val_acc: 0.7101\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5340 - acc: 0.7516 - val_loss: 0.5959 - val_acc: 0.7101\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5335 - acc: 0.7528 - val_loss: 0.5938 - val_acc: 0.7127\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5334 - acc: 0.7530 - val_loss: 0.5946 - val_acc: 0.7104\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5339 - acc: 0.7515 - val_loss: 0.5943 - val_acc: 0.7113\n",
      "auroc: 0.9171864859266439\n",
      "auprc: 0.8685185000482502\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 1s 72us/step - loss: 0.7497 - acc: 0.5492 - val_loss: 0.7135 - val_acc: 0.5574\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6990 - acc: 0.5786 - val_loss: 0.7036 - val_acc: 0.5642\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6831 - acc: 0.5894 - val_loss: 0.6909 - val_acc: 0.5731\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6694 - acc: 0.6028 - val_loss: 0.6826 - val_acc: 0.5847\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6587 - acc: 0.6148 - val_loss: 0.6740 - val_acc: 0.5940\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6493 - acc: 0.6256 - val_loss: 0.6667 - val_acc: 0.6080\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6390 - acc: 0.6371 - val_loss: 0.6582 - val_acc: 0.6154\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6303 - acc: 0.6479 - val_loss: 0.6509 - val_acc: 0.6263\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6218 - acc: 0.6588 - val_loss: 0.6445 - val_acc: 0.6371\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.6156 - acc: 0.6664 - val_loss: 0.6396 - val_acc: 0.6409\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6093 - acc: 0.6734 - val_loss: 0.6364 - val_acc: 0.6465\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.6038 - acc: 0.6814 - val_loss: 0.6308 - val_acc: 0.6572\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5992 - acc: 0.6871 - val_loss: 0.6293 - val_acc: 0.6635\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5954 - acc: 0.6897 - val_loss: 0.6248 - val_acc: 0.6654\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5919 - acc: 0.6941 - val_loss: 0.6218 - val_acc: 0.6670\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5885 - acc: 0.6986 - val_loss: 0.6208 - val_acc: 0.6726\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5853 - acc: 0.7029 - val_loss: 0.6170 - val_acc: 0.6726\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5824 - acc: 0.7046 - val_loss: 0.6153 - val_acc: 0.6769\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5801 - acc: 0.7089 - val_loss: 0.6117 - val_acc: 0.6807\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5774 - acc: 0.7125 - val_loss: 0.6104 - val_acc: 0.6840\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5751 - acc: 0.7141 - val_loss: 0.6099 - val_acc: 0.6865\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5736 - acc: 0.7159 - val_loss: 0.6073 - val_acc: 0.6878\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5715 - acc: 0.7198 - val_loss: 0.6096 - val_acc: 0.6874\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5693 - acc: 0.7222 - val_loss: 0.6045 - val_acc: 0.6896\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5675 - acc: 0.7219 - val_loss: 0.6044 - val_acc: 0.6913\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5656 - acc: 0.7242 - val_loss: 0.6043 - val_acc: 0.6901\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5647 - acc: 0.7263 - val_loss: 0.6016 - val_acc: 0.6957\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5632 - acc: 0.7269 - val_loss: 0.6025 - val_acc: 0.6967\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5620 - acc: 0.7296 - val_loss: 0.6005 - val_acc: 0.6974\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5608 - acc: 0.7300 - val_loss: 0.6015 - val_acc: 0.6962\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5598 - acc: 0.7303 - val_loss: 0.5992 - val_acc: 0.6999\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5587 - acc: 0.7316 - val_loss: 0.5989 - val_acc: 0.6998\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5588 - acc: 0.7309 - val_loss: 0.5982 - val_acc: 0.7015\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5565 - acc: 0.7339 - val_loss: 0.5986 - val_acc: 0.7019\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5560 - acc: 0.7356 - val_loss: 0.5997 - val_acc: 0.7010\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5565 - acc: 0.7344 - val_loss: 0.5990 - val_acc: 0.7027\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5552 - acc: 0.7350 - val_loss: 0.5994 - val_acc: 0.7036\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5540 - acc: 0.7360 - val_loss: 0.5978 - val_acc: 0.7039\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5525 - acc: 0.7376 - val_loss: 0.5975 - val_acc: 0.7046\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5519 - acc: 0.7381 - val_loss: 0.5960 - val_acc: 0.7073\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5509 - acc: 0.7400 - val_loss: 0.5980 - val_acc: 0.7042\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5505 - acc: 0.7396 - val_loss: 0.5963 - val_acc: 0.7065\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5498 - acc: 0.7402 - val_loss: 0.5962 - val_acc: 0.7083\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5495 - acc: 0.7414 - val_loss: 0.5964 - val_acc: 0.7070\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5487 - acc: 0.7414 - val_loss: 0.5976 - val_acc: 0.7053\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5479 - acc: 0.7435 - val_loss: 0.5957 - val_acc: 0.7100\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5468 - acc: 0.7422 - val_loss: 0.5952 - val_acc: 0.7091\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5464 - acc: 0.7443 - val_loss: 0.5953 - val_acc: 0.7110\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5470 - acc: 0.7429 - val_loss: 0.5956 - val_acc: 0.7096\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5460 - acc: 0.7429 - val_loss: 0.5949 - val_acc: 0.7096\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 19us/step - loss: 0.5451 - acc: 0.7444 - val_loss: 0.5972 - val_acc: 0.7107\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5444 - acc: 0.7450 - val_loss: 0.5954 - val_acc: 0.7100\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5437 - acc: 0.7446 - val_loss: 0.5948 - val_acc: 0.7096\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5430 - acc: 0.7461 - val_loss: 0.5940 - val_acc: 0.7130\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5427 - acc: 0.7461 - val_loss: 0.5946 - val_acc: 0.7122\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 18us/step - loss: 0.5424 - acc: 0.7472 - val_loss: 0.5952 - val_acc: 0.7115\n",
      "Epoch 57/200\n",
      " 4500/12000 [==========>...................] - ETA: 0s - loss: 0.5309 - acc: 0.7536"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-5a774678796b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                  \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                                  \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                                                  y = y_train_mutate)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#     save_all(filepath = \"general_results_again\", model_arch = \"split_RC_200_auROC\", curr_seed = curr_seed,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#              callback = auroc_callback, model = model, val_data = (x_test, y_test))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-404468146470>\u001b[0m in \u001b[0;36mtrain_model_other\u001b[0;34m(model_wrapper, aug, curr_seed, batch_size, x, y)\u001b[0m\n\u001b[1;32m     45\u001b[0m     model.fit(x = x_train, y = y_train, validation_split = 3/7,  \n\u001b[1;32m     46\u001b[0m               \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m               batch_size=batch_size, epochs=200)\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for curr_seed in all_seeds: \n",
    "    model, early_stopping_callback = train_model_other(model_wrapper = RC_WRAPPER, \n",
    "                                                                 aug = None, \n",
    "                                                                 curr_seed = curr_seed,\n",
    "                                                                 batch_size = 500,\n",
    "                                                                 x = x_train, \n",
    "                                                                 y = y_train_mutate)\n",
    "#     save_all(filepath = \"general_results_again\", model_arch = \"split_RC_200_auROC\", curr_seed = curr_seed,\n",
    "#              callback = auroc_callback, model = model, val_data = (x_test, y_test))\n",
    "    save_all(filepath = \"general_results_again\", model_arch = \"split_RC_200_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_seed in all_seeds: \n",
    "    model, early_stopping_callback, auroc_callback = train_model(model_wrapper = RC_WRAPPER, \n",
    "                                                                 aug = None, \n",
    "                                                                 curr_seed = curr_seed,\n",
    "                                                                 batch_size = 500,\n",
    "                                                                 x = x_train, \n",
    "                                                                 y = y_train_mutate, \n",
    "                                                                 val_data = (x_val_200, y_val_200))\n",
    "    save_all(filepath = \"general_results_again\", model_arch = \"test_RC_200_auROC\", curr_seed = curr_seed,\n",
    "             callback = auroc_callback, model = model, val_data = (x_test_200, y_test_200))\n",
    "    save_all(filepath = \"general_results_again\", model_arch = \"test_RC_200_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_test_200, y_test_200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 5s 381us/step - loss: 0.7555 - acc: 0.5449 - val_loss: 0.6699 - val_acc: 0.6184\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.7017 - acc: 0.5734 - val_loss: 0.6651 - val_acc: 0.6037\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6835 - acc: 0.5868 - val_loss: 0.6440 - val_acc: 0.6293\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6682 - acc: 0.6019 - val_loss: 0.6268 - val_acc: 0.6464\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6550 - acc: 0.6160 - val_loss: 0.6095 - val_acc: 0.6648\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6426 - acc: 0.6311 - val_loss: 0.5955 - val_acc: 0.6788\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6318 - acc: 0.6437 - val_loss: 0.5704 - val_acc: 0.7074\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6220 - acc: 0.6530 - val_loss: 0.5455 - val_acc: 0.7266\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6142 - acc: 0.6627 - val_loss: 0.5350 - val_acc: 0.7375\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6075 - acc: 0.6695 - val_loss: 0.5243 - val_acc: 0.7480\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6012 - acc: 0.6780 - val_loss: 0.5162 - val_acc: 0.7530\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5968 - acc: 0.6847 - val_loss: 0.5056 - val_acc: 0.7651\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5920 - acc: 0.6912 - val_loss: 0.4997 - val_acc: 0.7729\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5883 - acc: 0.6949 - val_loss: 0.4925 - val_acc: 0.7783\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5848 - acc: 0.6981 - val_loss: 0.4882 - val_acc: 0.7827\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5810 - acc: 0.7038 - val_loss: 0.4855 - val_acc: 0.7882\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5789 - acc: 0.7071 - val_loss: 0.4744 - val_acc: 0.7961\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5752 - acc: 0.7109 - val_loss: 0.4728 - val_acc: 0.8003\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5734 - acc: 0.7150 - val_loss: 0.4605 - val_acc: 0.8066\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5708 - acc: 0.7154 - val_loss: 0.4625 - val_acc: 0.8099\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5682 - acc: 0.7196 - val_loss: 0.4569 - val_acc: 0.8135\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5668 - acc: 0.7207 - val_loss: 0.4588 - val_acc: 0.8148\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5646 - acc: 0.7234 - val_loss: 0.4533 - val_acc: 0.8170\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5633 - acc: 0.7248 - val_loss: 0.4499 - val_acc: 0.8227\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5615 - acc: 0.7274 - val_loss: 0.4443 - val_acc: 0.8260\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5603 - acc: 0.7295 - val_loss: 0.4387 - val_acc: 0.8289\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5585 - acc: 0.7300 - val_loss: 0.4441 - val_acc: 0.8275\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5579 - acc: 0.7313 - val_loss: 0.4400 - val_acc: 0.8310\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5570 - acc: 0.7319 - val_loss: 0.4351 - val_acc: 0.8327\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5550 - acc: 0.7337 - val_loss: 0.4401 - val_acc: 0.8315\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5535 - acc: 0.7360 - val_loss: 0.4274 - val_acc: 0.8412\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5518 - acc: 0.7372 - val_loss: 0.4230 - val_acc: 0.8444\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5510 - acc: 0.7380 - val_loss: 0.4321 - val_acc: 0.8384\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5501 - acc: 0.7383 - val_loss: 0.4224 - val_acc: 0.8457\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5506 - acc: 0.7383 - val_loss: 0.4183 - val_acc: 0.8485\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5492 - acc: 0.7390 - val_loss: 0.4156 - val_acc: 0.8497\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5474 - acc: 0.7409 - val_loss: 0.4191 - val_acc: 0.8463\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5475 - acc: 0.7412 - val_loss: 0.4133 - val_acc: 0.8521\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5472 - acc: 0.7400 - val_loss: 0.4241 - val_acc: 0.8459\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5455 - acc: 0.7417 - val_loss: 0.4143 - val_acc: 0.8508\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5446 - acc: 0.7440 - val_loss: 0.4142 - val_acc: 0.8527\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5436 - acc: 0.7442 - val_loss: 0.4115 - val_acc: 0.8527\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5436 - acc: 0.7440 - val_loss: 0.4099 - val_acc: 0.8502\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5431 - acc: 0.7452 - val_loss: 0.4130 - val_acc: 0.8514\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5426 - acc: 0.7466 - val_loss: 0.4169 - val_acc: 0.8496\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5416 - acc: 0.7459 - val_loss: 0.4060 - val_acc: 0.8553\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5417 - acc: 0.7458 - val_loss: 0.4068 - val_acc: 0.8559\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5411 - acc: 0.7471 - val_loss: 0.4055 - val_acc: 0.8547\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5399 - acc: 0.7484 - val_loss: 0.4093 - val_acc: 0.8527\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5392 - acc: 0.7473 - val_loss: 0.4119 - val_acc: 0.8507\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5391 - acc: 0.7486 - val_loss: 0.4047 - val_acc: 0.8555\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5400 - acc: 0.7485 - val_loss: 0.4007 - val_acc: 0.8574\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5387 - acc: 0.7491 - val_loss: 0.3999 - val_acc: 0.8588\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5385 - acc: 0.7490 - val_loss: 0.4087 - val_acc: 0.8511\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5382 - acc: 0.7496 - val_loss: 0.3999 - val_acc: 0.8576\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5374 - acc: 0.7498 - val_loss: 0.4061 - val_acc: 0.8533\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5374 - acc: 0.7500 - val_loss: 0.4075 - val_acc: 0.8522\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5367 - acc: 0.7503 - val_loss: 0.3989 - val_acc: 0.8589\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5365 - acc: 0.7511 - val_loss: 0.4050 - val_acc: 0.8537\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5369 - acc: 0.7486 - val_loss: 0.3995 - val_acc: 0.8577\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5362 - acc: 0.7499 - val_loss: 0.3950 - val_acc: 0.8562\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5360 - acc: 0.7514 - val_loss: 0.4046 - val_acc: 0.8568\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5357 - acc: 0.7499 - val_loss: 0.3973 - val_acc: 0.8584\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5355 - acc: 0.7512 - val_loss: 0.4030 - val_acc: 0.8552\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5361 - acc: 0.7499 - val_loss: 0.4025 - val_acc: 0.8572\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5355 - acc: 0.7493 - val_loss: 0.4035 - val_acc: 0.8559\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5340 - acc: 0.7504 - val_loss: 0.3965 - val_acc: 0.8596\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5339 - acc: 0.7500 - val_loss: 0.3985 - val_acc: 0.8578\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5332 - acc: 0.7513 - val_loss: 0.3950 - val_acc: 0.8587\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5334 - acc: 0.7509 - val_loss: 0.3991 - val_acc: 0.8571\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5332 - acc: 0.7508 - val_loss: 0.4031 - val_acc: 0.8564\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5331 - acc: 0.7511 - val_loss: 0.4060 - val_acc: 0.8530\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5324 - acc: 0.7519 - val_loss: 0.3976 - val_acc: 0.8589\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5322 - acc: 0.7535 - val_loss: 0.4011 - val_acc: 0.8534\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5324 - acc: 0.7524 - val_loss: 0.4008 - val_acc: 0.8561\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5321 - acc: 0.7524 - val_loss: 0.4011 - val_acc: 0.8567\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5315 - acc: 0.7525 - val_loss: 0.3957 - val_acc: 0.8581\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5318 - acc: 0.7519 - val_loss: 0.3958 - val_acc: 0.8566\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5313 - acc: 0.7523 - val_loss: 0.4014 - val_acc: 0.8569\n",
      "auroc: 0.9232922596414567\n",
      "auprc: 0.8759376549873809\n",
      "auroc: 0.9228565561491031\n",
      "auprc: 0.8753968373299347\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 5s 394us/step - loss: 0.7652 - acc: 0.5498 - val_loss: 0.6777 - val_acc: 0.6026\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6992 - acc: 0.5710 - val_loss: 0.6547 - val_acc: 0.6226\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6779 - acc: 0.5940 - val_loss: 0.6352 - val_acc: 0.6395\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6629 - acc: 0.6094 - val_loss: 0.6222 - val_acc: 0.6506\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6514 - acc: 0.6239 - val_loss: 0.6053 - val_acc: 0.6695\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6411 - acc: 0.6357 - val_loss: 0.5921 - val_acc: 0.6850\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6327 - acc: 0.6470 - val_loss: 0.5766 - val_acc: 0.7011\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6256 - acc: 0.6545 - val_loss: 0.5614 - val_acc: 0.7148\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6192 - acc: 0.6624 - val_loss: 0.5572 - val_acc: 0.7208\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6135 - acc: 0.6689 - val_loss: 0.5397 - val_acc: 0.7370\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6076 - acc: 0.6750 - val_loss: 0.5319 - val_acc: 0.7428\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6028 - acc: 0.6814 - val_loss: 0.5219 - val_acc: 0.7523\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5986 - acc: 0.6857 - val_loss: 0.5193 - val_acc: 0.7557\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5938 - acc: 0.6911 - val_loss: 0.5055 - val_acc: 0.7699\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5903 - acc: 0.6955 - val_loss: 0.4977 - val_acc: 0.7763\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5866 - acc: 0.6983 - val_loss: 0.5007 - val_acc: 0.7775\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5833 - acc: 0.7030 - val_loss: 0.4845 - val_acc: 0.7892\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5803 - acc: 0.7059 - val_loss: 0.4822 - val_acc: 0.7909\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5771 - acc: 0.7092 - val_loss: 0.4753 - val_acc: 0.8008\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5758 - acc: 0.7103 - val_loss: 0.4695 - val_acc: 0.8053\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5720 - acc: 0.7144 - val_loss: 0.4703 - val_acc: 0.8053\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5705 - acc: 0.7165 - val_loss: 0.4569 - val_acc: 0.8107\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5688 - acc: 0.7174 - val_loss: 0.4537 - val_acc: 0.8178\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5667 - acc: 0.7203 - val_loss: 0.4531 - val_acc: 0.8219\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5652 - acc: 0.7223 - val_loss: 0.4536 - val_acc: 0.8177\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5638 - acc: 0.7223 - val_loss: 0.4470 - val_acc: 0.8245\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5617 - acc: 0.7254 - val_loss: 0.4428 - val_acc: 0.8273\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5604 - acc: 0.7280 - val_loss: 0.4445 - val_acc: 0.8256\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5593 - acc: 0.7297 - val_loss: 0.4453 - val_acc: 0.8286\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5581 - acc: 0.7294 - val_loss: 0.4390 - val_acc: 0.8330\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5568 - acc: 0.7320 - val_loss: 0.4357 - val_acc: 0.8335\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5564 - acc: 0.7302 - val_loss: 0.4308 - val_acc: 0.8338\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5551 - acc: 0.7327 - val_loss: 0.4344 - val_acc: 0.8343\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5540 - acc: 0.7339 - val_loss: 0.4323 - val_acc: 0.8374\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5527 - acc: 0.7361 - val_loss: 0.4250 - val_acc: 0.8379\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5523 - acc: 0.7348 - val_loss: 0.4381 - val_acc: 0.8339\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5524 - acc: 0.7355 - val_loss: 0.4225 - val_acc: 0.8408\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5504 - acc: 0.7366 - val_loss: 0.4269 - val_acc: 0.8412\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5501 - acc: 0.7369 - val_loss: 0.4174 - val_acc: 0.8429\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5491 - acc: 0.7390 - val_loss: 0.4232 - val_acc: 0.8430\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5500 - acc: 0.7388 - val_loss: 0.4327 - val_acc: 0.8356\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5487 - acc: 0.7390 - val_loss: 0.4254 - val_acc: 0.8401\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5474 - acc: 0.7396 - val_loss: 0.4245 - val_acc: 0.8423\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5473 - acc: 0.7390 - val_loss: 0.4190 - val_acc: 0.8467\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5461 - acc: 0.7431 - val_loss: 0.4170 - val_acc: 0.8479\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5455 - acc: 0.7434 - val_loss: 0.4180 - val_acc: 0.8468\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5467 - acc: 0.7401 - val_loss: 0.4156 - val_acc: 0.8466\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5460 - acc: 0.7418 - val_loss: 0.4113 - val_acc: 0.8455\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5436 - acc: 0.7440 - val_loss: 0.4170 - val_acc: 0.8474\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5430 - acc: 0.7436 - val_loss: 0.4163 - val_acc: 0.8475\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5429 - acc: 0.7436 - val_loss: 0.4200 - val_acc: 0.8468\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5426 - acc: 0.7445 - val_loss: 0.4143 - val_acc: 0.8468\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5422 - acc: 0.7451 - val_loss: 0.4133 - val_acc: 0.8471\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5413 - acc: 0.7456 - val_loss: 0.4154 - val_acc: 0.8480\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5411 - acc: 0.7452 - val_loss: 0.4145 - val_acc: 0.8479\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5414 - acc: 0.7444 - val_loss: 0.4155 - val_acc: 0.8504\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5405 - acc: 0.7458 - val_loss: 0.4118 - val_acc: 0.8489\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5401 - acc: 0.7450 - val_loss: 0.4218 - val_acc: 0.8427\n",
      "auroc: 0.9128410572556579\n",
      "auprc: 0.8608947821627729\n",
      "auroc: 0.9104705476492475\n",
      "auprc: 0.8572321405304387\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 5s 412us/step - loss: 0.8140 - acc: 0.5426 - val_loss: 0.6904 - val_acc: 0.5969\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6993 - acc: 0.5723 - val_loss: 0.6523 - val_acc: 0.6236\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6780 - acc: 0.5969 - val_loss: 0.6341 - val_acc: 0.6417\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6633 - acc: 0.6140 - val_loss: 0.6245 - val_acc: 0.6474\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6513 - acc: 0.6251 - val_loss: 0.5972 - val_acc: 0.6802\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6401 - acc: 0.6366 - val_loss: 0.5788 - val_acc: 0.6981\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6298 - acc: 0.6521 - val_loss: 0.5672 - val_acc: 0.7101\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6215 - acc: 0.6620 - val_loss: 0.5511 - val_acc: 0.7261\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6145 - acc: 0.6698 - val_loss: 0.5352 - val_acc: 0.7444\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6077 - acc: 0.6771 - val_loss: 0.5220 - val_acc: 0.7557\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6019 - acc: 0.6832 - val_loss: 0.5176 - val_acc: 0.7611\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5971 - acc: 0.6864 - val_loss: 0.5056 - val_acc: 0.7712\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5920 - acc: 0.6933 - val_loss: 0.4937 - val_acc: 0.7807\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5879 - acc: 0.6967 - val_loss: 0.4891 - val_acc: 0.7870\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5846 - acc: 0.7008 - val_loss: 0.4761 - val_acc: 0.7933\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5821 - acc: 0.7039 - val_loss: 0.4789 - val_acc: 0.7975\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5788 - acc: 0.7080 - val_loss: 0.4672 - val_acc: 0.8051\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5763 - acc: 0.7104 - val_loss: 0.4628 - val_acc: 0.8091\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5739 - acc: 0.7126 - val_loss: 0.4655 - val_acc: 0.8101\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5720 - acc: 0.7146 - val_loss: 0.4603 - val_acc: 0.8129\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5700 - acc: 0.7173 - val_loss: 0.4578 - val_acc: 0.8164\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5680 - acc: 0.7189 - val_loss: 0.4540 - val_acc: 0.8185\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5661 - acc: 0.7207 - val_loss: 0.4551 - val_acc: 0.8209\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5637 - acc: 0.7243 - val_loss: 0.4510 - val_acc: 0.8228\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5628 - acc: 0.7245 - val_loss: 0.4466 - val_acc: 0.8267\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5614 - acc: 0.7263 - val_loss: 0.4409 - val_acc: 0.8304\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5592 - acc: 0.7282 - val_loss: 0.4441 - val_acc: 0.8300\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5580 - acc: 0.7303 - val_loss: 0.4393 - val_acc: 0.8339\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5562 - acc: 0.7314 - val_loss: 0.4337 - val_acc: 0.8366\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5558 - acc: 0.7325 - val_loss: 0.4281 - val_acc: 0.8393\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5543 - acc: 0.7327 - val_loss: 0.4372 - val_acc: 0.8376\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5532 - acc: 0.7342 - val_loss: 0.4254 - val_acc: 0.8396\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5521 - acc: 0.7363 - val_loss: 0.4248 - val_acc: 0.8424\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5510 - acc: 0.7379 - val_loss: 0.4274 - val_acc: 0.8425\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5499 - acc: 0.7382 - val_loss: 0.4265 - val_acc: 0.8439\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5496 - acc: 0.7378 - val_loss: 0.4294 - val_acc: 0.8415\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5488 - acc: 0.7396 - val_loss: 0.4216 - val_acc: 0.8447\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5474 - acc: 0.7408 - val_loss: 0.4188 - val_acc: 0.8470\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5473 - acc: 0.7403 - val_loss: 0.4190 - val_acc: 0.8481\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5464 - acc: 0.7415 - val_loss: 0.4111 - val_acc: 0.8511\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5455 - acc: 0.7420 - val_loss: 0.4146 - val_acc: 0.8498\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5450 - acc: 0.7420 - val_loss: 0.4141 - val_acc: 0.8516\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5442 - acc: 0.7434 - val_loss: 0.4147 - val_acc: 0.8510\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5437 - acc: 0.7433 - val_loss: 0.4089 - val_acc: 0.8467\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5437 - acc: 0.7444 - val_loss: 0.4174 - val_acc: 0.8507\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5420 - acc: 0.7457 - val_loss: 0.4141 - val_acc: 0.8521\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5413 - acc: 0.7463 - val_loss: 0.4155 - val_acc: 0.8527\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5414 - acc: 0.7456 - val_loss: 0.4159 - val_acc: 0.8517\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5407 - acc: 0.7461 - val_loss: 0.4069 - val_acc: 0.8553\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5400 - acc: 0.7468 - val_loss: 0.4032 - val_acc: 0.8565\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5403 - acc: 0.7477 - val_loss: 0.4159 - val_acc: 0.8459\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5401 - acc: 0.7474 - val_loss: 0.4070 - val_acc: 0.8547\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5384 - acc: 0.7483 - val_loss: 0.4024 - val_acc: 0.8573\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5381 - acc: 0.7488 - val_loss: 0.4071 - val_acc: 0.8560\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5383 - acc: 0.7481 - val_loss: 0.4052 - val_acc: 0.8545\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5369 - acc: 0.7490 - val_loss: 0.4052 - val_acc: 0.8586\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5380 - acc: 0.7499 - val_loss: 0.4032 - val_acc: 0.8585\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5381 - acc: 0.7493 - val_loss: 0.4139 - val_acc: 0.8546\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5369 - acc: 0.7510 - val_loss: 0.4086 - val_acc: 0.8556\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5369 - acc: 0.7498 - val_loss: 0.3992 - val_acc: 0.8577\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5360 - acc: 0.7509 - val_loss: 0.3967 - val_acc: 0.8578\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5354 - acc: 0.7510 - val_loss: 0.3985 - val_acc: 0.8611\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5349 - acc: 0.7525 - val_loss: 0.4038 - val_acc: 0.8593\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5341 - acc: 0.7533 - val_loss: 0.4001 - val_acc: 0.8604\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5341 - acc: 0.7521 - val_loss: 0.3982 - val_acc: 0.8599\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5335 - acc: 0.7518 - val_loss: 0.3997 - val_acc: 0.8603\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5329 - acc: 0.7526 - val_loss: 0.4043 - val_acc: 0.8570\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5326 - acc: 0.7531 - val_loss: 0.3955 - val_acc: 0.8620\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5322 - acc: 0.7540 - val_loss: 0.3996 - val_acc: 0.8594\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5330 - acc: 0.7526 - val_loss: 0.4044 - val_acc: 0.8567\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5324 - acc: 0.7539 - val_loss: 0.4033 - val_acc: 0.8515\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5327 - acc: 0.7536 - val_loss: 0.3911 - val_acc: 0.8611\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5321 - acc: 0.7540 - val_loss: 0.4016 - val_acc: 0.8586\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5318 - acc: 0.7548 - val_loss: 0.3939 - val_acc: 0.8617\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5313 - acc: 0.7555 - val_loss: 0.3995 - val_acc: 0.8594\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5312 - acc: 0.7538 - val_loss: 0.4002 - val_acc: 0.8582\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5299 - acc: 0.7556 - val_loss: 0.3979 - val_acc: 0.8604\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5304 - acc: 0.7538 - val_loss: 0.3942 - val_acc: 0.8621\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5309 - acc: 0.7561 - val_loss: 0.3968 - val_acc: 0.8599\n",
      "Epoch 80/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5314 - acc: 0.7549 - val_loss: 0.3975 - val_acc: 0.8544\n",
      "Epoch 81/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5302 - acc: 0.7563 - val_loss: 0.3996 - val_acc: 0.8561\n",
      "Epoch 82/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5301 - acc: 0.7552 - val_loss: 0.3979 - val_acc: 0.8583\n",
      "auroc: 0.9233781328702424\n",
      "auprc: 0.8757507147512622\n",
      "auroc: 0.9229173914951178\n",
      "auprc: 0.8747058939870246\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 5s 420us/step - loss: 0.8284 - acc: 0.5364 - val_loss: 0.6906 - val_acc: 0.6075\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.7072 - acc: 0.5707 - val_loss: 0.6587 - val_acc: 0.6147\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6827 - acc: 0.5901 - val_loss: 0.6416 - val_acc: 0.6334\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6680 - acc: 0.6036 - val_loss: 0.6197 - val_acc: 0.6583\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6556 - acc: 0.6162 - val_loss: 0.6116 - val_acc: 0.6645\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6454 - acc: 0.6314 - val_loss: 0.5887 - val_acc: 0.6900\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6362 - acc: 0.6418 - val_loss: 0.5799 - val_acc: 0.6966\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6274 - acc: 0.6503 - val_loss: 0.5652 - val_acc: 0.7119\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6198 - acc: 0.6594 - val_loss: 0.5550 - val_acc: 0.7219\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.6133 - acc: 0.6682 - val_loss: 0.5473 - val_acc: 0.7303\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6077 - acc: 0.6745 - val_loss: 0.5330 - val_acc: 0.7432\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6026 - acc: 0.6799 - val_loss: 0.5186 - val_acc: 0.7546\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5981 - acc: 0.6867 - val_loss: 0.5157 - val_acc: 0.7596\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5934 - acc: 0.6919 - val_loss: 0.5088 - val_acc: 0.7680\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5896 - acc: 0.6963 - val_loss: 0.5012 - val_acc: 0.7760\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5861 - acc: 0.7002 - val_loss: 0.4904 - val_acc: 0.7811\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5830 - acc: 0.7048 - val_loss: 0.4896 - val_acc: 0.7856\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5808 - acc: 0.7065 - val_loss: 0.4868 - val_acc: 0.7890\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5770 - acc: 0.7095 - val_loss: 0.4774 - val_acc: 0.7971\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5743 - acc: 0.7137 - val_loss: 0.4747 - val_acc: 0.8013\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5720 - acc: 0.7158 - val_loss: 0.4637 - val_acc: 0.8049\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5702 - acc: 0.7185 - val_loss: 0.4676 - val_acc: 0.8020\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5685 - acc: 0.7191 - val_loss: 0.4649 - val_acc: 0.8064\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5660 - acc: 0.7220 - val_loss: 0.4532 - val_acc: 0.8137\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5643 - acc: 0.7230 - val_loss: 0.4583 - val_acc: 0.8160\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5624 - acc: 0.7267 - val_loss: 0.4524 - val_acc: 0.8199\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5608 - acc: 0.7278 - val_loss: 0.4434 - val_acc: 0.8217\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5598 - acc: 0.7280 - val_loss: 0.4489 - val_acc: 0.8232\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5579 - acc: 0.7301 - val_loss: 0.4437 - val_acc: 0.8263\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5566 - acc: 0.7321 - val_loss: 0.4370 - val_acc: 0.8293\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5556 - acc: 0.7334 - val_loss: 0.4381 - val_acc: 0.8290\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5539 - acc: 0.7343 - val_loss: 0.4385 - val_acc: 0.8305\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5533 - acc: 0.7348 - val_loss: 0.4389 - val_acc: 0.8315\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5520 - acc: 0.7364 - val_loss: 0.4348 - val_acc: 0.8271\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5524 - acc: 0.7364 - val_loss: 0.4318 - val_acc: 0.8345\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5505 - acc: 0.7377 - val_loss: 0.4267 - val_acc: 0.8375\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5489 - acc: 0.7396 - val_loss: 0.4285 - val_acc: 0.8380\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5487 - acc: 0.7398 - val_loss: 0.4283 - val_acc: 0.8356\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5478 - acc: 0.7394 - val_loss: 0.4198 - val_acc: 0.8404\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5476 - acc: 0.7391 - val_loss: 0.4226 - val_acc: 0.8408\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5466 - acc: 0.7415 - val_loss: 0.4276 - val_acc: 0.8379\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5453 - acc: 0.7427 - val_loss: 0.4204 - val_acc: 0.8436\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5445 - acc: 0.7426 - val_loss: 0.4197 - val_acc: 0.8444\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5432 - acc: 0.7446 - val_loss: 0.4201 - val_acc: 0.8439\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5430 - acc: 0.7453 - val_loss: 0.4162 - val_acc: 0.8449\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5427 - acc: 0.7452 - val_loss: 0.4232 - val_acc: 0.8415\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5420 - acc: 0.7457 - val_loss: 0.4144 - val_acc: 0.8480\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5413 - acc: 0.7449 - val_loss: 0.4175 - val_acc: 0.8450\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5410 - acc: 0.7458 - val_loss: 0.4067 - val_acc: 0.8480\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5411 - acc: 0.7434 - val_loss: 0.4202 - val_acc: 0.8461\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5393 - acc: 0.7470 - val_loss: 0.4107 - val_acc: 0.8486\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5403 - acc: 0.7468 - val_loss: 0.4154 - val_acc: 0.8474\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5393 - acc: 0.7472 - val_loss: 0.4096 - val_acc: 0.8493\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5381 - acc: 0.7487 - val_loss: 0.4077 - val_acc: 0.8512\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5376 - acc: 0.7483 - val_loss: 0.4140 - val_acc: 0.8490\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5375 - acc: 0.7489 - val_loss: 0.4084 - val_acc: 0.8504\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5361 - acc: 0.7499 - val_loss: 0.4074 - val_acc: 0.8524\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5368 - acc: 0.7499 - val_loss: 0.4062 - val_acc: 0.8520\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5359 - acc: 0.7518 - val_loss: 0.4031 - val_acc: 0.8526\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5354 - acc: 0.7506 - val_loss: 0.4052 - val_acc: 0.8540\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5352 - acc: 0.7506 - val_loss: 0.4124 - val_acc: 0.8506\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5356 - acc: 0.7496 - val_loss: 0.4046 - val_acc: 0.8540\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5339 - acc: 0.7518 - val_loss: 0.4103 - val_acc: 0.8514\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5342 - acc: 0.7505 - val_loss: 0.4057 - val_acc: 0.8540\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5341 - acc: 0.7504 - val_loss: 0.4028 - val_acc: 0.8542\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5337 - acc: 0.7501 - val_loss: 0.4016 - val_acc: 0.8527\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5330 - acc: 0.7531 - val_loss: 0.4042 - val_acc: 0.8535\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5332 - acc: 0.7527 - val_loss: 0.4057 - val_acc: 0.8513\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5322 - acc: 0.7527 - val_loss: 0.4002 - val_acc: 0.8549\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5320 - acc: 0.7530 - val_loss: 0.4049 - val_acc: 0.8527\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5318 - acc: 0.7530 - val_loss: 0.3969 - val_acc: 0.8541\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5313 - acc: 0.7531 - val_loss: 0.4026 - val_acc: 0.8546\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5306 - acc: 0.7541 - val_loss: 0.4037 - val_acc: 0.8531\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5309 - acc: 0.7533 - val_loss: 0.4022 - val_acc: 0.8523\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5315 - acc: 0.7527 - val_loss: 0.3966 - val_acc: 0.8539\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5315 - acc: 0.7544 - val_loss: 0.4028 - val_acc: 0.8510\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5306 - acc: 0.7540 - val_loss: 0.4007 - val_acc: 0.8529\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5296 - acc: 0.7555 - val_loss: 0.4033 - val_acc: 0.8509\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5310 - acc: 0.7536 - val_loss: 0.4098 - val_acc: 0.8484\n",
      "Epoch 80/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5295 - acc: 0.7553 - val_loss: 0.4018 - val_acc: 0.8523\n",
      "Epoch 81/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5299 - acc: 0.7535 - val_loss: 0.4027 - val_acc: 0.8533\n",
      "Epoch 82/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5286 - acc: 0.7559 - val_loss: 0.3990 - val_acc: 0.8542\n",
      "Epoch 83/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5280 - acc: 0.7566 - val_loss: 0.3963 - val_acc: 0.8549\n",
      "Epoch 84/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5281 - acc: 0.7560 - val_loss: 0.4001 - val_acc: 0.8544\n",
      "Epoch 85/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5278 - acc: 0.7561 - val_loss: 0.3983 - val_acc: 0.8553\n",
      "Epoch 86/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5280 - acc: 0.7560 - val_loss: 0.3952 - val_acc: 0.8556\n",
      "Epoch 87/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5283 - acc: 0.7553 - val_loss: 0.3963 - val_acc: 0.8556\n",
      "Epoch 88/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5282 - acc: 0.7556 - val_loss: 0.3999 - val_acc: 0.8529\n",
      "Epoch 89/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5273 - acc: 0.7557 - val_loss: 0.3976 - val_acc: 0.8541\n",
      "Epoch 90/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5265 - acc: 0.7566 - val_loss: 0.3915 - val_acc: 0.8549\n",
      "Epoch 91/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5270 - acc: 0.7552 - val_loss: 0.3965 - val_acc: 0.8549\n",
      "Epoch 92/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5269 - acc: 0.7564 - val_loss: 0.3972 - val_acc: 0.8534\n",
      "Epoch 93/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5271 - acc: 0.7560 - val_loss: 0.4031 - val_acc: 0.8507\n",
      "Epoch 94/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5276 - acc: 0.7542 - val_loss: 0.3999 - val_acc: 0.8526\n",
      "Epoch 95/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5260 - acc: 0.7567 - val_loss: 0.3978 - val_acc: 0.8543\n",
      "Epoch 96/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5257 - acc: 0.7567 - val_loss: 0.3937 - val_acc: 0.8548\n",
      "Epoch 97/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5260 - acc: 0.7571 - val_loss: 0.3944 - val_acc: 0.8527\n",
      "Epoch 98/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5282 - acc: 0.7554 - val_loss: 0.4004 - val_acc: 0.8515\n",
      "Epoch 99/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5251 - acc: 0.7567 - val_loss: 0.3978 - val_acc: 0.8530\n",
      "Epoch 100/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5249 - acc: 0.7570 - val_loss: 0.3979 - val_acc: 0.8533\n",
      "auroc: 0.9202610159023497\n",
      "auprc: 0.873344547694063\n",
      "auroc: 0.9203129051790032\n",
      "auprc: 0.8731671246099771\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 5s 431us/step - loss: 0.7774 - acc: 0.5445 - val_loss: 0.6540 - val_acc: 0.6403\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6967 - acc: 0.5824 - val_loss: 0.6392 - val_acc: 0.6391\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6756 - acc: 0.6005 - val_loss: 0.6182 - val_acc: 0.6594\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6609 - acc: 0.6168 - val_loss: 0.5992 - val_acc: 0.6829\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6496 - acc: 0.6286 - val_loss: 0.5920 - val_acc: 0.6860\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6397 - acc: 0.6402 - val_loss: 0.5777 - val_acc: 0.6993\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6311 - acc: 0.6503 - val_loss: 0.5646 - val_acc: 0.7147\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6236 - acc: 0.6581 - val_loss: 0.5615 - val_acc: 0.7161\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6173 - acc: 0.6664 - val_loss: 0.5467 - val_acc: 0.7256\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6122 - acc: 0.6717 - val_loss: 0.5384 - val_acc: 0.7347\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6064 - acc: 0.6783 - val_loss: 0.5293 - val_acc: 0.7456\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6016 - acc: 0.6813 - val_loss: 0.5156 - val_acc: 0.7581\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5977 - acc: 0.6860 - val_loss: 0.5118 - val_acc: 0.7603\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5937 - acc: 0.6919 - val_loss: 0.5006 - val_acc: 0.7701\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5909 - acc: 0.6946 - val_loss: 0.4970 - val_acc: 0.7737\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5873 - acc: 0.6987 - val_loss: 0.4930 - val_acc: 0.7791\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5848 - acc: 0.7024 - val_loss: 0.4824 - val_acc: 0.7856\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5819 - acc: 0.7048 - val_loss: 0.4865 - val_acc: 0.7864\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5794 - acc: 0.7076 - val_loss: 0.4808 - val_acc: 0.7929\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5769 - acc: 0.7104 - val_loss: 0.4777 - val_acc: 0.7961\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5749 - acc: 0.7130 - val_loss: 0.4697 - val_acc: 0.8018\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5724 - acc: 0.7154 - val_loss: 0.4658 - val_acc: 0.8032\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5706 - acc: 0.7164 - val_loss: 0.4648 - val_acc: 0.8049\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5689 - acc: 0.7200 - val_loss: 0.4621 - val_acc: 0.8089\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5669 - acc: 0.7223 - val_loss: 0.4560 - val_acc: 0.8120\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5659 - acc: 0.7241 - val_loss: 0.4532 - val_acc: 0.8156\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5642 - acc: 0.7266 - val_loss: 0.4598 - val_acc: 0.8118\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5630 - acc: 0.7274 - val_loss: 0.4499 - val_acc: 0.8167\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5616 - acc: 0.7297 - val_loss: 0.4533 - val_acc: 0.8176\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5607 - acc: 0.7288 - val_loss: 0.4477 - val_acc: 0.8191\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5595 - acc: 0.7311 - val_loss: 0.4450 - val_acc: 0.8228\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5587 - acc: 0.7307 - val_loss: 0.4451 - val_acc: 0.8217\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5580 - acc: 0.7314 - val_loss: 0.4379 - val_acc: 0.8255\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5561 - acc: 0.7339 - val_loss: 0.4351 - val_acc: 0.8291\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5562 - acc: 0.7346 - val_loss: 0.4456 - val_acc: 0.8188\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5546 - acc: 0.7368 - val_loss: 0.4377 - val_acc: 0.8308\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5533 - acc: 0.7388 - val_loss: 0.4365 - val_acc: 0.8286\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5535 - acc: 0.7360 - val_loss: 0.4281 - val_acc: 0.8356\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5520 - acc: 0.7391 - val_loss: 0.4294 - val_acc: 0.8358\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5515 - acc: 0.7378 - val_loss: 0.4281 - val_acc: 0.8344\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5504 - acc: 0.7394 - val_loss: 0.4289 - val_acc: 0.8361\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5495 - acc: 0.7405 - val_loss: 0.4251 - val_acc: 0.8380\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5493 - acc: 0.7399 - val_loss: 0.4271 - val_acc: 0.8370\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5496 - acc: 0.7401 - val_loss: 0.4250 - val_acc: 0.8398\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5479 - acc: 0.7403 - val_loss: 0.4244 - val_acc: 0.8376\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5475 - acc: 0.7410 - val_loss: 0.4239 - val_acc: 0.8420\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5463 - acc: 0.7443 - val_loss: 0.4256 - val_acc: 0.8379\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5463 - acc: 0.7412 - val_loss: 0.4248 - val_acc: 0.8404\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5447 - acc: 0.7438 - val_loss: 0.4173 - val_acc: 0.8446\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5443 - acc: 0.7443 - val_loss: 0.4246 - val_acc: 0.8395\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5438 - acc: 0.7448 - val_loss: 0.4197 - val_acc: 0.8413\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5447 - acc: 0.7444 - val_loss: 0.4150 - val_acc: 0.8459\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5427 - acc: 0.7453 - val_loss: 0.4190 - val_acc: 0.8439\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5428 - acc: 0.7440 - val_loss: 0.4180 - val_acc: 0.8416\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5419 - acc: 0.7455 - val_loss: 0.4183 - val_acc: 0.8456\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5415 - acc: 0.7458 - val_loss: 0.4169 - val_acc: 0.8455\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5423 - acc: 0.7456 - val_loss: 0.4113 - val_acc: 0.8473\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5409 - acc: 0.7459 - val_loss: 0.4163 - val_acc: 0.8469\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5404 - acc: 0.7471 - val_loss: 0.4167 - val_acc: 0.8472\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5399 - acc: 0.7476 - val_loss: 0.4138 - val_acc: 0.8479\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5393 - acc: 0.7472 - val_loss: 0.4144 - val_acc: 0.8463\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5390 - acc: 0.7463 - val_loss: 0.4137 - val_acc: 0.8471\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5384 - acc: 0.7489 - val_loss: 0.4200 - val_acc: 0.8450\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5384 - acc: 0.7485 - val_loss: 0.4134 - val_acc: 0.8481\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5378 - acc: 0.7481 - val_loss: 0.4159 - val_acc: 0.8480\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5377 - acc: 0.7494 - val_loss: 0.4107 - val_acc: 0.8494\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5381 - acc: 0.7479 - val_loss: 0.4096 - val_acc: 0.8483\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5368 - acc: 0.7494 - val_loss: 0.4226 - val_acc: 0.8424\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5361 - acc: 0.7491 - val_loss: 0.4076 - val_acc: 0.8502\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5354 - acc: 0.7499 - val_loss: 0.4026 - val_acc: 0.8481\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5361 - acc: 0.7494 - val_loss: 0.4116 - val_acc: 0.8457\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5358 - acc: 0.7493 - val_loss: 0.4095 - val_acc: 0.8497\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5349 - acc: 0.7512 - val_loss: 0.4123 - val_acc: 0.8490\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5342 - acc: 0.7510 - val_loss: 0.4083 - val_acc: 0.8515\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5343 - acc: 0.7498 - val_loss: 0.4130 - val_acc: 0.8493\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5339 - acc: 0.7519 - val_loss: 0.4066 - val_acc: 0.8516\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5347 - acc: 0.7496 - val_loss: 0.4065 - val_acc: 0.8512\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5335 - acc: 0.7508 - val_loss: 0.4173 - val_acc: 0.8466\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5334 - acc: 0.7498 - val_loss: 0.4071 - val_acc: 0.8506\n",
      "Epoch 80/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5338 - acc: 0.7493 - val_loss: 0.4046 - val_acc: 0.8514\n",
      "auroc: 0.9178216658744267\n",
      "auprc: 0.8678808444628027\n",
      "auroc: 0.9171189715669792\n",
      "auprc: 0.8662337314461604\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 5s 447us/step - loss: 0.7667 - acc: 0.5455 - val_loss: 0.6657 - val_acc: 0.6410\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.7007 - acc: 0.5796 - val_loss: 0.6496 - val_acc: 0.6373\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6808 - acc: 0.5972 - val_loss: 0.6408 - val_acc: 0.6376\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6684 - acc: 0.6076 - val_loss: 0.6267 - val_acc: 0.6539\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6574 - acc: 0.6193 - val_loss: 0.6112 - val_acc: 0.6713\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6482 - acc: 0.6280 - val_loss: 0.6023 - val_acc: 0.6795\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6406 - acc: 0.6367 - val_loss: 0.5876 - val_acc: 0.6907\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6323 - acc: 0.6462 - val_loss: 0.5747 - val_acc: 0.7066\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6251 - acc: 0.6543 - val_loss: 0.5631 - val_acc: 0.7177\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6199 - acc: 0.6595 - val_loss: 0.5512 - val_acc: 0.7303\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6136 - acc: 0.6674 - val_loss: 0.5426 - val_acc: 0.7392\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6081 - acc: 0.6745 - val_loss: 0.5311 - val_acc: 0.7444\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6046 - acc: 0.6776 - val_loss: 0.5246 - val_acc: 0.7562\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5993 - acc: 0.6857 - val_loss: 0.5090 - val_acc: 0.7670\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5943 - acc: 0.6885 - val_loss: 0.5048 - val_acc: 0.7715\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5910 - acc: 0.6933 - val_loss: 0.4963 - val_acc: 0.7804\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5873 - acc: 0.6974 - val_loss: 0.4952 - val_acc: 0.7839\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5845 - acc: 0.7017 - val_loss: 0.4890 - val_acc: 0.7884\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5815 - acc: 0.7053 - val_loss: 0.4812 - val_acc: 0.7950\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5791 - acc: 0.7076 - val_loss: 0.4749 - val_acc: 0.8009\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5762 - acc: 0.7108 - val_loss: 0.4759 - val_acc: 0.8011\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5736 - acc: 0.7129 - val_loss: 0.4709 - val_acc: 0.8028\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5725 - acc: 0.7143 - val_loss: 0.4687 - val_acc: 0.8095\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5695 - acc: 0.7180 - val_loss: 0.4576 - val_acc: 0.8133\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5689 - acc: 0.7184 - val_loss: 0.4615 - val_acc: 0.8152\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5667 - acc: 0.7221 - val_loss: 0.4538 - val_acc: 0.8191\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5646 - acc: 0.7232 - val_loss: 0.4456 - val_acc: 0.8199\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5640 - acc: 0.7244 - val_loss: 0.4514 - val_acc: 0.8220\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5624 - acc: 0.7240 - val_loss: 0.4493 - val_acc: 0.8207\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5610 - acc: 0.7270 - val_loss: 0.4449 - val_acc: 0.8276\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5605 - acc: 0.7275 - val_loss: 0.4415 - val_acc: 0.8281\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5583 - acc: 0.7293 - val_loss: 0.4381 - val_acc: 0.8307\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5571 - acc: 0.7300 - val_loss: 0.4446 - val_acc: 0.8289\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5563 - acc: 0.7318 - val_loss: 0.4353 - val_acc: 0.8310\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5553 - acc: 0.7332 - val_loss: 0.4403 - val_acc: 0.8329\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5549 - acc: 0.7337 - val_loss: 0.4312 - val_acc: 0.8368\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5540 - acc: 0.7345 - val_loss: 0.4304 - val_acc: 0.8377\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5526 - acc: 0.7348 - val_loss: 0.4297 - val_acc: 0.8396\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5523 - acc: 0.7359 - val_loss: 0.4297 - val_acc: 0.8394\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5516 - acc: 0.7376 - val_loss: 0.4282 - val_acc: 0.8338\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5510 - acc: 0.7391 - val_loss: 0.4245 - val_acc: 0.8422\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5501 - acc: 0.7394 - val_loss: 0.4256 - val_acc: 0.8414\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5492 - acc: 0.7395 - val_loss: 0.4236 - val_acc: 0.8409\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5485 - acc: 0.7403 - val_loss: 0.4260 - val_acc: 0.8391\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5478 - acc: 0.7413 - val_loss: 0.4180 - val_acc: 0.8430\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5476 - acc: 0.7428 - val_loss: 0.4196 - val_acc: 0.8433\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5465 - acc: 0.7426 - val_loss: 0.4184 - val_acc: 0.8435\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5464 - acc: 0.7429 - val_loss: 0.4131 - val_acc: 0.8458\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5458 - acc: 0.7445 - val_loss: 0.4164 - val_acc: 0.8411\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5457 - acc: 0.7433 - val_loss: 0.4211 - val_acc: 0.8455\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5458 - acc: 0.7436 - val_loss: 0.4226 - val_acc: 0.8426\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5460 - acc: 0.7417 - val_loss: 0.4161 - val_acc: 0.8387\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5444 - acc: 0.7446 - val_loss: 0.4137 - val_acc: 0.8438\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5431 - acc: 0.7456 - val_loss: 0.4215 - val_acc: 0.8431\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5445 - acc: 0.7453 - val_loss: 0.4124 - val_acc: 0.8480\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5431 - acc: 0.7456 - val_loss: 0.4111 - val_acc: 0.8480\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5430 - acc: 0.7450 - val_loss: 0.4140 - val_acc: 0.8471\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5425 - acc: 0.7459 - val_loss: 0.4128 - val_acc: 0.8496\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5421 - acc: 0.7477 - val_loss: 0.4089 - val_acc: 0.8493\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5418 - acc: 0.7469 - val_loss: 0.4134 - val_acc: 0.8475\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5417 - acc: 0.7468 - val_loss: 0.4049 - val_acc: 0.8480\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5408 - acc: 0.7482 - val_loss: 0.4085 - val_acc: 0.8489\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5398 - acc: 0.7473 - val_loss: 0.4178 - val_acc: 0.8462\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5393 - acc: 0.7484 - val_loss: 0.4046 - val_acc: 0.8490\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5387 - acc: 0.7497 - val_loss: 0.4087 - val_acc: 0.8512\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5380 - acc: 0.7503 - val_loss: 0.4110 - val_acc: 0.8482\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5390 - acc: 0.7487 - val_loss: 0.4020 - val_acc: 0.8496\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5385 - acc: 0.7503 - val_loss: 0.4060 - val_acc: 0.8475\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5376 - acc: 0.7481 - val_loss: 0.4185 - val_acc: 0.8458\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5374 - acc: 0.7502 - val_loss: 0.4100 - val_acc: 0.8497\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5372 - acc: 0.7504 - val_loss: 0.4017 - val_acc: 0.8507\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5363 - acc: 0.7511 - val_loss: 0.4060 - val_acc: 0.8499\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5363 - acc: 0.7506 - val_loss: 0.4053 - val_acc: 0.8498\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5358 - acc: 0.7504 - val_loss: 0.4063 - val_acc: 0.8523\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5355 - acc: 0.7521 - val_loss: 0.4083 - val_acc: 0.8486\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5355 - acc: 0.7516 - val_loss: 0.4003 - val_acc: 0.8515\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5346 - acc: 0.7524 - val_loss: 0.4084 - val_acc: 0.8509\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5341 - acc: 0.7526 - val_loss: 0.4091 - val_acc: 0.8466\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5348 - acc: 0.7511 - val_loss: 0.4009 - val_acc: 0.8533\n",
      "Epoch 80/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5339 - acc: 0.7534 - val_loss: 0.3966 - val_acc: 0.8530\n",
      "Epoch 81/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5331 - acc: 0.7537 - val_loss: 0.4079 - val_acc: 0.8489\n",
      "Epoch 82/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5336 - acc: 0.7539 - val_loss: 0.4031 - val_acc: 0.8463\n",
      "Epoch 83/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5347 - acc: 0.7517 - val_loss: 0.3991 - val_acc: 0.8507\n",
      "Epoch 84/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5340 - acc: 0.7514 - val_loss: 0.3981 - val_acc: 0.8533\n",
      "Epoch 85/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5330 - acc: 0.7530 - val_loss: 0.3996 - val_acc: 0.8507\n",
      "Epoch 86/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5322 - acc: 0.7547 - val_loss: 0.4080 - val_acc: 0.8485\n",
      "Epoch 87/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5324 - acc: 0.7536 - val_loss: 0.4067 - val_acc: 0.8494\n",
      "Epoch 88/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5322 - acc: 0.7536 - val_loss: 0.4078 - val_acc: 0.8490\n",
      "Epoch 89/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5318 - acc: 0.7534 - val_loss: 0.3997 - val_acc: 0.8519\n",
      "Epoch 90/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5308 - acc: 0.7550 - val_loss: 0.3965 - val_acc: 0.8534\n",
      "Epoch 91/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5315 - acc: 0.7557 - val_loss: 0.4004 - val_acc: 0.8497\n",
      "Epoch 92/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5317 - acc: 0.7545 - val_loss: 0.4050 - val_acc: 0.8493\n",
      "Epoch 93/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5315 - acc: 0.7533 - val_loss: 0.3997 - val_acc: 0.8497\n",
      "Epoch 94/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5303 - acc: 0.7558 - val_loss: 0.3966 - val_acc: 0.8509\n",
      "Epoch 95/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5296 - acc: 0.7553 - val_loss: 0.4030 - val_acc: 0.8525\n",
      "Epoch 96/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5314 - acc: 0.7555 - val_loss: 0.3997 - val_acc: 0.8511\n",
      "Epoch 97/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5317 - acc: 0.7542 - val_loss: 0.4019 - val_acc: 0.8529\n",
      "Epoch 98/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5296 - acc: 0.7558 - val_loss: 0.4014 - val_acc: 0.8517\n",
      "Epoch 99/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5292 - acc: 0.7555 - val_loss: 0.3938 - val_acc: 0.8544\n",
      "Epoch 100/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5284 - acc: 0.7573 - val_loss: 0.3972 - val_acc: 0.8533\n",
      "Epoch 101/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5282 - acc: 0.7582 - val_loss: 0.4028 - val_acc: 0.8512\n",
      "Epoch 102/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5287 - acc: 0.7568 - val_loss: 0.3990 - val_acc: 0.8541\n",
      "Epoch 103/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5277 - acc: 0.7563 - val_loss: 0.3983 - val_acc: 0.8536\n",
      "Epoch 104/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5289 - acc: 0.7573 - val_loss: 0.3979 - val_acc: 0.8537\n",
      "Epoch 105/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5282 - acc: 0.7571 - val_loss: 0.3935 - val_acc: 0.8537\n",
      "Epoch 106/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5279 - acc: 0.7571 - val_loss: 0.4028 - val_acc: 0.8484\n",
      "Epoch 107/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5275 - acc: 0.7570 - val_loss: 0.3955 - val_acc: 0.8548\n",
      "Epoch 108/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5274 - acc: 0.7559 - val_loss: 0.3977 - val_acc: 0.8538\n",
      "Epoch 109/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5276 - acc: 0.7576 - val_loss: 0.3972 - val_acc: 0.8540\n",
      "Epoch 110/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5274 - acc: 0.7550 - val_loss: 0.3949 - val_acc: 0.8553\n",
      "Epoch 111/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5267 - acc: 0.7576 - val_loss: 0.3933 - val_acc: 0.8543\n",
      "Epoch 112/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5266 - acc: 0.7569 - val_loss: 0.3991 - val_acc: 0.8521\n",
      "Epoch 113/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5261 - acc: 0.7584 - val_loss: 0.4027 - val_acc: 0.8516\n",
      "Epoch 114/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5263 - acc: 0.7571 - val_loss: 0.4027 - val_acc: 0.8509\n",
      "Epoch 115/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5265 - acc: 0.7583 - val_loss: 0.3987 - val_acc: 0.8502\n",
      "Epoch 116/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5257 - acc: 0.7584 - val_loss: 0.3975 - val_acc: 0.8511\n",
      "Epoch 117/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5258 - acc: 0.7579 - val_loss: 0.4025 - val_acc: 0.8429\n",
      "Epoch 118/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5258 - acc: 0.7572 - val_loss: 0.3979 - val_acc: 0.8533\n",
      "Epoch 119/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5246 - acc: 0.7586 - val_loss: 0.4035 - val_acc: 0.8483\n",
      "Epoch 120/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5252 - acc: 0.7581 - val_loss: 0.4013 - val_acc: 0.8517\n",
      "Epoch 121/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5258 - acc: 0.7573 - val_loss: 0.3978 - val_acc: 0.8520\n",
      "auroc: 0.9230187592565132\n",
      "auprc: 0.8764468263333659\n",
      "auroc: 0.9221946327720533\n",
      "auprc: 0.8750544949830757\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 5s 450us/step - loss: 0.7558 - acc: 0.5440 - val_loss: 0.6840 - val_acc: 0.5977\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.7031 - acc: 0.5699 - val_loss: 0.6623 - val_acc: 0.6123\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6844 - acc: 0.5882 - val_loss: 0.6487 - val_acc: 0.6244\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6720 - acc: 0.6006 - val_loss: 0.6349 - val_acc: 0.6403\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6611 - acc: 0.6133 - val_loss: 0.6222 - val_acc: 0.6547\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6523 - acc: 0.6210 - val_loss: 0.6099 - val_acc: 0.6665\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.6438 - acc: 0.6305 - val_loss: 0.5992 - val_acc: 0.6779\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6364 - acc: 0.6384 - val_loss: 0.5879 - val_acc: 0.6947\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6294 - acc: 0.6466 - val_loss: 0.5739 - val_acc: 0.7069\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6220 - acc: 0.6555 - val_loss: 0.5614 - val_acc: 0.7177\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6156 - acc: 0.6631 - val_loss: 0.5512 - val_acc: 0.7311\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6100 - acc: 0.6699 - val_loss: 0.5351 - val_acc: 0.7421\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6048 - acc: 0.6773 - val_loss: 0.5273 - val_acc: 0.7496\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5994 - acc: 0.6815 - val_loss: 0.5204 - val_acc: 0.7573\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5956 - acc: 0.6868 - val_loss: 0.5131 - val_acc: 0.7659\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5910 - acc: 0.6916 - val_loss: 0.5012 - val_acc: 0.7750\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5867 - acc: 0.6978 - val_loss: 0.4982 - val_acc: 0.7812\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5838 - acc: 0.7016 - val_loss: 0.4863 - val_acc: 0.7899\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5802 - acc: 0.7053 - val_loss: 0.4810 - val_acc: 0.7950\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5769 - acc: 0.7084 - val_loss: 0.4703 - val_acc: 0.8010\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5739 - acc: 0.7129 - val_loss: 0.4652 - val_acc: 0.8069\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5708 - acc: 0.7159 - val_loss: 0.4613 - val_acc: 0.8106\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5692 - acc: 0.7187 - val_loss: 0.4580 - val_acc: 0.8156\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5672 - acc: 0.7207 - val_loss: 0.4622 - val_acc: 0.8160\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5655 - acc: 0.7244 - val_loss: 0.4522 - val_acc: 0.8206\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5633 - acc: 0.7261 - val_loss: 0.4462 - val_acc: 0.8222\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5615 - acc: 0.7277 - val_loss: 0.4430 - val_acc: 0.8291\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5608 - acc: 0.7269 - val_loss: 0.4388 - val_acc: 0.8324\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5587 - acc: 0.7303 - val_loss: 0.4413 - val_acc: 0.8297\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5579 - acc: 0.7303 - val_loss: 0.4383 - val_acc: 0.8307\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5572 - acc: 0.7313 - val_loss: 0.4372 - val_acc: 0.8328\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5562 - acc: 0.7315 - val_loss: 0.4278 - val_acc: 0.8380\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5546 - acc: 0.7334 - val_loss: 0.4376 - val_acc: 0.8353\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5538 - acc: 0.7342 - val_loss: 0.4305 - val_acc: 0.8385\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5524 - acc: 0.7366 - val_loss: 0.4290 - val_acc: 0.8373\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5517 - acc: 0.7357 - val_loss: 0.4262 - val_acc: 0.8423\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5503 - acc: 0.7379 - val_loss: 0.4261 - val_acc: 0.8402\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5502 - acc: 0.7380 - val_loss: 0.4210 - val_acc: 0.8417\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5500 - acc: 0.7370 - val_loss: 0.4281 - val_acc: 0.8394\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5490 - acc: 0.7393 - val_loss: 0.4304 - val_acc: 0.8360\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5507 - acc: 0.7372 - val_loss: 0.4214 - val_acc: 0.8446\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5488 - acc: 0.7376 - val_loss: 0.4267 - val_acc: 0.8409\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5465 - acc: 0.7401 - val_loss: 0.4220 - val_acc: 0.8446\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5459 - acc: 0.7418 - val_loss: 0.4195 - val_acc: 0.8459\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5447 - acc: 0.7419 - val_loss: 0.4194 - val_acc: 0.8451\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5445 - acc: 0.7426 - val_loss: 0.4223 - val_acc: 0.8454\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5440 - acc: 0.7425 - val_loss: 0.4221 - val_acc: 0.8445\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5436 - acc: 0.7429 - val_loss: 0.4249 - val_acc: 0.8435\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5430 - acc: 0.7433 - val_loss: 0.4160 - val_acc: 0.8466\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5424 - acc: 0.7446 - val_loss: 0.4143 - val_acc: 0.8500\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5430 - acc: 0.7443 - val_loss: 0.4187 - val_acc: 0.8465\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5415 - acc: 0.7454 - val_loss: 0.4141 - val_acc: 0.8490\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5411 - acc: 0.7469 - val_loss: 0.4099 - val_acc: 0.8504\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5413 - acc: 0.7441 - val_loss: 0.4247 - val_acc: 0.8395\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5421 - acc: 0.7439 - val_loss: 0.4126 - val_acc: 0.8488\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5408 - acc: 0.7451 - val_loss: 0.4180 - val_acc: 0.8463\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5404 - acc: 0.7463 - val_loss: 0.4132 - val_acc: 0.8470\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5415 - acc: 0.7438 - val_loss: 0.4073 - val_acc: 0.8504\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5394 - acc: 0.7460 - val_loss: 0.4092 - val_acc: 0.8500\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5395 - acc: 0.7468 - val_loss: 0.4130 - val_acc: 0.8453\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5391 - acc: 0.7467 - val_loss: 0.4056 - val_acc: 0.8499\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5386 - acc: 0.7476 - val_loss: 0.4131 - val_acc: 0.8471\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5378 - acc: 0.7481 - val_loss: 0.4056 - val_acc: 0.8500\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5378 - acc: 0.7482 - val_loss: 0.4077 - val_acc: 0.8496\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5384 - acc: 0.7470 - val_loss: 0.4088 - val_acc: 0.8511\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5378 - acc: 0.7481 - val_loss: 0.4091 - val_acc: 0.8523\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5365 - acc: 0.7491 - val_loss: 0.4109 - val_acc: 0.8495\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5371 - acc: 0.7473 - val_loss: 0.4090 - val_acc: 0.8473\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5357 - acc: 0.7498 - val_loss: 0.4072 - val_acc: 0.8516\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5352 - acc: 0.7506 - val_loss: 0.4055 - val_acc: 0.8511\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5354 - acc: 0.7492 - val_loss: 0.4108 - val_acc: 0.8494\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5360 - acc: 0.7471 - val_loss: 0.4094 - val_acc: 0.8492\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5342 - acc: 0.7503 - val_loss: 0.4020 - val_acc: 0.8520\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5342 - acc: 0.7511 - val_loss: 0.4127 - val_acc: 0.8483\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5339 - acc: 0.7503 - val_loss: 0.4057 - val_acc: 0.8497\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5344 - acc: 0.7499 - val_loss: 0.4062 - val_acc: 0.8516\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5339 - acc: 0.7499 - val_loss: 0.4111 - val_acc: 0.8491\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5339 - acc: 0.7498 - val_loss: 0.4073 - val_acc: 0.8500\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5336 - acc: 0.7504 - val_loss: 0.4042 - val_acc: 0.8514\n",
      "Epoch 80/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5337 - acc: 0.7497 - val_loss: 0.4044 - val_acc: 0.8491\n",
      "Epoch 81/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5331 - acc: 0.7498 - val_loss: 0.4045 - val_acc: 0.8510\n",
      "Epoch 82/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5325 - acc: 0.7504 - val_loss: 0.4069 - val_acc: 0.8501\n",
      "Epoch 83/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5332 - acc: 0.7499 - val_loss: 0.4051 - val_acc: 0.8503\n",
      "auroc: 0.914077824678713\n",
      "auprc: 0.8623667382483792\n",
      "auroc: 0.9135279502998973\n",
      "auprc: 0.8610297787586677\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 6s 484us/step - loss: 1.0188 - acc: 0.5193 - val_loss: 0.7129 - val_acc: 0.5888\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.7198 - acc: 0.5733 - val_loss: 0.6824 - val_acc: 0.5792\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6850 - acc: 0.5843 - val_loss: 0.6441 - val_acc: 0.6311\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6693 - acc: 0.6024 - val_loss: 0.6355 - val_acc: 0.6389\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6583 - acc: 0.6154 - val_loss: 0.6167 - val_acc: 0.6625\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6486 - acc: 0.6254 - val_loss: 0.6054 - val_acc: 0.6751\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6393 - acc: 0.6371 - val_loss: 0.5945 - val_acc: 0.6871\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.6323 - acc: 0.6477 - val_loss: 0.5804 - val_acc: 0.7020\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6246 - acc: 0.6563 - val_loss: 0.5696 - val_acc: 0.7140\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6179 - acc: 0.6641 - val_loss: 0.5576 - val_acc: 0.7264\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6121 - acc: 0.6702 - val_loss: 0.5475 - val_acc: 0.7354\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.6069 - acc: 0.6767 - val_loss: 0.5325 - val_acc: 0.7485\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.6018 - acc: 0.6835 - val_loss: 0.5289 - val_acc: 0.7530\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5973 - acc: 0.6902 - val_loss: 0.5190 - val_acc: 0.7606\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5935 - acc: 0.6934 - val_loss: 0.5134 - val_acc: 0.7680\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5895 - acc: 0.6979 - val_loss: 0.5033 - val_acc: 0.7746\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5862 - acc: 0.7025 - val_loss: 0.4999 - val_acc: 0.7784\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5830 - acc: 0.7074 - val_loss: 0.4895 - val_acc: 0.7863\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5805 - acc: 0.7096 - val_loss: 0.4876 - val_acc: 0.7907\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5775 - acc: 0.7120 - val_loss: 0.4781 - val_acc: 0.7979\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5757 - acc: 0.7127 - val_loss: 0.4763 - val_acc: 0.7999\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5734 - acc: 0.7166 - val_loss: 0.4694 - val_acc: 0.8061\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5704 - acc: 0.7194 - val_loss: 0.4692 - val_acc: 0.8053\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5688 - acc: 0.7224 - val_loss: 0.4624 - val_acc: 0.8131\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5671 - acc: 0.7240 - val_loss: 0.4579 - val_acc: 0.8144\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5660 - acc: 0.7245 - val_loss: 0.4562 - val_acc: 0.8186\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 24us/step - loss: 0.5644 - acc: 0.7250 - val_loss: 0.4535 - val_acc: 0.8199\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5627 - acc: 0.7280 - val_loss: 0.4480 - val_acc: 0.8243\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5619 - acc: 0.7294 - val_loss: 0.4431 - val_acc: 0.8279\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5603 - acc: 0.7304 - val_loss: 0.4445 - val_acc: 0.8285\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5589 - acc: 0.7311 - val_loss: 0.4372 - val_acc: 0.8320\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5569 - acc: 0.7346 - val_loss: 0.4395 - val_acc: 0.8318\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5571 - acc: 0.7336 - val_loss: 0.4472 - val_acc: 0.8296\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5557 - acc: 0.7346 - val_loss: 0.4313 - val_acc: 0.8370\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5542 - acc: 0.7372 - val_loss: 0.4374 - val_acc: 0.8344\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5530 - acc: 0.7381 - val_loss: 0.4381 - val_acc: 0.8344\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5522 - acc: 0.7383 - val_loss: 0.4341 - val_acc: 0.8396\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5516 - acc: 0.7381 - val_loss: 0.4260 - val_acc: 0.8426\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5507 - acc: 0.7396 - val_loss: 0.4293 - val_acc: 0.8396\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5499 - acc: 0.7402 - val_loss: 0.4224 - val_acc: 0.8438\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5490 - acc: 0.7411 - val_loss: 0.4233 - val_acc: 0.8453\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5489 - acc: 0.7405 - val_loss: 0.4291 - val_acc: 0.8441\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5481 - acc: 0.7406 - val_loss: 0.4193 - val_acc: 0.8461\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5468 - acc: 0.7418 - val_loss: 0.4219 - val_acc: 0.8476\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5463 - acc: 0.7418 - val_loss: 0.4234 - val_acc: 0.8463\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5456 - acc: 0.7435 - val_loss: 0.4178 - val_acc: 0.8497\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5451 - acc: 0.7443 - val_loss: 0.4124 - val_acc: 0.8504\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5446 - acc: 0.7433 - val_loss: 0.4202 - val_acc: 0.8495\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5436 - acc: 0.7438 - val_loss: 0.4137 - val_acc: 0.8497\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5427 - acc: 0.7454 - val_loss: 0.4091 - val_acc: 0.8522\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5432 - acc: 0.7448 - val_loss: 0.4097 - val_acc: 0.8488\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5428 - acc: 0.7442 - val_loss: 0.4165 - val_acc: 0.8487\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5413 - acc: 0.7459 - val_loss: 0.4127 - val_acc: 0.8527\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5405 - acc: 0.7475 - val_loss: 0.4134 - val_acc: 0.8497\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5406 - acc: 0.7478 - val_loss: 0.4086 - val_acc: 0.8534\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5405 - acc: 0.7470 - val_loss: 0.4095 - val_acc: 0.8503\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5396 - acc: 0.7472 - val_loss: 0.4106 - val_acc: 0.8539\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5389 - acc: 0.7482 - val_loss: 0.4041 - val_acc: 0.8537\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5385 - acc: 0.7484 - val_loss: 0.4076 - val_acc: 0.8554\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5387 - acc: 0.7472 - val_loss: 0.4087 - val_acc: 0.8539\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5382 - acc: 0.7488 - val_loss: 0.4092 - val_acc: 0.8544\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5378 - acc: 0.7493 - val_loss: 0.4045 - val_acc: 0.8570\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5369 - acc: 0.7501 - val_loss: 0.4096 - val_acc: 0.8530\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5371 - acc: 0.7489 - val_loss: 0.4061 - val_acc: 0.8531\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5363 - acc: 0.7499 - val_loss: 0.4023 - val_acc: 0.8571\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5364 - acc: 0.7508 - val_loss: 0.4088 - val_acc: 0.8539\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5363 - acc: 0.7498 - val_loss: 0.4083 - val_acc: 0.8547\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5355 - acc: 0.7501 - val_loss: 0.3965 - val_acc: 0.8566\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5356 - acc: 0.7502 - val_loss: 0.4052 - val_acc: 0.8564\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5347 - acc: 0.7520 - val_loss: 0.4027 - val_acc: 0.8570\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5340 - acc: 0.7522 - val_loss: 0.4040 - val_acc: 0.8564\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5343 - acc: 0.7504 - val_loss: 0.4033 - val_acc: 0.8549\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5341 - acc: 0.7509 - val_loss: 0.4034 - val_acc: 0.8569\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5346 - acc: 0.7518 - val_loss: 0.3995 - val_acc: 0.8567\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5345 - acc: 0.7514 - val_loss: 0.4021 - val_acc: 0.8556\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5339 - acc: 0.7528 - val_loss: 0.3977 - val_acc: 0.8557\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5346 - acc: 0.7508 - val_loss: 0.4081 - val_acc: 0.8507\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5326 - acc: 0.7534 - val_loss: 0.4006 - val_acc: 0.8570\n",
      "auroc: 0.9201620741693842\n",
      "auprc: 0.8698529990347406\n",
      "auroc: 0.9186390684060631\n",
      "auprc: 0.8669812891612331\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 6s 495us/step - loss: 0.7556 - acc: 0.5428 - val_loss: 0.6819 - val_acc: 0.5955\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.7029 - acc: 0.5722 - val_loss: 0.6600 - val_acc: 0.6187\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6843 - acc: 0.5893 - val_loss: 0.6446 - val_acc: 0.6318\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6696 - acc: 0.6048 - val_loss: 0.6347 - val_acc: 0.6379\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.6570 - acc: 0.6226 - val_loss: 0.6150 - val_acc: 0.6624\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6447 - acc: 0.6316 - val_loss: 0.5934 - val_acc: 0.6816\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6339 - acc: 0.6461 - val_loss: 0.5799 - val_acc: 0.6970\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.6245 - acc: 0.6572 - val_loss: 0.5623 - val_acc: 0.7140\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.6160 - acc: 0.6666 - val_loss: 0.5497 - val_acc: 0.7250\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.6084 - acc: 0.6758 - val_loss: 0.5317 - val_acc: 0.7433\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.6023 - acc: 0.6835 - val_loss: 0.5208 - val_acc: 0.7550\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5968 - acc: 0.6885 - val_loss: 0.5117 - val_acc: 0.7659\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5918 - acc: 0.6944 - val_loss: 0.4963 - val_acc: 0.7761\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5872 - acc: 0.7001 - val_loss: 0.4913 - val_acc: 0.7847\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5836 - acc: 0.7040 - val_loss: 0.4878 - val_acc: 0.7889\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5795 - acc: 0.7085 - val_loss: 0.4723 - val_acc: 0.7990\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5766 - acc: 0.7118 - val_loss: 0.4747 - val_acc: 0.8010\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5737 - acc: 0.7161 - val_loss: 0.4672 - val_acc: 0.8055\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5717 - acc: 0.7179 - val_loss: 0.4619 - val_acc: 0.8124\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5442 - acc: 0.7463 - val_loss: 0.4212 - val_acc: 0.8445\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5437 - acc: 0.7464 - val_loss: 0.4120 - val_acc: 0.8527\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5434 - acc: 0.7458 - val_loss: 0.4163 - val_acc: 0.8484\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5420 - acc: 0.7478 - val_loss: 0.4104 - val_acc: 0.8482\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5424 - acc: 0.7473 - val_loss: 0.4117 - val_acc: 0.8520\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5416 - acc: 0.7482 - val_loss: 0.4055 - val_acc: 0.8554\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5411 - acc: 0.7474 - val_loss: 0.4091 - val_acc: 0.8533\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5394 - acc: 0.7491 - val_loss: 0.4055 - val_acc: 0.8531\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5392 - acc: 0.7504 - val_loss: 0.4128 - val_acc: 0.8515\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5394 - acc: 0.7493 - val_loss: 0.4041 - val_acc: 0.8578\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5399 - acc: 0.7496 - val_loss: 0.4032 - val_acc: 0.8579\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5389 - acc: 0.7496 - val_loss: 0.4028 - val_acc: 0.8576\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5374 - acc: 0.7509 - val_loss: 0.4066 - val_acc: 0.8563\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5365 - acc: 0.7519 - val_loss: 0.4100 - val_acc: 0.8523\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5375 - acc: 0.7499 - val_loss: 0.4103 - val_acc: 0.8484\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5374 - acc: 0.7481 - val_loss: 0.4063 - val_acc: 0.8545\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5355 - acc: 0.7518 - val_loss: 0.4039 - val_acc: 0.8543\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5353 - acc: 0.7511 - val_loss: 0.4072 - val_acc: 0.8536\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5365 - acc: 0.7516 - val_loss: 0.4011 - val_acc: 0.8557\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5348 - acc: 0.7512 - val_loss: 0.4005 - val_acc: 0.8555\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5345 - acc: 0.7531 - val_loss: 0.4041 - val_acc: 0.8561\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5336 - acc: 0.7528 - val_loss: 0.4031 - val_acc: 0.8556\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5337 - acc: 0.7539 - val_loss: 0.3986 - val_acc: 0.8564\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5332 - acc: 0.7533 - val_loss: 0.4079 - val_acc: 0.8526\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5349 - acc: 0.7515 - val_loss: 0.4036 - val_acc: 0.8562\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5323 - acc: 0.7556 - val_loss: 0.3994 - val_acc: 0.8579\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5323 - acc: 0.7543 - val_loss: 0.3951 - val_acc: 0.8559\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5321 - acc: 0.7545 - val_loss: 0.4067 - val_acc: 0.8540\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5322 - acc: 0.7540 - val_loss: 0.3947 - val_acc: 0.8548\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5317 - acc: 0.7539 - val_loss: 0.4049 - val_acc: 0.8521\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5309 - acc: 0.7534 - val_loss: 0.3988 - val_acc: 0.8544\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5314 - acc: 0.7533 - val_loss: 0.4082 - val_acc: 0.8523\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5304 - acc: 0.7537 - val_loss: 0.3999 - val_acc: 0.8563\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5300 - acc: 0.7556 - val_loss: 0.3935 - val_acc: 0.8557\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5306 - acc: 0.7534 - val_loss: 0.3953 - val_acc: 0.8547\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5293 - acc: 0.7553 - val_loss: 0.3948 - val_acc: 0.8559\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5291 - acc: 0.7558 - val_loss: 0.3973 - val_acc: 0.8559\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5291 - acc: 0.7545 - val_loss: 0.4013 - val_acc: 0.8550\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5288 - acc: 0.7550 - val_loss: 0.3975 - val_acc: 0.8544\n",
      "Epoch 80/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5283 - acc: 0.7561 - val_loss: 0.3954 - val_acc: 0.8546\n",
      "Epoch 81/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5302 - acc: 0.7534 - val_loss: 0.4069 - val_acc: 0.8504\n",
      "Epoch 82/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5289 - acc: 0.7555 - val_loss: 0.3999 - val_acc: 0.8529\n",
      "Epoch 83/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5282 - acc: 0.7559 - val_loss: 0.3930 - val_acc: 0.8548\n",
      "Epoch 84/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5297 - acc: 0.7538 - val_loss: 0.3960 - val_acc: 0.8538\n",
      "Epoch 85/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5290 - acc: 0.7544 - val_loss: 0.3924 - val_acc: 0.8544\n",
      "Epoch 86/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5283 - acc: 0.7554 - val_loss: 0.3972 - val_acc: 0.8519\n",
      "Epoch 87/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5270 - acc: 0.7554 - val_loss: 0.3998 - val_acc: 0.8540\n",
      "Epoch 88/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5267 - acc: 0.7568 - val_loss: 0.4024 - val_acc: 0.8487\n",
      "Epoch 89/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5276 - acc: 0.7562 - val_loss: 0.3939 - val_acc: 0.8561\n",
      "Epoch 90/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5279 - acc: 0.7546 - val_loss: 0.3941 - val_acc: 0.8569\n",
      "Epoch 91/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5269 - acc: 0.7565 - val_loss: 0.3950 - val_acc: 0.8571\n",
      "Epoch 92/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5266 - acc: 0.7556 - val_loss: 0.4044 - val_acc: 0.8513\n",
      "Epoch 93/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5281 - acc: 0.7535 - val_loss: 0.3991 - val_acc: 0.8525\n",
      "Epoch 94/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5267 - acc: 0.7562 - val_loss: 0.3962 - val_acc: 0.8551\n",
      "Epoch 95/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5260 - acc: 0.7571 - val_loss: 0.4013 - val_acc: 0.8527\n",
      "auroc: 0.9211385353113002\n",
      "auprc: 0.8724090913211716\n",
      "auroc: 0.9207857859009576\n",
      "auprc: 0.8719457081348857\n"
     ]
    }
   ],
   "source": [
    "for curr_seed in all_seeds: \n",
    "    model, early_stopping_callback, auroc_callback = train_model(model_wrapper = RC_WRAPPER, \n",
    "                                                                 aug = None, \n",
    "                                                                 curr_seed = curr_seed,\n",
    "                                                                 batch_size = 500,\n",
    "                                                                 x = x_train_200, \n",
    "                                                                 y = y_train_200_mutate, \n",
    "                                                                 val_data = (x_val_200, y_val_200))\n",
    "    save_all(filepath = \"general_results_again\", model_arch = \"test_RC_200_auROC\", curr_seed = curr_seed,\n",
    "             callback = auroc_callback, model = model, val_data = (x_test_200, y_test_200))\n",
    "    save_all(filepath = \"general_results_again\", model_arch = \"test_RC_200_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_test_200, y_test_200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 3s 257us/step - loss: 0.7947 - acc: 0.5394 - val_loss: 0.6699 - val_acc: 0.6231\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.7019 - acc: 0.5668 - val_loss: 0.6466 - val_acc: 0.6300\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6790 - acc: 0.5928 - val_loss: 0.6278 - val_acc: 0.6479\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6636 - acc: 0.6108 - val_loss: 0.6034 - val_acc: 0.6760\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6504 - acc: 0.6271 - val_loss: 0.5922 - val_acc: 0.6860\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6392 - acc: 0.6377 - val_loss: 0.5781 - val_acc: 0.7000\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6299 - acc: 0.6510 - val_loss: 0.5626 - val_acc: 0.7174\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6225 - acc: 0.6574 - val_loss: 0.5499 - val_acc: 0.7283\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6159 - acc: 0.6666 - val_loss: 0.5413 - val_acc: 0.7372\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6101 - acc: 0.6708 - val_loss: 0.5344 - val_acc: 0.7421\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6053 - acc: 0.6780 - val_loss: 0.5188 - val_acc: 0.7587\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6000 - acc: 0.6856 - val_loss: 0.5213 - val_acc: 0.7593\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5953 - acc: 0.6903 - val_loss: 0.5058 - val_acc: 0.7714\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5913 - acc: 0.6949 - val_loss: 0.5029 - val_acc: 0.7717\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5873 - acc: 0.6978 - val_loss: 0.4950 - val_acc: 0.7815\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5836 - acc: 0.7040 - val_loss: 0.4872 - val_acc: 0.7898\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5808 - acc: 0.7059 - val_loss: 0.4843 - val_acc: 0.7929\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5775 - acc: 0.7110 - val_loss: 0.4764 - val_acc: 0.7997\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5740 - acc: 0.7129 - val_loss: 0.4675 - val_acc: 0.8048\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5715 - acc: 0.7172 - val_loss: 0.4637 - val_acc: 0.8062\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5688 - acc: 0.7198 - val_loss: 0.4590 - val_acc: 0.8113\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5675 - acc: 0.7213 - val_loss: 0.4589 - val_acc: 0.8131\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5662 - acc: 0.7219 - val_loss: 0.4580 - val_acc: 0.8109\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5636 - acc: 0.7253 - val_loss: 0.4504 - val_acc: 0.8192\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5624 - acc: 0.7258 - val_loss: 0.4471 - val_acc: 0.8214\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5608 - acc: 0.7291 - val_loss: 0.4443 - val_acc: 0.8216\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5590 - acc: 0.7309 - val_loss: 0.4404 - val_acc: 0.8256\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5573 - acc: 0.7321 - val_loss: 0.4371 - val_acc: 0.8290\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5560 - acc: 0.7329 - val_loss: 0.4419 - val_acc: 0.8293\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5553 - acc: 0.7336 - val_loss: 0.4421 - val_acc: 0.8290\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5547 - acc: 0.7328 - val_loss: 0.4417 - val_acc: 0.8303\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5531 - acc: 0.7350 - val_loss: 0.4310 - val_acc: 0.8349\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5522 - acc: 0.7363 - val_loss: 0.4354 - val_acc: 0.8337\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5509 - acc: 0.7381 - val_loss: 0.4272 - val_acc: 0.8367\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5496 - acc: 0.7383 - val_loss: 0.4283 - val_acc: 0.8380\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5487 - acc: 0.7375 - val_loss: 0.4245 - val_acc: 0.8398\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5485 - acc: 0.7398 - val_loss: 0.4289 - val_acc: 0.8364\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5478 - acc: 0.7408 - val_loss: 0.4263 - val_acc: 0.8386\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5477 - acc: 0.7402 - val_loss: 0.4270 - val_acc: 0.8378\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5473 - acc: 0.7400 - val_loss: 0.4172 - val_acc: 0.8419\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5463 - acc: 0.7417 - val_loss: 0.4239 - val_acc: 0.8410\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5463 - acc: 0.7413 - val_loss: 0.4137 - val_acc: 0.8445\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5453 - acc: 0.7425 - val_loss: 0.4211 - val_acc: 0.8437\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5444 - acc: 0.7435 - val_loss: 0.4125 - val_acc: 0.8468\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5436 - acc: 0.7444 - val_loss: 0.4212 - val_acc: 0.8398\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5436 - acc: 0.7445 - val_loss: 0.4144 - val_acc: 0.8457\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5434 - acc: 0.7430 - val_loss: 0.4204 - val_acc: 0.8402\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5429 - acc: 0.7452 - val_loss: 0.4111 - val_acc: 0.8476\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5415 - acc: 0.7456 - val_loss: 0.4133 - val_acc: 0.8456\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5418 - acc: 0.7446 - val_loss: 0.4180 - val_acc: 0.8449\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5411 - acc: 0.7459 - val_loss: 0.4144 - val_acc: 0.8458\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5402 - acc: 0.7459 - val_loss: 0.4159 - val_acc: 0.8472\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5402 - acc: 0.7465 - val_loss: 0.4107 - val_acc: 0.8476\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5410 - acc: 0.7463 - val_loss: 0.4103 - val_acc: 0.8479\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5398 - acc: 0.7474 - val_loss: 0.4139 - val_acc: 0.8457\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5383 - acc: 0.7474 - val_loss: 0.4133 - val_acc: 0.8481\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5386 - acc: 0.7480 - val_loss: 0.4118 - val_acc: 0.8443\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5372 - acc: 0.7495 - val_loss: 0.4083 - val_acc: 0.8514\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5377 - acc: 0.7489 - val_loss: 0.4146 - val_acc: 0.8464\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5369 - acc: 0.7488 - val_loss: 0.4095 - val_acc: 0.8512\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5369 - acc: 0.7488 - val_loss: 0.4056 - val_acc: 0.8482\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5378 - acc: 0.7484 - val_loss: 0.4120 - val_acc: 0.8465\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5364 - acc: 0.7506 - val_loss: 0.4071 - val_acc: 0.8505\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5350 - acc: 0.7511 - val_loss: 0.4097 - val_acc: 0.8480\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5355 - acc: 0.7501 - val_loss: 0.4054 - val_acc: 0.8526\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5356 - acc: 0.7509 - val_loss: 0.4018 - val_acc: 0.8524\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5352 - acc: 0.7503 - val_loss: 0.4033 - val_acc: 0.8534\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5356 - acc: 0.7516 - val_loss: 0.3972 - val_acc: 0.8531\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5349 - acc: 0.7508 - val_loss: 0.4102 - val_acc: 0.8505\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5351 - acc: 0.7489 - val_loss: 0.4081 - val_acc: 0.8499\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5338 - acc: 0.7510 - val_loss: 0.4002 - val_acc: 0.8493\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5333 - acc: 0.7523 - val_loss: 0.4068 - val_acc: 0.8480\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5333 - acc: 0.7522 - val_loss: 0.4010 - val_acc: 0.8529\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5337 - acc: 0.7521 - val_loss: 0.4063 - val_acc: 0.8499\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5332 - acc: 0.7514 - val_loss: 0.4048 - val_acc: 0.8503\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5323 - acc: 0.7528 - val_loss: 0.4092 - val_acc: 0.8479\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5319 - acc: 0.7526 - val_loss: 0.4006 - val_acc: 0.8533\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5310 - acc: 0.7532 - val_loss: 0.4047 - val_acc: 0.8512\n",
      "auroc: 0.9154753532815527\n",
      "auprc: 0.8644280611552991\n",
      "auroc: 0.9152591512359344\n",
      "auprc: 0.8636324022278239\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 3s 270us/step - loss: 0.7555 - acc: 0.5449 - val_loss: 0.6699 - val_acc: 0.6185\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.7017 - acc: 0.5734 - val_loss: 0.6651 - val_acc: 0.6038\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6835 - acc: 0.5868 - val_loss: 0.6440 - val_acc: 0.6291\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6682 - acc: 0.6021 - val_loss: 0.6268 - val_acc: 0.6464\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6550 - acc: 0.6159 - val_loss: 0.6096 - val_acc: 0.6646\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6426 - acc: 0.6312 - val_loss: 0.5956 - val_acc: 0.6785\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6318 - acc: 0.6439 - val_loss: 0.5706 - val_acc: 0.7077\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6220 - acc: 0.6534 - val_loss: 0.5456 - val_acc: 0.7259\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6142 - acc: 0.6623 - val_loss: 0.5351 - val_acc: 0.7365\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6075 - acc: 0.6699 - val_loss: 0.5244 - val_acc: 0.7481\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6013 - acc: 0.6784 - val_loss: 0.5162 - val_acc: 0.7527\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5968 - acc: 0.6845 - val_loss: 0.5055 - val_acc: 0.7643\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5920 - acc: 0.6910 - val_loss: 0.4995 - val_acc: 0.7730\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5884 - acc: 0.6958 - val_loss: 0.4924 - val_acc: 0.7779\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5848 - acc: 0.6989 - val_loss: 0.4884 - val_acc: 0.7831\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5811 - acc: 0.7039 - val_loss: 0.4855 - val_acc: 0.7878\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5789 - acc: 0.7075 - val_loss: 0.4745 - val_acc: 0.7960\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5752 - acc: 0.7113 - val_loss: 0.4729 - val_acc: 0.8000\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5733 - acc: 0.7135 - val_loss: 0.4604 - val_acc: 0.8060\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5707 - acc: 0.7157 - val_loss: 0.4625 - val_acc: 0.8101\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5680 - acc: 0.7198 - val_loss: 0.4572 - val_acc: 0.8134\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5667 - acc: 0.7208 - val_loss: 0.4589 - val_acc: 0.8158\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5645 - acc: 0.7235 - val_loss: 0.4537 - val_acc: 0.8181\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5633 - acc: 0.7252 - val_loss: 0.4504 - val_acc: 0.8222\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5615 - acc: 0.7280 - val_loss: 0.4447 - val_acc: 0.8259\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5604 - acc: 0.7290 - val_loss: 0.4392 - val_acc: 0.8277\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5586 - acc: 0.7299 - val_loss: 0.4447 - val_acc: 0.8273\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5581 - acc: 0.7311 - val_loss: 0.4409 - val_acc: 0.8310\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5572 - acc: 0.7328 - val_loss: 0.4357 - val_acc: 0.8312\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5553 - acc: 0.7338 - val_loss: 0.4409 - val_acc: 0.8321\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5538 - acc: 0.7354 - val_loss: 0.4282 - val_acc: 0.8405\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5522 - acc: 0.7369 - val_loss: 0.4237 - val_acc: 0.8442\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5514 - acc: 0.7374 - val_loss: 0.4321 - val_acc: 0.8394\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5505 - acc: 0.7390 - val_loss: 0.4226 - val_acc: 0.8459\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5510 - acc: 0.7384 - val_loss: 0.4185 - val_acc: 0.8486\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5495 - acc: 0.7389 - val_loss: 0.4159 - val_acc: 0.8498\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5479 - acc: 0.7414 - val_loss: 0.4184 - val_acc: 0.8482\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5480 - acc: 0.7411 - val_loss: 0.4132 - val_acc: 0.8515\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5476 - acc: 0.7410 - val_loss: 0.4237 - val_acc: 0.8449\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5458 - acc: 0.7420 - val_loss: 0.4144 - val_acc: 0.8506\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5450 - acc: 0.7428 - val_loss: 0.4141 - val_acc: 0.8526\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5439 - acc: 0.7436 - val_loss: 0.4111 - val_acc: 0.8543\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5440 - acc: 0.7422 - val_loss: 0.4101 - val_acc: 0.8511\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5435 - acc: 0.7448 - val_loss: 0.4126 - val_acc: 0.8546\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5429 - acc: 0.7445 - val_loss: 0.4164 - val_acc: 0.8509\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5419 - acc: 0.7449 - val_loss: 0.4062 - val_acc: 0.8568\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5420 - acc: 0.7468 - val_loss: 0.4066 - val_acc: 0.8562\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5414 - acc: 0.7465 - val_loss: 0.4050 - val_acc: 0.8540\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5401 - acc: 0.7469 - val_loss: 0.4095 - val_acc: 0.8562\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5395 - acc: 0.7471 - val_loss: 0.4112 - val_acc: 0.8524\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5392 - acc: 0.7479 - val_loss: 0.4042 - val_acc: 0.8576\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5401 - acc: 0.7475 - val_loss: 0.4004 - val_acc: 0.8584\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5388 - acc: 0.7486 - val_loss: 0.3988 - val_acc: 0.8594\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5385 - acc: 0.7495 - val_loss: 0.4084 - val_acc: 0.8527\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5383 - acc: 0.7488 - val_loss: 0.3993 - val_acc: 0.8581\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5375 - acc: 0.7481 - val_loss: 0.4050 - val_acc: 0.8523\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5376 - acc: 0.7481 - val_loss: 0.4070 - val_acc: 0.8553\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5369 - acc: 0.7500 - val_loss: 0.3978 - val_acc: 0.8598\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5368 - acc: 0.7506 - val_loss: 0.4044 - val_acc: 0.8570\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5370 - acc: 0.7493 - val_loss: 0.3989 - val_acc: 0.8580\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5361 - acc: 0.7508 - val_loss: 0.3943 - val_acc: 0.8572\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5359 - acc: 0.7504 - val_loss: 0.4042 - val_acc: 0.8580\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5355 - acc: 0.7506 - val_loss: 0.3969 - val_acc: 0.8597\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5352 - acc: 0.7499 - val_loss: 0.4028 - val_acc: 0.8539\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5359 - acc: 0.7506 - val_loss: 0.4014 - val_acc: 0.8591\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5352 - acc: 0.7516 - val_loss: 0.4023 - val_acc: 0.8569\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5338 - acc: 0.7507 - val_loss: 0.3955 - val_acc: 0.8601\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5337 - acc: 0.7508 - val_loss: 0.3975 - val_acc: 0.8576\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5328 - acc: 0.7525 - val_loss: 0.3943 - val_acc: 0.8597\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5330 - acc: 0.7518 - val_loss: 0.3976 - val_acc: 0.8589\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5329 - acc: 0.7523 - val_loss: 0.4025 - val_acc: 0.8565\n",
      "auroc: 0.9237448984252318\n",
      "auprc: 0.8765274798423593\n",
      "auroc: 0.9236327738000142\n",
      "auprc: 0.8762825218186014\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 3s 279us/step - loss: 0.7595 - acc: 0.5501 - val_loss: 0.6825 - val_acc: 0.5910\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6983 - acc: 0.5711 - val_loss: 0.6553 - val_acc: 0.6206\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6774 - acc: 0.5940 - val_loss: 0.6349 - val_acc: 0.6404\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6628 - acc: 0.6079 - val_loss: 0.6225 - val_acc: 0.6503\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6514 - acc: 0.6236 - val_loss: 0.6049 - val_acc: 0.6700\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6411 - acc: 0.6361 - val_loss: 0.5917 - val_acc: 0.6865\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6325 - acc: 0.6471 - val_loss: 0.5759 - val_acc: 0.7017\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6254 - acc: 0.6549 - val_loss: 0.5608 - val_acc: 0.7149\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6188 - acc: 0.6619 - val_loss: 0.5563 - val_acc: 0.7216\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6131 - acc: 0.6679 - val_loss: 0.5392 - val_acc: 0.7365\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6072 - acc: 0.6753 - val_loss: 0.5308 - val_acc: 0.7440\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6022 - acc: 0.6816 - val_loss: 0.5221 - val_acc: 0.7522\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5980 - acc: 0.6863 - val_loss: 0.5191 - val_acc: 0.7553\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5934 - acc: 0.6921 - val_loss: 0.5052 - val_acc: 0.7697\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5898 - acc: 0.6953 - val_loss: 0.4967 - val_acc: 0.7746\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5861 - acc: 0.6994 - val_loss: 0.5004 - val_acc: 0.7767\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5828 - acc: 0.7035 - val_loss: 0.4839 - val_acc: 0.7883\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5799 - acc: 0.7068 - val_loss: 0.4812 - val_acc: 0.7914\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5766 - acc: 0.7106 - val_loss: 0.4751 - val_acc: 0.7996\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5754 - acc: 0.7111 - val_loss: 0.4689 - val_acc: 0.8049\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5717 - acc: 0.7156 - val_loss: 0.4692 - val_acc: 0.8056\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5701 - acc: 0.7163 - val_loss: 0.4568 - val_acc: 0.8107\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5686 - acc: 0.7183 - val_loss: 0.4533 - val_acc: 0.8170\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5666 - acc: 0.7214 - val_loss: 0.4520 - val_acc: 0.8206\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5649 - acc: 0.7226 - val_loss: 0.4525 - val_acc: 0.8179\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5635 - acc: 0.7243 - val_loss: 0.4466 - val_acc: 0.8237\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5614 - acc: 0.7265 - val_loss: 0.4420 - val_acc: 0.8265\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5600 - acc: 0.7272 - val_loss: 0.4433 - val_acc: 0.8260\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5588 - acc: 0.7294 - val_loss: 0.4445 - val_acc: 0.8297\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5574 - acc: 0.7314 - val_loss: 0.4383 - val_acc: 0.8313\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5560 - acc: 0.7324 - val_loss: 0.4350 - val_acc: 0.8331\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5557 - acc: 0.7325 - val_loss: 0.4303 - val_acc: 0.8345\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5542 - acc: 0.7345 - val_loss: 0.4334 - val_acc: 0.8338\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5531 - acc: 0.7352 - val_loss: 0.4312 - val_acc: 0.8388\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5517 - acc: 0.7359 - val_loss: 0.4236 - val_acc: 0.8400\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5514 - acc: 0.7367 - val_loss: 0.4366 - val_acc: 0.8352\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5516 - acc: 0.7356 - val_loss: 0.4216 - val_acc: 0.8431\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5497 - acc: 0.7386 - val_loss: 0.4261 - val_acc: 0.8439\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5495 - acc: 0.7382 - val_loss: 0.4161 - val_acc: 0.8445\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5484 - acc: 0.7386 - val_loss: 0.4212 - val_acc: 0.8450\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5495 - acc: 0.7389 - val_loss: 0.4317 - val_acc: 0.8352\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5482 - acc: 0.7411 - val_loss: 0.4235 - val_acc: 0.8437\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5469 - acc: 0.7406 - val_loss: 0.4237 - val_acc: 0.8454\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5470 - acc: 0.7422 - val_loss: 0.4189 - val_acc: 0.8476\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5457 - acc: 0.7436 - val_loss: 0.4159 - val_acc: 0.8486\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5449 - acc: 0.7436 - val_loss: 0.4168 - val_acc: 0.8467\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5462 - acc: 0.7403 - val_loss: 0.4151 - val_acc: 0.8460\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5454 - acc: 0.7426 - val_loss: 0.4105 - val_acc: 0.8466\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5431 - acc: 0.7457 - val_loss: 0.4160 - val_acc: 0.8477\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5424 - acc: 0.7453 - val_loss: 0.4151 - val_acc: 0.8478\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5423 - acc: 0.7448 - val_loss: 0.4185 - val_acc: 0.8479\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5423 - acc: 0.7458 - val_loss: 0.4131 - val_acc: 0.8475\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5419 - acc: 0.7464 - val_loss: 0.4128 - val_acc: 0.8485\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5408 - acc: 0.7473 - val_loss: 0.4142 - val_acc: 0.8485\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5404 - acc: 0.7470 - val_loss: 0.4140 - val_acc: 0.8477\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5407 - acc: 0.7464 - val_loss: 0.4138 - val_acc: 0.8488\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5397 - acc: 0.7482 - val_loss: 0.4101 - val_acc: 0.8501\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5395 - acc: 0.7485 - val_loss: 0.4197 - val_acc: 0.8449\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5401 - acc: 0.7468 - val_loss: 0.4112 - val_acc: 0.8464\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5386 - acc: 0.7489 - val_loss: 0.4139 - val_acc: 0.8482\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5383 - acc: 0.7494 - val_loss: 0.4115 - val_acc: 0.8499\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5377 - acc: 0.7484 - val_loss: 0.4084 - val_acc: 0.8495\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5379 - acc: 0.7492 - val_loss: 0.4047 - val_acc: 0.8516\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5377 - acc: 0.7498 - val_loss: 0.4009 - val_acc: 0.8516\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5374 - acc: 0.7500 - val_loss: 0.4071 - val_acc: 0.8516\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5367 - acc: 0.7499 - val_loss: 0.4059 - val_acc: 0.8514\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5357 - acc: 0.7511 - val_loss: 0.4073 - val_acc: 0.8511\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5355 - acc: 0.7517 - val_loss: 0.4042 - val_acc: 0.8507\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5360 - acc: 0.7511 - val_loss: 0.4046 - val_acc: 0.8528\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5357 - acc: 0.7516 - val_loss: 0.4140 - val_acc: 0.8443\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5348 - acc: 0.7507 - val_loss: 0.4080 - val_acc: 0.8515\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5352 - acc: 0.7518 - val_loss: 0.4094 - val_acc: 0.8506\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5345 - acc: 0.7521 - val_loss: 0.4020 - val_acc: 0.8538\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5345 - acc: 0.7526 - val_loss: 0.4063 - val_acc: 0.8509\n",
      "auroc: 0.9157604278921192\n",
      "auprc: 0.8652315205770725\n",
      "auroc: 0.915235360160728\n",
      "auprc: 0.8639365872979345\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 3s 291us/step - loss: 0.8237 - acc: 0.5386 - val_loss: 0.6955 - val_acc: 0.5949\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.7010 - acc: 0.5714 - val_loss: 0.6545 - val_acc: 0.6204\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6783 - acc: 0.5970 - val_loss: 0.6349 - val_acc: 0.6412\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6635 - acc: 0.6120 - val_loss: 0.6249 - val_acc: 0.6474\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6515 - acc: 0.6244 - val_loss: 0.5983 - val_acc: 0.6795\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6404 - acc: 0.6364 - val_loss: 0.5795 - val_acc: 0.6974\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6300 - acc: 0.6509 - val_loss: 0.5680 - val_acc: 0.7084\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6217 - acc: 0.6612 - val_loss: 0.5525 - val_acc: 0.7240\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6148 - acc: 0.6689 - val_loss: 0.5364 - val_acc: 0.7427\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6082 - acc: 0.6766 - val_loss: 0.5238 - val_acc: 0.7544\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6024 - acc: 0.6836 - val_loss: 0.5191 - val_acc: 0.7607\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5976 - acc: 0.6874 - val_loss: 0.5072 - val_acc: 0.7701\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5927 - acc: 0.6925 - val_loss: 0.4954 - val_acc: 0.7792\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5887 - acc: 0.6967 - val_loss: 0.4911 - val_acc: 0.7849\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5854 - acc: 0.7001 - val_loss: 0.4777 - val_acc: 0.7904\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5828 - acc: 0.7030 - val_loss: 0.4801 - val_acc: 0.7957\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5795 - acc: 0.7074 - val_loss: 0.4691 - val_acc: 0.8019\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5770 - acc: 0.7087 - val_loss: 0.4645 - val_acc: 0.8070\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5747 - acc: 0.7113 - val_loss: 0.4669 - val_acc: 0.8077\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5727 - acc: 0.7136 - val_loss: 0.4622 - val_acc: 0.8123\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5706 - acc: 0.7163 - val_loss: 0.4587 - val_acc: 0.8160\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5687 - acc: 0.7176 - val_loss: 0.4548 - val_acc: 0.8166\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5668 - acc: 0.7211 - val_loss: 0.4567 - val_acc: 0.8187\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5645 - acc: 0.7228 - val_loss: 0.4520 - val_acc: 0.8216\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5636 - acc: 0.7252 - val_loss: 0.4476 - val_acc: 0.8249\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5621 - acc: 0.7254 - val_loss: 0.4415 - val_acc: 0.8290\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5601 - acc: 0.7276 - val_loss: 0.4447 - val_acc: 0.8276\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5587 - acc: 0.7294 - val_loss: 0.4406 - val_acc: 0.8320\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5570 - acc: 0.7305 - val_loss: 0.4351 - val_acc: 0.8344\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5565 - acc: 0.7319 - val_loss: 0.4293 - val_acc: 0.8373\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5551 - acc: 0.7325 - val_loss: 0.4382 - val_acc: 0.8352\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5541 - acc: 0.7340 - val_loss: 0.4257 - val_acc: 0.8403\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5527 - acc: 0.7353 - val_loss: 0.4256 - val_acc: 0.8413\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5518 - acc: 0.7368 - val_loss: 0.4283 - val_acc: 0.8423\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5505 - acc: 0.7381 - val_loss: 0.4278 - val_acc: 0.8421\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5502 - acc: 0.7380 - val_loss: 0.4298 - val_acc: 0.8413\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5493 - acc: 0.7399 - val_loss: 0.4221 - val_acc: 0.8448\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 23us/step - loss: 0.5480 - acc: 0.7404 - val_loss: 0.4198 - val_acc: 0.8457\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5478 - acc: 0.7405 - val_loss: 0.4194 - val_acc: 0.8480\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5470 - acc: 0.7423 - val_loss: 0.4116 - val_acc: 0.8508\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5459 - acc: 0.7429 - val_loss: 0.4141 - val_acc: 0.8517\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5454 - acc: 0.7430 - val_loss: 0.4144 - val_acc: 0.8511\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5447 - acc: 0.7434 - val_loss: 0.4146 - val_acc: 0.8506\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5442 - acc: 0.7432 - val_loss: 0.4087 - val_acc: 0.8485\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5443 - acc: 0.7421 - val_loss: 0.4168 - val_acc: 0.8513\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5424 - acc: 0.7460 - val_loss: 0.4132 - val_acc: 0.8532\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5418 - acc: 0.7450 - val_loss: 0.4165 - val_acc: 0.8518\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5421 - acc: 0.7448 - val_loss: 0.4157 - val_acc: 0.8512\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5411 - acc: 0.7463 - val_loss: 0.4068 - val_acc: 0.8556\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5404 - acc: 0.7462 - val_loss: 0.4031 - val_acc: 0.8566\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5406 - acc: 0.7471 - val_loss: 0.4167 - val_acc: 0.8454\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5404 - acc: 0.7466 - val_loss: 0.4071 - val_acc: 0.8556\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5387 - acc: 0.7486 - val_loss: 0.4023 - val_acc: 0.8571\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5384 - acc: 0.7493 - val_loss: 0.4072 - val_acc: 0.8559\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5384 - acc: 0.7483 - val_loss: 0.4050 - val_acc: 0.8554\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5371 - acc: 0.7485 - val_loss: 0.4060 - val_acc: 0.8566\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5380 - acc: 0.7484 - val_loss: 0.4038 - val_acc: 0.8569\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5382 - acc: 0.7475 - val_loss: 0.4150 - val_acc: 0.8524\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5370 - acc: 0.7513 - val_loss: 0.4080 - val_acc: 0.8539\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5367 - acc: 0.7491 - val_loss: 0.4001 - val_acc: 0.8573\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5358 - acc: 0.7503 - val_loss: 0.3961 - val_acc: 0.8582\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5354 - acc: 0.7498 - val_loss: 0.3993 - val_acc: 0.8584\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5349 - acc: 0.7511 - val_loss: 0.4053 - val_acc: 0.8563\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5339 - acc: 0.7515 - val_loss: 0.4001 - val_acc: 0.8579\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5339 - acc: 0.7510 - val_loss: 0.3983 - val_acc: 0.8570\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5334 - acc: 0.7517 - val_loss: 0.4002 - val_acc: 0.8576\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5326 - acc: 0.7531 - val_loss: 0.4041 - val_acc: 0.8551\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5323 - acc: 0.7536 - val_loss: 0.3950 - val_acc: 0.8590\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5320 - acc: 0.7544 - val_loss: 0.3985 - val_acc: 0.8583\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5324 - acc: 0.7544 - val_loss: 0.4039 - val_acc: 0.8553\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5320 - acc: 0.7539 - val_loss: 0.4018 - val_acc: 0.8534\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5324 - acc: 0.7537 - val_loss: 0.3904 - val_acc: 0.8600\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5317 - acc: 0.7534 - val_loss: 0.4015 - val_acc: 0.8573\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5314 - acc: 0.7558 - val_loss: 0.3936 - val_acc: 0.8585\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5307 - acc: 0.7559 - val_loss: 0.3988 - val_acc: 0.8580\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5305 - acc: 0.7549 - val_loss: 0.3993 - val_acc: 0.8580\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5294 - acc: 0.7563 - val_loss: 0.3975 - val_acc: 0.8581\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5299 - acc: 0.7551 - val_loss: 0.3943 - val_acc: 0.8602\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5304 - acc: 0.7549 - val_loss: 0.3966 - val_acc: 0.8589\n",
      "Epoch 80/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5309 - acc: 0.7551 - val_loss: 0.3964 - val_acc: 0.8560\n",
      "Epoch 81/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5294 - acc: 0.7556 - val_loss: 0.3993 - val_acc: 0.8546\n",
      "Epoch 82/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5295 - acc: 0.7543 - val_loss: 0.3985 - val_acc: 0.8561\n",
      "auroc: 0.9209825696316539\n",
      "auprc: 0.8724711720281418\n",
      "auroc: 0.920898323382327\n",
      "auprc: 0.8720166700870052\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 4s 308us/step - loss: 0.8284 - acc: 0.5364 - val_loss: 0.6906 - val_acc: 0.6074\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.7072 - acc: 0.5707 - val_loss: 0.6587 - val_acc: 0.6146\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6827 - acc: 0.5901 - val_loss: 0.6416 - val_acc: 0.6334\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6680 - acc: 0.6036 - val_loss: 0.6197 - val_acc: 0.6583\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6556 - acc: 0.6162 - val_loss: 0.6116 - val_acc: 0.6644\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6454 - acc: 0.6314 - val_loss: 0.5887 - val_acc: 0.6899\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6362 - acc: 0.6418 - val_loss: 0.5799 - val_acc: 0.6968\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6274 - acc: 0.6503 - val_loss: 0.5652 - val_acc: 0.7117\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6198 - acc: 0.6594 - val_loss: 0.5551 - val_acc: 0.7220\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6133 - acc: 0.6683 - val_loss: 0.5475 - val_acc: 0.7299\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6077 - acc: 0.6747 - val_loss: 0.5331 - val_acc: 0.7431\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6026 - acc: 0.6802 - val_loss: 0.5187 - val_acc: 0.7547\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5981 - acc: 0.6864 - val_loss: 0.5158 - val_acc: 0.7600\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5934 - acc: 0.6919 - val_loss: 0.5091 - val_acc: 0.7681\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5896 - acc: 0.6959 - val_loss: 0.5013 - val_acc: 0.7758\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5861 - acc: 0.6996 - val_loss: 0.4906 - val_acc: 0.7818\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5830 - acc: 0.7049 - val_loss: 0.4897 - val_acc: 0.7847\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5807 - acc: 0.7077 - val_loss: 0.4868 - val_acc: 0.7888\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5769 - acc: 0.7091 - val_loss: 0.4773 - val_acc: 0.7983\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5742 - acc: 0.7121 - val_loss: 0.4747 - val_acc: 0.8018\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5720 - acc: 0.7161 - val_loss: 0.4636 - val_acc: 0.8040\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5700 - acc: 0.7181 - val_loss: 0.4673 - val_acc: 0.8023\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5684 - acc: 0.7186 - val_loss: 0.4644 - val_acc: 0.8067\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5658 - acc: 0.7213 - val_loss: 0.4528 - val_acc: 0.8138\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5642 - acc: 0.7220 - val_loss: 0.4577 - val_acc: 0.8154\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5622 - acc: 0.7259 - val_loss: 0.4518 - val_acc: 0.8203\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5607 - acc: 0.7286 - val_loss: 0.4431 - val_acc: 0.8229\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5597 - acc: 0.7271 - val_loss: 0.4486 - val_acc: 0.8240\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5578 - acc: 0.7311 - val_loss: 0.4435 - val_acc: 0.8267\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5564 - acc: 0.7326 - val_loss: 0.4369 - val_acc: 0.8296\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5553 - acc: 0.7338 - val_loss: 0.4382 - val_acc: 0.8304\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5537 - acc: 0.7352 - val_loss: 0.4388 - val_acc: 0.8313\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5530 - acc: 0.7347 - val_loss: 0.4394 - val_acc: 0.8319\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5518 - acc: 0.7361 - val_loss: 0.4353 - val_acc: 0.8264\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5523 - acc: 0.7356 - val_loss: 0.4320 - val_acc: 0.8347\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5504 - acc: 0.7387 - val_loss: 0.4267 - val_acc: 0.8380\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5488 - acc: 0.7400 - val_loss: 0.4286 - val_acc: 0.8385\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5487 - acc: 0.7403 - val_loss: 0.4284 - val_acc: 0.8364\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5478 - acc: 0.7389 - val_loss: 0.4197 - val_acc: 0.8420\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5476 - acc: 0.7393 - val_loss: 0.4224 - val_acc: 0.8402\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5466 - acc: 0.7417 - val_loss: 0.4281 - val_acc: 0.8374\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5455 - acc: 0.7422 - val_loss: 0.4207 - val_acc: 0.8446\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5447 - acc: 0.7429 - val_loss: 0.4193 - val_acc: 0.8446\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5434 - acc: 0.7439 - val_loss: 0.4205 - val_acc: 0.8447\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5431 - acc: 0.7449 - val_loss: 0.4163 - val_acc: 0.8452\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5428 - acc: 0.7444 - val_loss: 0.4234 - val_acc: 0.8426\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5421 - acc: 0.7454 - val_loss: 0.4147 - val_acc: 0.8474\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5413 - acc: 0.7453 - val_loss: 0.4173 - val_acc: 0.8453\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5410 - acc: 0.7461 - val_loss: 0.4071 - val_acc: 0.8479\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5410 - acc: 0.7437 - val_loss: 0.4204 - val_acc: 0.8463\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5393 - acc: 0.7460 - val_loss: 0.4110 - val_acc: 0.8478\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5403 - acc: 0.7447 - val_loss: 0.4159 - val_acc: 0.8458\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5392 - acc: 0.7479 - val_loss: 0.4101 - val_acc: 0.8493\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5380 - acc: 0.7492 - val_loss: 0.4083 - val_acc: 0.8511\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5375 - acc: 0.7484 - val_loss: 0.4139 - val_acc: 0.8477\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5373 - acc: 0.7491 - val_loss: 0.4090 - val_acc: 0.8505\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5361 - acc: 0.7500 - val_loss: 0.4079 - val_acc: 0.8524\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5366 - acc: 0.7502 - val_loss: 0.4068 - val_acc: 0.8514\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5357 - acc: 0.7499 - val_loss: 0.4032 - val_acc: 0.8517\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5353 - acc: 0.7497 - val_loss: 0.4056 - val_acc: 0.8527\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5350 - acc: 0.7507 - val_loss: 0.4122 - val_acc: 0.8507\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5354 - acc: 0.7503 - val_loss: 0.4041 - val_acc: 0.8538\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5338 - acc: 0.7510 - val_loss: 0.4106 - val_acc: 0.8506\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5341 - acc: 0.7509 - val_loss: 0.4059 - val_acc: 0.8530\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5339 - acc: 0.7508 - val_loss: 0.4028 - val_acc: 0.8548\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5336 - acc: 0.7511 - val_loss: 0.4020 - val_acc: 0.8537\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5330 - acc: 0.7514 - val_loss: 0.4045 - val_acc: 0.8529\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5332 - acc: 0.7518 - val_loss: 0.4058 - val_acc: 0.8514\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5322 - acc: 0.7526 - val_loss: 0.4003 - val_acc: 0.8550\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5319 - acc: 0.7520 - val_loss: 0.4051 - val_acc: 0.8530\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5318 - acc: 0.7523 - val_loss: 0.3974 - val_acc: 0.8542\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5313 - acc: 0.7528 - val_loss: 0.4033 - val_acc: 0.8551\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5306 - acc: 0.7535 - val_loss: 0.4038 - val_acc: 0.8552\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5307 - acc: 0.7539 - val_loss: 0.4027 - val_acc: 0.8514\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5314 - acc: 0.7528 - val_loss: 0.3964 - val_acc: 0.8558\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5315 - acc: 0.7531 - val_loss: 0.4034 - val_acc: 0.8517\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5304 - acc: 0.7527 - val_loss: 0.4007 - val_acc: 0.8544\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5295 - acc: 0.7544 - val_loss: 0.4042 - val_acc: 0.8509\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5309 - acc: 0.7534 - val_loss: 0.4104 - val_acc: 0.8483\n",
      "Epoch 80/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5294 - acc: 0.7534 - val_loss: 0.4019 - val_acc: 0.8537\n",
      "Epoch 81/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5297 - acc: 0.7537 - val_loss: 0.4029 - val_acc: 0.8534\n",
      "Epoch 82/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5286 - acc: 0.7556 - val_loss: 0.3988 - val_acc: 0.8541\n",
      "Epoch 83/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5280 - acc: 0.7551 - val_loss: 0.3968 - val_acc: 0.8553\n",
      "Epoch 84/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5280 - acc: 0.7558 - val_loss: 0.4010 - val_acc: 0.8537\n",
      "Epoch 85/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5278 - acc: 0.7560 - val_loss: 0.3990 - val_acc: 0.8543\n",
      "auroc: 0.9207649142572633\n",
      "auprc: 0.873727196615528\n",
      "auroc: 0.920762376345993\n",
      "auprc: 0.8732827797898116\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 4s 318us/step - loss: 0.7774 - acc: 0.5445 - val_loss: 0.6541 - val_acc: 0.6404\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6967 - acc: 0.5824 - val_loss: 0.6392 - val_acc: 0.6391\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6756 - acc: 0.6004 - val_loss: 0.6183 - val_acc: 0.6593\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6609 - acc: 0.6169 - val_loss: 0.5992 - val_acc: 0.6827\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6496 - acc: 0.6286 - val_loss: 0.5920 - val_acc: 0.6854\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6398 - acc: 0.6404 - val_loss: 0.5778 - val_acc: 0.6993\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6311 - acc: 0.6500 - val_loss: 0.5646 - val_acc: 0.7146\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6236 - acc: 0.6581 - val_loss: 0.5615 - val_acc: 0.7169\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6173 - acc: 0.6670 - val_loss: 0.5465 - val_acc: 0.7254\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6122 - acc: 0.6709 - val_loss: 0.5381 - val_acc: 0.7352\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6063 - acc: 0.6790 - val_loss: 0.5293 - val_acc: 0.7458\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6016 - acc: 0.6826 - val_loss: 0.5154 - val_acc: 0.7566\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5977 - acc: 0.6869 - val_loss: 0.5117 - val_acc: 0.7605\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5937 - acc: 0.6910 - val_loss: 0.5006 - val_acc: 0.7686\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5909 - acc: 0.6936 - val_loss: 0.4968 - val_acc: 0.7753\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5873 - acc: 0.6976 - val_loss: 0.4928 - val_acc: 0.7787\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5849 - acc: 0.7021 - val_loss: 0.4822 - val_acc: 0.7855\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5819 - acc: 0.7051 - val_loss: 0.4860 - val_acc: 0.7872\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5792 - acc: 0.7082 - val_loss: 0.4805 - val_acc: 0.7917\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5767 - acc: 0.7102 - val_loss: 0.4772 - val_acc: 0.7959\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5747 - acc: 0.7129 - val_loss: 0.4692 - val_acc: 0.8009\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5722 - acc: 0.7159 - val_loss: 0.4653 - val_acc: 0.8040\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5704 - acc: 0.7182 - val_loss: 0.4646 - val_acc: 0.8053\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5687 - acc: 0.7203 - val_loss: 0.4624 - val_acc: 0.8088\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5667 - acc: 0.7221 - val_loss: 0.4560 - val_acc: 0.8126\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5657 - acc: 0.7238 - val_loss: 0.4532 - val_acc: 0.8153\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5640 - acc: 0.7257 - val_loss: 0.4598 - val_acc: 0.8131\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5629 - acc: 0.7264 - val_loss: 0.4497 - val_acc: 0.8172\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5616 - acc: 0.7285 - val_loss: 0.4534 - val_acc: 0.8170\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5606 - acc: 0.7293 - val_loss: 0.4478 - val_acc: 0.8210\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5593 - acc: 0.7317 - val_loss: 0.4455 - val_acc: 0.8241\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5586 - acc: 0.7304 - val_loss: 0.4454 - val_acc: 0.8235\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5579 - acc: 0.7317 - val_loss: 0.4384 - val_acc: 0.8256\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5560 - acc: 0.7340 - val_loss: 0.4358 - val_acc: 0.8287\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5562 - acc: 0.7334 - val_loss: 0.4460 - val_acc: 0.8201\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5547 - acc: 0.7364 - val_loss: 0.4393 - val_acc: 0.8301\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5534 - acc: 0.7377 - val_loss: 0.4381 - val_acc: 0.8283\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5536 - acc: 0.7361 - val_loss: 0.4291 - val_acc: 0.8347\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5520 - acc: 0.7386 - val_loss: 0.4306 - val_acc: 0.8346\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5516 - acc: 0.7372 - val_loss: 0.4301 - val_acc: 0.8336\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5505 - acc: 0.7392 - val_loss: 0.4313 - val_acc: 0.8337\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5497 - acc: 0.7399 - val_loss: 0.4273 - val_acc: 0.8359\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5496 - acc: 0.7400 - val_loss: 0.4295 - val_acc: 0.8357\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5501 - acc: 0.7404 - val_loss: 0.4270 - val_acc: 0.8387\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5484 - acc: 0.7399 - val_loss: 0.4266 - val_acc: 0.8360\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5480 - acc: 0.7411 - val_loss: 0.4255 - val_acc: 0.8390\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5467 - acc: 0.7433 - val_loss: 0.4272 - val_acc: 0.8368\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5467 - acc: 0.7430 - val_loss: 0.4270 - val_acc: 0.8366\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5452 - acc: 0.7436 - val_loss: 0.4187 - val_acc: 0.8430\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5447 - acc: 0.7437 - val_loss: 0.4269 - val_acc: 0.8385\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5442 - acc: 0.7442 - val_loss: 0.4218 - val_acc: 0.8398\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5448 - acc: 0.7433 - val_loss: 0.4171 - val_acc: 0.8436\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5430 - acc: 0.7451 - val_loss: 0.4209 - val_acc: 0.8405\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5432 - acc: 0.7448 - val_loss: 0.4197 - val_acc: 0.8412\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5421 - acc: 0.7464 - val_loss: 0.4205 - val_acc: 0.8419\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5419 - acc: 0.7457 - val_loss: 0.4178 - val_acc: 0.8438\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5425 - acc: 0.7469 - val_loss: 0.4122 - val_acc: 0.8451\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5413 - acc: 0.7470 - val_loss: 0.4179 - val_acc: 0.8448\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5407 - acc: 0.7470 - val_loss: 0.4182 - val_acc: 0.8447\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5400 - acc: 0.7487 - val_loss: 0.4153 - val_acc: 0.8447\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5395 - acc: 0.7480 - val_loss: 0.4163 - val_acc: 0.8454\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5392 - acc: 0.7482 - val_loss: 0.4151 - val_acc: 0.8452\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5385 - acc: 0.7484 - val_loss: 0.4212 - val_acc: 0.8433\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5387 - acc: 0.7486 - val_loss: 0.4158 - val_acc: 0.8469\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5381 - acc: 0.7481 - val_loss: 0.4176 - val_acc: 0.8459\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5381 - acc: 0.7489 - val_loss: 0.4131 - val_acc: 0.8456\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5387 - acc: 0.7484 - val_loss: 0.4127 - val_acc: 0.8440\n",
      "auroc: 0.9133984215072354\n",
      "auprc: 0.8601063430474817\n",
      "auroc: 0.9117524457446464\n",
      "auprc: 0.8569920624124053\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 4s 331us/step - loss: 0.7667 - acc: 0.5455 - val_loss: 0.6657 - val_acc: 0.6411\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.7007 - acc: 0.5796 - val_loss: 0.6496 - val_acc: 0.6374\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6808 - acc: 0.5972 - val_loss: 0.6408 - val_acc: 0.6376\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6684 - acc: 0.6076 - val_loss: 0.6267 - val_acc: 0.6540\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6575 - acc: 0.6193 - val_loss: 0.6112 - val_acc: 0.6713\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6482 - acc: 0.6283 - val_loss: 0.6023 - val_acc: 0.6794\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6407 - acc: 0.6366 - val_loss: 0.5877 - val_acc: 0.6906\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6323 - acc: 0.6461 - val_loss: 0.5748 - val_acc: 0.7059\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6251 - acc: 0.6548 - val_loss: 0.5631 - val_acc: 0.7181\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6199 - acc: 0.6599 - val_loss: 0.5510 - val_acc: 0.7303\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6136 - acc: 0.6674 - val_loss: 0.5423 - val_acc: 0.7400\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6081 - acc: 0.6749 - val_loss: 0.5310 - val_acc: 0.7443\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.6045 - acc: 0.6770 - val_loss: 0.5244 - val_acc: 0.7560\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5993 - acc: 0.6853 - val_loss: 0.5090 - val_acc: 0.7676\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5943 - acc: 0.6889 - val_loss: 0.5048 - val_acc: 0.7706\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5910 - acc: 0.6921 - val_loss: 0.4961 - val_acc: 0.7799\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5872 - acc: 0.6980 - val_loss: 0.4954 - val_acc: 0.7839\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5844 - acc: 0.7014 - val_loss: 0.4888 - val_acc: 0.7882\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5815 - acc: 0.7040 - val_loss: 0.4811 - val_acc: 0.7945\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5792 - acc: 0.7073 - val_loss: 0.4745 - val_acc: 0.8007\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5764 - acc: 0.7104 - val_loss: 0.4757 - val_acc: 0.8013\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5738 - acc: 0.7117 - val_loss: 0.4710 - val_acc: 0.8028\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5728 - acc: 0.7137 - val_loss: 0.4688 - val_acc: 0.8086\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5698 - acc: 0.7169 - val_loss: 0.4580 - val_acc: 0.8125\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5690 - acc: 0.7171 - val_loss: 0.4619 - val_acc: 0.8129\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5668 - acc: 0.7207 - val_loss: 0.4541 - val_acc: 0.8176\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5647 - acc: 0.7228 - val_loss: 0.4457 - val_acc: 0.8201\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5642 - acc: 0.7236 - val_loss: 0.4516 - val_acc: 0.8211\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5626 - acc: 0.7242 - val_loss: 0.4505 - val_acc: 0.8174\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5613 - acc: 0.7250 - val_loss: 0.4453 - val_acc: 0.8262\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5610 - acc: 0.7258 - val_loss: 0.4420 - val_acc: 0.8288\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5589 - acc: 0.7293 - val_loss: 0.4392 - val_acc: 0.8295\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5578 - acc: 0.7295 - val_loss: 0.4455 - val_acc: 0.8264\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5571 - acc: 0.7301 - val_loss: 0.4362 - val_acc: 0.8294\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5560 - acc: 0.7313 - val_loss: 0.4409 - val_acc: 0.8321\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5557 - acc: 0.7310 - val_loss: 0.4317 - val_acc: 0.8380\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5547 - acc: 0.7340 - val_loss: 0.4312 - val_acc: 0.8373\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5534 - acc: 0.7346 - val_loss: 0.4304 - val_acc: 0.8395\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5530 - acc: 0.7358 - val_loss: 0.4302 - val_acc: 0.8394\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5521 - acc: 0.7349 - val_loss: 0.4287 - val_acc: 0.8333\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5514 - acc: 0.7363 - val_loss: 0.4255 - val_acc: 0.8429\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5505 - acc: 0.7386 - val_loss: 0.4267 - val_acc: 0.8410\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5495 - acc: 0.7395 - val_loss: 0.4238 - val_acc: 0.8406\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5487 - acc: 0.7396 - val_loss: 0.4259 - val_acc: 0.8389\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5479 - acc: 0.7417 - val_loss: 0.4184 - val_acc: 0.8441\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5477 - acc: 0.7412 - val_loss: 0.4202 - val_acc: 0.8435\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5465 - acc: 0.7423 - val_loss: 0.4183 - val_acc: 0.8446\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5464 - acc: 0.7426 - val_loss: 0.4128 - val_acc: 0.8466\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5457 - acc: 0.7427 - val_loss: 0.4167 - val_acc: 0.8413\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5456 - acc: 0.7422 - val_loss: 0.4211 - val_acc: 0.8457\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5457 - acc: 0.7431 - val_loss: 0.4232 - val_acc: 0.8429\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5458 - acc: 0.7418 - val_loss: 0.4171 - val_acc: 0.8399\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5441 - acc: 0.7453 - val_loss: 0.4146 - val_acc: 0.8456\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5428 - acc: 0.7442 - val_loss: 0.4222 - val_acc: 0.8440\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5442 - acc: 0.7450 - val_loss: 0.4130 - val_acc: 0.8493\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5429 - acc: 0.7459 - val_loss: 0.4116 - val_acc: 0.8497\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5427 - acc: 0.7446 - val_loss: 0.4148 - val_acc: 0.8479\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5423 - acc: 0.7449 - val_loss: 0.4132 - val_acc: 0.8498\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5418 - acc: 0.7463 - val_loss: 0.4098 - val_acc: 0.8498\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5418 - acc: 0.7468 - val_loss: 0.4137 - val_acc: 0.8478\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5416 - acc: 0.7455 - val_loss: 0.4047 - val_acc: 0.8483\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5406 - acc: 0.7464 - val_loss: 0.4093 - val_acc: 0.8489\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5399 - acc: 0.7465 - val_loss: 0.4175 - val_acc: 0.8463\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5394 - acc: 0.7478 - val_loss: 0.4044 - val_acc: 0.8503\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5388 - acc: 0.7484 - val_loss: 0.4082 - val_acc: 0.8511\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5383 - acc: 0.7486 - val_loss: 0.4107 - val_acc: 0.8491\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5394 - acc: 0.7479 - val_loss: 0.4020 - val_acc: 0.8516\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5389 - acc: 0.7497 - val_loss: 0.4061 - val_acc: 0.8497\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5382 - acc: 0.7484 - val_loss: 0.4188 - val_acc: 0.8453\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5379 - acc: 0.7481 - val_loss: 0.4100 - val_acc: 0.8500\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5378 - acc: 0.7499 - val_loss: 0.4023 - val_acc: 0.8519\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5369 - acc: 0.7496 - val_loss: 0.4063 - val_acc: 0.8510\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5369 - acc: 0.7493 - val_loss: 0.4073 - val_acc: 0.8498\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5364 - acc: 0.7499 - val_loss: 0.4072 - val_acc: 0.8509\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5361 - acc: 0.7495 - val_loss: 0.4090 - val_acc: 0.8495\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5361 - acc: 0.7499 - val_loss: 0.4007 - val_acc: 0.8520\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5354 - acc: 0.7515 - val_loss: 0.4095 - val_acc: 0.8511\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5351 - acc: 0.7514 - val_loss: 0.4099 - val_acc: 0.8463\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5357 - acc: 0.7494 - val_loss: 0.4023 - val_acc: 0.8527\n",
      "Epoch 80/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5346 - acc: 0.7523 - val_loss: 0.3990 - val_acc: 0.8524\n",
      "Epoch 81/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5338 - acc: 0.7518 - val_loss: 0.4087 - val_acc: 0.8500\n",
      "Epoch 82/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5346 - acc: 0.7520 - val_loss: 0.4037 - val_acc: 0.8479\n",
      "Epoch 83/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5356 - acc: 0.7506 - val_loss: 0.4000 - val_acc: 0.8521\n",
      "Epoch 84/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5347 - acc: 0.7503 - val_loss: 0.3997 - val_acc: 0.8523\n",
      "Epoch 85/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5339 - acc: 0.7517 - val_loss: 0.4026 - val_acc: 0.8498\n",
      "Epoch 86/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5334 - acc: 0.7527 - val_loss: 0.4105 - val_acc: 0.8485\n",
      "Epoch 87/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5337 - acc: 0.7524 - val_loss: 0.4091 - val_acc: 0.8476\n",
      "Epoch 88/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5333 - acc: 0.7526 - val_loss: 0.4092 - val_acc: 0.8481\n",
      "Epoch 89/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5329 - acc: 0.7514 - val_loss: 0.4017 - val_acc: 0.8519\n",
      "Epoch 90/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5319 - acc: 0.7536 - val_loss: 0.3988 - val_acc: 0.8527\n",
      "Epoch 91/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5323 - acc: 0.7538 - val_loss: 0.4020 - val_acc: 0.8504\n",
      "Epoch 92/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5327 - acc: 0.7538 - val_loss: 0.4070 - val_acc: 0.8476\n",
      "Epoch 93/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5327 - acc: 0.7536 - val_loss: 0.4012 - val_acc: 0.8502\n",
      "Epoch 94/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5315 - acc: 0.7544 - val_loss: 0.3984 - val_acc: 0.8498\n",
      "Epoch 95/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5308 - acc: 0.7550 - val_loss: 0.4043 - val_acc: 0.8507\n",
      "Epoch 96/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5325 - acc: 0.7532 - val_loss: 0.4013 - val_acc: 0.8497\n",
      "Epoch 97/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5329 - acc: 0.7523 - val_loss: 0.4043 - val_acc: 0.8518\n",
      "Epoch 98/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5310 - acc: 0.7549 - val_loss: 0.4023 - val_acc: 0.8508\n",
      "Epoch 99/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5306 - acc: 0.7547 - val_loss: 0.3961 - val_acc: 0.8536\n",
      "Epoch 100/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5297 - acc: 0.7555 - val_loss: 0.3996 - val_acc: 0.8521\n",
      "Epoch 101/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5298 - acc: 0.7553 - val_loss: 0.4045 - val_acc: 0.8500\n",
      "Epoch 102/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5303 - acc: 0.7542 - val_loss: 0.4021 - val_acc: 0.8514\n",
      "Epoch 103/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5292 - acc: 0.7561 - val_loss: 0.4013 - val_acc: 0.8521\n",
      "Epoch 104/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5305 - acc: 0.7551 - val_loss: 0.4012 - val_acc: 0.8506\n",
      "Epoch 105/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5297 - acc: 0.7551 - val_loss: 0.3965 - val_acc: 0.8516\n",
      "Epoch 106/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5292 - acc: 0.7551 - val_loss: 0.4053 - val_acc: 0.8470\n",
      "Epoch 107/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5291 - acc: 0.7545 - val_loss: 0.3979 - val_acc: 0.8538\n",
      "Epoch 108/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5291 - acc: 0.7554 - val_loss: 0.4017 - val_acc: 0.8529\n",
      "Epoch 109/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5289 - acc: 0.7561 - val_loss: 0.4001 - val_acc: 0.8530\n",
      "auroc: 0.9213940992532006\n",
      "auprc: 0.8735951670291123\n",
      "auroc: 0.9211154231903108\n",
      "auprc: 0.873129990584561\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 4s 342us/step - loss: 0.7602 - acc: 0.5440 - val_loss: 0.6815 - val_acc: 0.6032\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.7038 - acc: 0.5688 - val_loss: 0.6599 - val_acc: 0.6171\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6846 - acc: 0.5890 - val_loss: 0.6485 - val_acc: 0.6241\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6722 - acc: 0.6014 - val_loss: 0.6355 - val_acc: 0.6404\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6615 - acc: 0.6124 - val_loss: 0.6230 - val_acc: 0.6551\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6528 - acc: 0.6212 - val_loss: 0.6111 - val_acc: 0.6666\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6444 - acc: 0.6298 - val_loss: 0.6003 - val_acc: 0.6770\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6370 - acc: 0.6382 - val_loss: 0.5884 - val_acc: 0.6952\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6300 - acc: 0.6472 - val_loss: 0.5754 - val_acc: 0.7039\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6227 - acc: 0.6563 - val_loss: 0.5637 - val_acc: 0.7157\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6164 - acc: 0.6617 - val_loss: 0.5523 - val_acc: 0.7274\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6109 - acc: 0.6686 - val_loss: 0.5368 - val_acc: 0.7398\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6059 - acc: 0.6761 - val_loss: 0.5286 - val_acc: 0.7477\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6004 - acc: 0.6808 - val_loss: 0.5224 - val_acc: 0.7559\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5967 - acc: 0.6857 - val_loss: 0.5147 - val_acc: 0.7639\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5920 - acc: 0.6914 - val_loss: 0.5038 - val_acc: 0.7745\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5877 - acc: 0.6957 - val_loss: 0.4998 - val_acc: 0.7801\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5848 - acc: 0.6999 - val_loss: 0.4882 - val_acc: 0.7895\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5812 - acc: 0.7038 - val_loss: 0.4842 - val_acc: 0.7933\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5780 - acc: 0.7071 - val_loss: 0.4731 - val_acc: 0.7992\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5750 - acc: 0.7140 - val_loss: 0.4678 - val_acc: 0.8056\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5720 - acc: 0.7160 - val_loss: 0.4640 - val_acc: 0.8083\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5703 - acc: 0.7184 - val_loss: 0.4621 - val_acc: 0.8110\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5683 - acc: 0.7188 - val_loss: 0.4654 - val_acc: 0.8120\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5666 - acc: 0.7216 - val_loss: 0.4551 - val_acc: 0.8164\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5644 - acc: 0.7245 - val_loss: 0.4487 - val_acc: 0.8194\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5624 - acc: 0.7256 - val_loss: 0.4453 - val_acc: 0.8274\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5615 - acc: 0.7265 - val_loss: 0.4404 - val_acc: 0.8305\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5593 - acc: 0.7291 - val_loss: 0.4423 - val_acc: 0.8291\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5584 - acc: 0.7286 - val_loss: 0.4389 - val_acc: 0.8311\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5578 - acc: 0.7296 - val_loss: 0.4383 - val_acc: 0.8317\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5566 - acc: 0.7310 - val_loss: 0.4284 - val_acc: 0.8369\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5548 - acc: 0.7324 - val_loss: 0.4383 - val_acc: 0.8341\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5541 - acc: 0.7319 - val_loss: 0.4304 - val_acc: 0.8372\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5525 - acc: 0.7351 - val_loss: 0.4296 - val_acc: 0.8359\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5517 - acc: 0.7338 - val_loss: 0.4261 - val_acc: 0.8434\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5503 - acc: 0.7373 - val_loss: 0.4250 - val_acc: 0.8400\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5502 - acc: 0.7352 - val_loss: 0.4202 - val_acc: 0.8419\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5503 - acc: 0.7361 - val_loss: 0.4276 - val_acc: 0.8411\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5492 - acc: 0.7384 - val_loss: 0.4292 - val_acc: 0.8368\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5508 - acc: 0.7370 - val_loss: 0.4192 - val_acc: 0.8440\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5489 - acc: 0.7372 - val_loss: 0.4252 - val_acc: 0.8401\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5467 - acc: 0.7394 - val_loss: 0.4203 - val_acc: 0.8458\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5459 - acc: 0.7413 - val_loss: 0.4174 - val_acc: 0.8457\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5448 - acc: 0.7408 - val_loss: 0.4183 - val_acc: 0.8446\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5444 - acc: 0.7417 - val_loss: 0.4203 - val_acc: 0.8448\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5439 - acc: 0.7417 - val_loss: 0.4198 - val_acc: 0.8451\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5435 - acc: 0.7420 - val_loss: 0.4232 - val_acc: 0.8439\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5428 - acc: 0.7441 - val_loss: 0.4143 - val_acc: 0.8478\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5420 - acc: 0.7441 - val_loss: 0.4122 - val_acc: 0.8492\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5425 - acc: 0.7429 - val_loss: 0.4163 - val_acc: 0.8459\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5412 - acc: 0.7436 - val_loss: 0.4119 - val_acc: 0.8495\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5408 - acc: 0.7450 - val_loss: 0.4081 - val_acc: 0.8496\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5409 - acc: 0.7444 - val_loss: 0.4232 - val_acc: 0.8403\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5415 - acc: 0.7439 - val_loss: 0.4109 - val_acc: 0.8485\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5405 - acc: 0.7449 - val_loss: 0.4161 - val_acc: 0.8471\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5401 - acc: 0.7452 - val_loss: 0.4106 - val_acc: 0.8491\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5411 - acc: 0.7431 - val_loss: 0.4055 - val_acc: 0.8507\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5390 - acc: 0.7457 - val_loss: 0.4073 - val_acc: 0.8514\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5392 - acc: 0.7460 - val_loss: 0.4107 - val_acc: 0.8460\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5387 - acc: 0.7456 - val_loss: 0.4039 - val_acc: 0.8522\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5382 - acc: 0.7473 - val_loss: 0.4110 - val_acc: 0.8501\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5375 - acc: 0.7468 - val_loss: 0.4034 - val_acc: 0.8528\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5373 - acc: 0.7480 - val_loss: 0.4066 - val_acc: 0.8517\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5379 - acc: 0.7471 - val_loss: 0.4075 - val_acc: 0.8529\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5371 - acc: 0.7470 - val_loss: 0.4064 - val_acc: 0.8533\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5361 - acc: 0.7495 - val_loss: 0.4089 - val_acc: 0.8522\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5367 - acc: 0.7483 - val_loss: 0.4071 - val_acc: 0.8507\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5352 - acc: 0.7498 - val_loss: 0.4045 - val_acc: 0.8538\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5348 - acc: 0.7494 - val_loss: 0.4029 - val_acc: 0.8529\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5349 - acc: 0.7493 - val_loss: 0.4081 - val_acc: 0.8529\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5355 - acc: 0.7489 - val_loss: 0.4072 - val_acc: 0.8525\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5338 - acc: 0.7503 - val_loss: 0.4008 - val_acc: 0.8546\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5339 - acc: 0.7503 - val_loss: 0.4112 - val_acc: 0.8513\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5337 - acc: 0.7496 - val_loss: 0.4044 - val_acc: 0.8539\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5339 - acc: 0.7498 - val_loss: 0.4052 - val_acc: 0.8536\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5334 - acc: 0.7508 - val_loss: 0.4078 - val_acc: 0.8520\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5335 - acc: 0.7521 - val_loss: 0.4071 - val_acc: 0.8526\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5333 - acc: 0.7506 - val_loss: 0.4035 - val_acc: 0.8555\n",
      "Epoch 80/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5333 - acc: 0.7500 - val_loss: 0.4026 - val_acc: 0.8507\n",
      "Epoch 81/200\n",
      "12000/12000 [==============================] - 0s 20us/step - loss: 0.5327 - acc: 0.7506 - val_loss: 0.4043 - val_acc: 0.8547\n",
      "Epoch 82/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5321 - acc: 0.7517 - val_loss: 0.4058 - val_acc: 0.8529\n",
      "Epoch 83/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5331 - acc: 0.7500 - val_loss: 0.4031 - val_acc: 0.8533\n",
      "auroc: 0.9118452537939302\n",
      "auprc: 0.859697597528848\n",
      "auroc: 0.911450060904663\n",
      "auprc: 0.8589799228125651\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 4s 356us/step - loss: 0.9949 - acc: 0.5212 - val_loss: 0.7063 - val_acc: 0.5969\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.7172 - acc: 0.5735 - val_loss: 0.6825 - val_acc: 0.5783\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6845 - acc: 0.5858 - val_loss: 0.6448 - val_acc: 0.6310\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6691 - acc: 0.6026 - val_loss: 0.6357 - val_acc: 0.6371\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6583 - acc: 0.6156 - val_loss: 0.6168 - val_acc: 0.6631\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6484 - acc: 0.6264 - val_loss: 0.6051 - val_acc: 0.6774\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6391 - acc: 0.6383 - val_loss: 0.5942 - val_acc: 0.6871\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.6319 - acc: 0.6481 - val_loss: 0.5796 - val_acc: 0.7020\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6242 - acc: 0.6572 - val_loss: 0.5687 - val_acc: 0.7157\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6173 - acc: 0.6644 - val_loss: 0.5560 - val_acc: 0.7284\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6114 - acc: 0.6708 - val_loss: 0.5459 - val_acc: 0.7373\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6061 - acc: 0.6759 - val_loss: 0.5303 - val_acc: 0.7503\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6009 - acc: 0.6834 - val_loss: 0.5272 - val_acc: 0.7541\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5964 - acc: 0.6899 - val_loss: 0.5167 - val_acc: 0.7637\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5925 - acc: 0.6933 - val_loss: 0.5111 - val_acc: 0.7699\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5886 - acc: 0.6994 - val_loss: 0.5003 - val_acc: 0.7779\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5853 - acc: 0.7042 - val_loss: 0.4977 - val_acc: 0.7808\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5822 - acc: 0.7074 - val_loss: 0.4876 - val_acc: 0.7873\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5798 - acc: 0.7087 - val_loss: 0.4858 - val_acc: 0.7923\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5768 - acc: 0.7126 - val_loss: 0.4758 - val_acc: 0.8002\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5751 - acc: 0.7139 - val_loss: 0.4737 - val_acc: 0.8036\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5729 - acc: 0.7169 - val_loss: 0.4679 - val_acc: 0.8086\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5700 - acc: 0.7200 - val_loss: 0.4671 - val_acc: 0.8065\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5684 - acc: 0.7218 - val_loss: 0.4599 - val_acc: 0.8141\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5669 - acc: 0.7224 - val_loss: 0.4555 - val_acc: 0.8153\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5657 - acc: 0.7244 - val_loss: 0.4543 - val_acc: 0.8209\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5642 - acc: 0.7267 - val_loss: 0.4515 - val_acc: 0.8225\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5624 - acc: 0.7282 - val_loss: 0.4461 - val_acc: 0.8251\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5616 - acc: 0.7289 - val_loss: 0.4409 - val_acc: 0.8303\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5599 - acc: 0.7308 - val_loss: 0.4430 - val_acc: 0.8303\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5585 - acc: 0.7313 - val_loss: 0.4354 - val_acc: 0.8327\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5565 - acc: 0.7341 - val_loss: 0.4381 - val_acc: 0.8337\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5567 - acc: 0.7331 - val_loss: 0.4465 - val_acc: 0.8296\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5551 - acc: 0.7335 - val_loss: 0.4304 - val_acc: 0.8375\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5534 - acc: 0.7360 - val_loss: 0.4365 - val_acc: 0.8365\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5524 - acc: 0.7359 - val_loss: 0.4371 - val_acc: 0.8334\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5515 - acc: 0.7369 - val_loss: 0.4328 - val_acc: 0.8392\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5508 - acc: 0.7385 - val_loss: 0.4248 - val_acc: 0.8439\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5500 - acc: 0.7388 - val_loss: 0.4281 - val_acc: 0.8407\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5492 - acc: 0.7408 - val_loss: 0.4206 - val_acc: 0.8433\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5482 - acc: 0.7400 - val_loss: 0.4213 - val_acc: 0.8448\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5483 - acc: 0.7394 - val_loss: 0.4276 - val_acc: 0.8434\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5473 - acc: 0.7399 - val_loss: 0.4179 - val_acc: 0.8467\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5461 - acc: 0.7418 - val_loss: 0.4206 - val_acc: 0.8481\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5458 - acc: 0.7410 - val_loss: 0.4224 - val_acc: 0.8459\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5450 - acc: 0.7420 - val_loss: 0.4166 - val_acc: 0.8489\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5445 - acc: 0.7433 - val_loss: 0.4112 - val_acc: 0.8520\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5442 - acc: 0.7435 - val_loss: 0.4192 - val_acc: 0.8499\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5432 - acc: 0.7436 - val_loss: 0.4131 - val_acc: 0.8506\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5425 - acc: 0.7446 - val_loss: 0.4081 - val_acc: 0.8524\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5429 - acc: 0.7430 - val_loss: 0.4093 - val_acc: 0.8482\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5424 - acc: 0.7445 - val_loss: 0.4151 - val_acc: 0.8500\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5411 - acc: 0.7461 - val_loss: 0.4126 - val_acc: 0.8530\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5403 - acc: 0.7467 - val_loss: 0.4130 - val_acc: 0.8493\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5404 - acc: 0.7468 - val_loss: 0.4080 - val_acc: 0.8524\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5403 - acc: 0.7460 - val_loss: 0.4091 - val_acc: 0.8503\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5393 - acc: 0.7479 - val_loss: 0.4102 - val_acc: 0.8533\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5387 - acc: 0.7477 - val_loss: 0.4040 - val_acc: 0.8542\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5383 - acc: 0.7472 - val_loss: 0.4065 - val_acc: 0.8556\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5384 - acc: 0.7471 - val_loss: 0.4085 - val_acc: 0.8536\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5379 - acc: 0.7492 - val_loss: 0.4092 - val_acc: 0.8529\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5375 - acc: 0.7488 - val_loss: 0.4044 - val_acc: 0.8557\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5366 - acc: 0.7502 - val_loss: 0.4092 - val_acc: 0.8524\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5369 - acc: 0.7481 - val_loss: 0.4061 - val_acc: 0.8525\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5361 - acc: 0.7499 - val_loss: 0.4019 - val_acc: 0.8556\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5361 - acc: 0.7501 - val_loss: 0.4086 - val_acc: 0.8528\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5360 - acc: 0.7492 - val_loss: 0.4085 - val_acc: 0.8520\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5352 - acc: 0.7501 - val_loss: 0.3963 - val_acc: 0.8566\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5354 - acc: 0.7502 - val_loss: 0.4047 - val_acc: 0.8552\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5343 - acc: 0.7508 - val_loss: 0.4023 - val_acc: 0.8556\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5337 - acc: 0.7531 - val_loss: 0.4043 - val_acc: 0.8540\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5340 - acc: 0.7509 - val_loss: 0.4037 - val_acc: 0.8522\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5340 - acc: 0.7501 - val_loss: 0.4037 - val_acc: 0.8556\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5343 - acc: 0.7518 - val_loss: 0.3995 - val_acc: 0.8550\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5344 - acc: 0.7513 - val_loss: 0.4019 - val_acc: 0.8549\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5338 - acc: 0.7521 - val_loss: 0.3977 - val_acc: 0.8551\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5343 - acc: 0.7503 - val_loss: 0.4088 - val_acc: 0.8487\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5326 - acc: 0.7532 - val_loss: 0.4014 - val_acc: 0.8562\n",
      "auroc: 0.9194962368499165\n",
      "auprc: 0.8691279445116629\n",
      "auroc: 0.9185561153944124\n",
      "auprc: 0.8673900023497034\n",
      "Train on 12000 samples, validate on 9000 samples\n",
      "Epoch 1/200\n",
      "12000/12000 [==============================] - 4s 367us/step - loss: 0.7518 - acc: 0.5438 - val_loss: 0.6770 - val_acc: 0.6058\n",
      "Epoch 2/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.7024 - acc: 0.5724 - val_loss: 0.6596 - val_acc: 0.6198\n",
      "Epoch 3/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6839 - acc: 0.5894 - val_loss: 0.6452 - val_acc: 0.6302\n",
      "Epoch 4/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6692 - acc: 0.6044 - val_loss: 0.6342 - val_acc: 0.6403\n",
      "Epoch 5/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6565 - acc: 0.6228 - val_loss: 0.6139 - val_acc: 0.6629\n",
      "Epoch 6/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6441 - acc: 0.6328 - val_loss: 0.5925 - val_acc: 0.6814\n",
      "Epoch 7/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6334 - acc: 0.6462 - val_loss: 0.5787 - val_acc: 0.6976\n",
      "Epoch 8/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6239 - acc: 0.6579 - val_loss: 0.5610 - val_acc: 0.7138\n",
      "Epoch 9/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6156 - acc: 0.6669 - val_loss: 0.5488 - val_acc: 0.7251\n",
      "Epoch 10/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6079 - acc: 0.6768 - val_loss: 0.5306 - val_acc: 0.7435\n",
      "Epoch 11/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.6019 - acc: 0.6833 - val_loss: 0.5193 - val_acc: 0.7566\n",
      "Epoch 12/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5963 - acc: 0.6877 - val_loss: 0.5105 - val_acc: 0.7686\n",
      "Epoch 13/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5913 - acc: 0.6958 - val_loss: 0.4951 - val_acc: 0.7779\n",
      "Epoch 14/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5867 - acc: 0.7008 - val_loss: 0.4892 - val_acc: 0.7864\n",
      "Epoch 15/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5832 - acc: 0.7051 - val_loss: 0.4863 - val_acc: 0.7909\n",
      "Epoch 16/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5791 - acc: 0.7097 - val_loss: 0.4709 - val_acc: 0.8003\n",
      "Epoch 17/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5763 - acc: 0.7115 - val_loss: 0.4733 - val_acc: 0.8025\n",
      "Epoch 18/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5734 - acc: 0.7163 - val_loss: 0.4665 - val_acc: 0.8069\n",
      "Epoch 19/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5714 - acc: 0.7179 - val_loss: 0.4613 - val_acc: 0.8136\n",
      "Epoch 20/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5688 - acc: 0.7198 - val_loss: 0.4520 - val_acc: 0.8139\n",
      "Epoch 21/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5677 - acc: 0.7217 - val_loss: 0.4639 - val_acc: 0.8134\n",
      "Epoch 22/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5651 - acc: 0.7262 - val_loss: 0.4504 - val_acc: 0.8165\n",
      "Epoch 23/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5639 - acc: 0.7264 - val_loss: 0.4470 - val_acc: 0.8252\n",
      "Epoch 24/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5616 - acc: 0.7301 - val_loss: 0.4420 - val_acc: 0.8281\n",
      "Epoch 25/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5598 - acc: 0.7308 - val_loss: 0.4403 - val_acc: 0.8268\n",
      "Epoch 26/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5588 - acc: 0.7316 - val_loss: 0.4424 - val_acc: 0.8292\n",
      "Epoch 27/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5574 - acc: 0.7330 - val_loss: 0.4366 - val_acc: 0.8328\n",
      "Epoch 28/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5558 - acc: 0.7353 - val_loss: 0.4317 - val_acc: 0.8349\n",
      "Epoch 29/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5558 - acc: 0.7366 - val_loss: 0.4275 - val_acc: 0.8373\n",
      "Epoch 30/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5540 - acc: 0.7371 - val_loss: 0.4323 - val_acc: 0.8374\n",
      "Epoch 31/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5525 - acc: 0.7386 - val_loss: 0.4286 - val_acc: 0.8413\n",
      "Epoch 32/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5520 - acc: 0.7385 - val_loss: 0.4247 - val_acc: 0.8440\n",
      "Epoch 33/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5504 - acc: 0.7400 - val_loss: 0.4261 - val_acc: 0.8412\n",
      "Epoch 34/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5502 - acc: 0.7398 - val_loss: 0.4227 - val_acc: 0.8424\n",
      "Epoch 35/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5488 - acc: 0.7424 - val_loss: 0.4216 - val_acc: 0.8456\n",
      "Epoch 36/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5474 - acc: 0.7432 - val_loss: 0.4162 - val_acc: 0.8504\n",
      "Epoch 37/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5473 - acc: 0.7443 - val_loss: 0.4160 - val_acc: 0.8485\n",
      "Epoch 38/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5460 - acc: 0.7447 - val_loss: 0.4158 - val_acc: 0.8503\n",
      "Epoch 39/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5450 - acc: 0.7463 - val_loss: 0.4141 - val_acc: 0.8491\n",
      "Epoch 40/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5446 - acc: 0.7466 - val_loss: 0.4121 - val_acc: 0.8503\n",
      "Epoch 41/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5441 - acc: 0.7465 - val_loss: 0.4205 - val_acc: 0.8447\n",
      "Epoch 42/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5436 - acc: 0.7469 - val_loss: 0.4107 - val_acc: 0.8529\n",
      "Epoch 43/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5435 - acc: 0.7475 - val_loss: 0.4153 - val_acc: 0.8500\n",
      "Epoch 44/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5421 - acc: 0.7483 - val_loss: 0.4103 - val_acc: 0.8500\n",
      "Epoch 45/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5425 - acc: 0.7460 - val_loss: 0.4124 - val_acc: 0.8511\n",
      "Epoch 46/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5419 - acc: 0.7481 - val_loss: 0.4050 - val_acc: 0.8559\n",
      "Epoch 47/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5410 - acc: 0.7476 - val_loss: 0.4079 - val_acc: 0.8536\n",
      "Epoch 48/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5394 - acc: 0.7503 - val_loss: 0.4047 - val_acc: 0.8529\n",
      "Epoch 49/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5391 - acc: 0.7498 - val_loss: 0.4112 - val_acc: 0.8523\n",
      "Epoch 50/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5393 - acc: 0.7499 - val_loss: 0.4031 - val_acc: 0.8579\n",
      "Epoch 51/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5398 - acc: 0.7490 - val_loss: 0.4023 - val_acc: 0.8589\n",
      "Epoch 52/200\n",
      "12000/12000 [==============================] - ETA: 0s - loss: 0.5389 - acc: 0.749 - 0s 22us/step - loss: 0.5387 - acc: 0.7501 - val_loss: 0.4020 - val_acc: 0.8585\n",
      "Epoch 53/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5373 - acc: 0.7510 - val_loss: 0.4057 - val_acc: 0.8566\n",
      "Epoch 54/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5364 - acc: 0.7514 - val_loss: 0.4097 - val_acc: 0.8561\n",
      "Epoch 55/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5374 - acc: 0.7497 - val_loss: 0.4101 - val_acc: 0.8494\n",
      "Epoch 56/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5371 - acc: 0.7496 - val_loss: 0.4068 - val_acc: 0.8523\n",
      "Epoch 57/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5353 - acc: 0.7527 - val_loss: 0.4037 - val_acc: 0.8556\n",
      "Epoch 58/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5350 - acc: 0.7508 - val_loss: 0.4066 - val_acc: 0.8545\n",
      "Epoch 59/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5362 - acc: 0.7504 - val_loss: 0.4002 - val_acc: 0.8569\n",
      "Epoch 60/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5344 - acc: 0.7525 - val_loss: 0.3991 - val_acc: 0.8563\n",
      "Epoch 61/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5342 - acc: 0.7529 - val_loss: 0.4033 - val_acc: 0.8597\n",
      "Epoch 62/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5335 - acc: 0.7524 - val_loss: 0.4027 - val_acc: 0.8595\n",
      "Epoch 63/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5333 - acc: 0.7528 - val_loss: 0.3972 - val_acc: 0.8581\n",
      "Epoch 64/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5328 - acc: 0.7544 - val_loss: 0.4071 - val_acc: 0.8576\n",
      "Epoch 65/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5343 - acc: 0.7517 - val_loss: 0.4024 - val_acc: 0.8610\n",
      "Epoch 66/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5322 - acc: 0.7549 - val_loss: 0.3971 - val_acc: 0.8616\n",
      "Epoch 67/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5319 - acc: 0.7548 - val_loss: 0.3930 - val_acc: 0.8583\n",
      "Epoch 68/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5317 - acc: 0.7543 - val_loss: 0.4049 - val_acc: 0.8577\n",
      "Epoch 69/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5320 - acc: 0.7540 - val_loss: 0.3930 - val_acc: 0.8588\n",
      "Epoch 70/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5314 - acc: 0.7547 - val_loss: 0.4022 - val_acc: 0.8550\n",
      "Epoch 71/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5307 - acc: 0.7543 - val_loss: 0.3969 - val_acc: 0.8576\n",
      "Epoch 72/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5314 - acc: 0.7535 - val_loss: 0.4060 - val_acc: 0.8564\n",
      "Epoch 73/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5304 - acc: 0.7559 - val_loss: 0.3977 - val_acc: 0.8609\n",
      "Epoch 74/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5299 - acc: 0.7567 - val_loss: 0.3923 - val_acc: 0.8604\n",
      "Epoch 75/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5305 - acc: 0.7562 - val_loss: 0.3935 - val_acc: 0.8584\n",
      "Epoch 76/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5292 - acc: 0.7571 - val_loss: 0.3939 - val_acc: 0.8589\n",
      "Epoch 77/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5289 - acc: 0.7564 - val_loss: 0.3964 - val_acc: 0.8621\n",
      "Epoch 78/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5289 - acc: 0.7564 - val_loss: 0.4001 - val_acc: 0.8599\n",
      "Epoch 79/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5286 - acc: 0.7568 - val_loss: 0.3962 - val_acc: 0.8599\n",
      "Epoch 80/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5283 - acc: 0.7579 - val_loss: 0.3937 - val_acc: 0.8598\n",
      "Epoch 81/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5298 - acc: 0.7551 - val_loss: 0.4037 - val_acc: 0.8564\n",
      "Epoch 82/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5288 - acc: 0.7566 - val_loss: 0.3983 - val_acc: 0.8585\n",
      "Epoch 83/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5279 - acc: 0.7575 - val_loss: 0.3915 - val_acc: 0.8574\n",
      "Epoch 84/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5292 - acc: 0.7568 - val_loss: 0.3950 - val_acc: 0.8531\n",
      "Epoch 85/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5290 - acc: 0.7557 - val_loss: 0.3911 - val_acc: 0.8563\n",
      "Epoch 86/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5281 - acc: 0.7571 - val_loss: 0.3956 - val_acc: 0.8575\n",
      "Epoch 87/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5268 - acc: 0.7580 - val_loss: 0.3987 - val_acc: 0.8573\n",
      "Epoch 88/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5265 - acc: 0.7590 - val_loss: 0.4008 - val_acc: 0.8513\n",
      "Epoch 89/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5274 - acc: 0.7585 - val_loss: 0.3930 - val_acc: 0.8590\n",
      "Epoch 90/200\n",
      "12000/12000 [==============================] - 0s 22us/step - loss: 0.5273 - acc: 0.7586 - val_loss: 0.3922 - val_acc: 0.8609\n",
      "Epoch 91/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5266 - acc: 0.7587 - val_loss: 0.3928 - val_acc: 0.8609\n",
      "Epoch 92/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5264 - acc: 0.7584 - val_loss: 0.4032 - val_acc: 0.8551\n",
      "Epoch 93/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5278 - acc: 0.7571 - val_loss: 0.3976 - val_acc: 0.8572\n",
      "Epoch 94/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5263 - acc: 0.7576 - val_loss: 0.3945 - val_acc: 0.8593\n",
      "Epoch 95/200\n",
      "12000/12000 [==============================] - 0s 21us/step - loss: 0.5255 - acc: 0.7588 - val_loss: 0.3983 - val_acc: 0.8563\n",
      "auroc: 0.9240917426845346\n",
      "auprc: 0.8775154968437725\n",
      "auroc: 0.9236814684214935\n",
      "auprc: 0.8770584986757587\n"
     ]
    }
   ],
   "source": [
    "for curr_seed in all_seeds: \n",
    "    model, early_stopping_callback, auroc_callback = train_model(model_wrapper = REG_WRAPPER, \n",
    "                                                                 aug = None, \n",
    "                                                                 curr_seed = curr_seed, \n",
    "                                                                 x = x_train_200, \n",
    "                                                                 y = y_train_200_mutate, \n",
    "                                                                 batch_size = 500,\n",
    "                                                                 val_data = (x_val_200, y_val_200))\n",
    "    save_all(filepath = \"general_results_again\", model_arch = \"test_Standard_200_auROC\", curr_seed = curr_seed,\n",
    "             callback = auroc_callback, model = model, val_data = (x_test_200, y_test_200))\n",
    "    save_all(filepath = \"general_results_again\", model_arch = \"test_Standard_200_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_test_200, y_test_200))\n",
    "    \n",
    "#     model, early_stopping_callback, auroc_callback = train_model(model_wrapper = REG_WRAPPER, \n",
    "#                                                                  aug = \"rev_after_each\", \n",
    "#                                                                  curr_seed = curr_seed, \n",
    "#                                                                  x = x_train_200, \n",
    "#                                                                  y = y_train_200_mutate, \n",
    "#                                                                  val_data = (x_val_200, y_val_200))\n",
    "#     save_all(filepath = \"general_results\", model_arch = \"Aug_200_auROC\", curr_seed = curr_seed,\n",
    "#              callback = auroc_callback, model = model, val_data = (x_val_200, y_val_200))\n",
    "#     save_all(filepath = \"general_results\", model_arch = \"Aug_200_loss\", curr_seed = curr_seed,\n",
    "#              callback = early_stopping_callback, model = model, val_data = (x_val_200, y_val_200))\n",
    "    \n",
    "#     model, early_stopping_callback, auroc_callback = train_model(model_wrapper = REG_WRAPPER, \n",
    "#                                                                  aug = \"rev_after_all\", \n",
    "#                                                                  curr_seed = curr_seed, \n",
    "#                                                                  x = x_train_200, \n",
    "#                                                                  y = y_train_200_mutate, \n",
    "#                                                                  val_data = (x_val_200, y_val_200))\n",
    "#     save_all(filepath = \"general_results\", model_arch = \"Aug_Alt_200_auROC\", curr_seed = curr_seed,\n",
    "#              callback = auroc_callback, model = model, val_data = (x_val_200, y_val_200))\n",
    "#     save_all(filepath = \"general_results\", model_arch = \"Aug_Alt_200_loss\", curr_seed = curr_seed,\n",
    "#              callback = early_stopping_callback, model = model, val_data = (x_val_200, y_val_200))\n",
    "\n",
    "#     model, early_stopping_callback, auroc_callback = train_model(model_wrapper = REG_WRAPPER, \n",
    "#                                                                  aug = \"rev_after_all\", \n",
    "#                                                                  curr_seed = curr_seed,\n",
    "#                                                                  batch_size = 1000,\n",
    "#                                                                  x = x_train_200, \n",
    "#                                                                  y = y_train_200_mutate, \n",
    "#                                                                  val_data = (x_val_200, y_val_200))\n",
    "#     save_all(filepath = \"general_results\", model_arch = \"Aug_Alt_double_200_auROC\", curr_seed = curr_seed,\n",
    "#              callback = auroc_callback, model = model, val_data = (x_val_200, y_val_200))\n",
    "#     save_all(filepath = \"general_results\", model_arch = \"Aug_Alt_double_200_loss\", curr_seed = curr_seed,\n",
    "#              callback = early_stopping_callback, model = model, val_data = (x_val_200, y_val_200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 6s 339us/step - loss: 0.6994 - acc: 0.6273 - val_loss: 0.6046 - val_acc: 0.6791\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.5705 - acc: 0.7030 - val_loss: 0.5420 - val_acc: 0.7239\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.5067 - acc: 0.7585 - val_loss: 0.4826 - val_acc: 0.7678\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.4527 - acc: 0.7984 - val_loss: 0.4354 - val_acc: 0.8039\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.4079 - acc: 0.8295 - val_loss: 0.3931 - val_acc: 0.8343\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.3713 - acc: 0.8488 - val_loss: 0.3594 - val_acc: 0.8509\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.3410 - acc: 0.8684 - val_loss: 0.3315 - val_acc: 0.8680\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.3163 - acc: 0.8812 - val_loss: 0.3094 - val_acc: 0.8771\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.2956 - acc: 0.8913 - val_loss: 0.2898 - val_acc: 0.8893\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.2785 - acc: 0.8989 - val_loss: 0.2744 - val_acc: 0.8964\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2643 - acc: 0.9054 - val_loss: 0.2613 - val_acc: 0.9027\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.2530 - acc: 0.9099 - val_loss: 0.2515 - val_acc: 0.9059\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2425 - acc: 0.9149 - val_loss: 0.2424 - val_acc: 0.9098\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2343 - acc: 0.9176 - val_loss: 0.2354 - val_acc: 0.9124\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.2279 - acc: 0.9185 - val_loss: 0.2292 - val_acc: 0.9132\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2204 - acc: 0.9220 - val_loss: 0.2235 - val_acc: 0.9152\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.2151 - acc: 0.9237 - val_loss: 0.2191 - val_acc: 0.9160\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2098 - acc: 0.9253 - val_loss: 0.2147 - val_acc: 0.9196\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.2046 - acc: 0.9275 - val_loss: 0.2099 - val_acc: 0.9207\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.2015 - acc: 0.9282 - val_loss: 0.2103 - val_acc: 0.9170\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1982 - acc: 0.9290 - val_loss: 0.2037 - val_acc: 0.9234\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1941 - acc: 0.9316 - val_loss: 0.2005 - val_acc: 0.9234\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1910 - acc: 0.9325 - val_loss: 0.2019 - val_acc: 0.9209\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1878 - acc: 0.9331 - val_loss: 0.1955 - val_acc: 0.9252\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1845 - acc: 0.9346 - val_loss: 0.1935 - val_acc: 0.9248\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1827 - acc: 0.9351 - val_loss: 0.1913 - val_acc: 0.9281\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1801 - acc: 0.9358 - val_loss: 0.1888 - val_acc: 0.9280\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1773 - acc: 0.9370 - val_loss: 0.1871 - val_acc: 0.9283\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1758 - acc: 0.9371 - val_loss: 0.1856 - val_acc: 0.9294\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1732 - acc: 0.9383 - val_loss: 0.1832 - val_acc: 0.9298\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1709 - acc: 0.9395 - val_loss: 0.1822 - val_acc: 0.9306\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1690 - acc: 0.9404 - val_loss: 0.1810 - val_acc: 0.9305\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1679 - acc: 0.9407 - val_loss: 0.1786 - val_acc: 0.9313\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1653 - acc: 0.9410 - val_loss: 0.1773 - val_acc: 0.9325\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1638 - acc: 0.9421 - val_loss: 0.1760 - val_acc: 0.9319\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1623 - acc: 0.9428 - val_loss: 0.1752 - val_acc: 0.9318\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1609 - acc: 0.9432 - val_loss: 0.1730 - val_acc: 0.9334\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1591 - acc: 0.9436 - val_loss: 0.1723 - val_acc: 0.9336\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1582 - acc: 0.9442 - val_loss: 0.1720 - val_acc: 0.9347\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1566 - acc: 0.9454 - val_loss: 0.1717 - val_acc: 0.9344\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1551 - acc: 0.9453 - val_loss: 0.1693 - val_acc: 0.9350\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1530 - acc: 0.9464 - val_loss: 0.1686 - val_acc: 0.9352\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1526 - acc: 0.9458 - val_loss: 0.1689 - val_acc: 0.9363\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1513 - acc: 0.9467 - val_loss: 0.1659 - val_acc: 0.9366\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1507 - acc: 0.9468 - val_loss: 0.1656 - val_acc: 0.9363\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1494 - acc: 0.9470 - val_loss: 0.1653 - val_acc: 0.9371\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1482 - acc: 0.9478 - val_loss: 0.1630 - val_acc: 0.9379\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1462 - acc: 0.9490 - val_loss: 0.1624 - val_acc: 0.9372\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1454 - acc: 0.9491 - val_loss: 0.1630 - val_acc: 0.9377\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1450 - acc: 0.9498 - val_loss: 0.1611 - val_acc: 0.9384\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1435 - acc: 0.9494 - val_loss: 0.1623 - val_acc: 0.9385\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1427 - acc: 0.9500 - val_loss: 0.1592 - val_acc: 0.9391\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1409 - acc: 0.9507 - val_loss: 0.1588 - val_acc: 0.9398\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1404 - acc: 0.9513 - val_loss: 0.1579 - val_acc: 0.9396\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1395 - acc: 0.9513 - val_loss: 0.1586 - val_acc: 0.9384\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1396 - acc: 0.9513 - val_loss: 0.1565 - val_acc: 0.9406\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1384 - acc: 0.9515 - val_loss: 0.1580 - val_acc: 0.9389\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1374 - acc: 0.9523 - val_loss: 0.1562 - val_acc: 0.9393\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1356 - acc: 0.9530 - val_loss: 0.1548 - val_acc: 0.9409\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1352 - acc: 0.9534 - val_loss: 0.1559 - val_acc: 0.9392\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1358 - acc: 0.9531 - val_loss: 0.1541 - val_acc: 0.9408\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1336 - acc: 0.9536 - val_loss: 0.1551 - val_acc: 0.9392\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1334 - acc: 0.9536 - val_loss: 0.1533 - val_acc: 0.9411\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1320 - acc: 0.9541 - val_loss: 0.1522 - val_acc: 0.9417\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1317 - acc: 0.9529 - val_loss: 0.1516 - val_acc: 0.9415\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1311 - acc: 0.9547 - val_loss: 0.1520 - val_acc: 0.9415\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1305 - acc: 0.9545 - val_loss: 0.1508 - val_acc: 0.9420\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1290 - acc: 0.9556 - val_loss: 0.1535 - val_acc: 0.9411\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1289 - acc: 0.9551 - val_loss: 0.1521 - val_acc: 0.9418\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1282 - acc: 0.9555 - val_loss: 0.1498 - val_acc: 0.9422\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1282 - acc: 0.9562 - val_loss: 0.1493 - val_acc: 0.9431\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1267 - acc: 0.9560 - val_loss: 0.1489 - val_acc: 0.9427\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1260 - acc: 0.9559 - val_loss: 0.1498 - val_acc: 0.9420\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1254 - acc: 0.9562 - val_loss: 0.1483 - val_acc: 0.9427\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1244 - acc: 0.9571 - val_loss: 0.1484 - val_acc: 0.9430\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1239 - acc: 0.9568 - val_loss: 0.1472 - val_acc: 0.9433\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1243 - acc: 0.9562 - val_loss: 0.1485 - val_acc: 0.9431\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1230 - acc: 0.9574 - val_loss: 0.1467 - val_acc: 0.9431\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1227 - acc: 0.9572 - val_loss: 0.1463 - val_acc: 0.9438\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1212 - acc: 0.9581 - val_loss: 0.1467 - val_acc: 0.9439\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1211 - acc: 0.9584 - val_loss: 0.1456 - val_acc: 0.9442\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1202 - acc: 0.9591 - val_loss: 0.1459 - val_acc: 0.9442\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1198 - acc: 0.9588 - val_loss: 0.1454 - val_acc: 0.9440\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1192 - acc: 0.9590 - val_loss: 0.1467 - val_acc: 0.9434\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1186 - acc: 0.9598 - val_loss: 0.1444 - val_acc: 0.9445\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1186 - acc: 0.9590 - val_loss: 0.1444 - val_acc: 0.9449\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1181 - acc: 0.9598 - val_loss: 0.1449 - val_acc: 0.9443\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1171 - acc: 0.9602 - val_loss: 0.1448 - val_acc: 0.9445\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1168 - acc: 0.9600 - val_loss: 0.1440 - val_acc: 0.9447\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1161 - acc: 0.9602 - val_loss: 0.1432 - val_acc: 0.9455\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1160 - acc: 0.9601 - val_loss: 0.1424 - val_acc: 0.9460\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1153 - acc: 0.9603 - val_loss: 0.1421 - val_acc: 0.9458\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1150 - acc: 0.9604 - val_loss: 0.1423 - val_acc: 0.9458\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1145 - acc: 0.9612 - val_loss: 0.1415 - val_acc: 0.9464\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1134 - acc: 0.9616 - val_loss: 0.1411 - val_acc: 0.9460\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1130 - acc: 0.9614 - val_loss: 0.1422 - val_acc: 0.9461\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1147 - acc: 0.9611 - val_loss: 0.1407 - val_acc: 0.9465\n",
      "Epoch 98/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1131 - acc: 0.9610 - val_loss: 0.1407 - val_acc: 0.9462\n",
      "Epoch 99/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1121 - acc: 0.9623 - val_loss: 0.1407 - val_acc: 0.9468\n",
      "Epoch 100/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1117 - acc: 0.9618 - val_loss: 0.1420 - val_acc: 0.9447\n",
      "Epoch 101/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1113 - acc: 0.9627 - val_loss: 0.1394 - val_acc: 0.9472\n",
      "Epoch 102/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1106 - acc: 0.9621 - val_loss: 0.1395 - val_acc: 0.9470\n",
      "Epoch 103/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1104 - acc: 0.9621 - val_loss: 0.1428 - val_acc: 0.9442\n",
      "Epoch 104/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1108 - acc: 0.9625 - val_loss: 0.1391 - val_acc: 0.9467\n",
      "Epoch 105/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1100 - acc: 0.9625 - val_loss: 0.1396 - val_acc: 0.9459\n",
      "Epoch 106/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1090 - acc: 0.9631 - val_loss: 0.1424 - val_acc: 0.9451\n",
      "Epoch 107/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1091 - acc: 0.9628 - val_loss: 0.1382 - val_acc: 0.9471\n",
      "Epoch 108/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1089 - acc: 0.9625 - val_loss: 0.1399 - val_acc: 0.9461\n",
      "Epoch 109/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1093 - acc: 0.9625 - val_loss: 0.1395 - val_acc: 0.9461\n",
      "Epoch 110/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1078 - acc: 0.9634 - val_loss: 0.1389 - val_acc: 0.9467\n",
      "Epoch 111/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1070 - acc: 0.9641 - val_loss: 0.1385 - val_acc: 0.9465\n",
      "Epoch 112/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1067 - acc: 0.9643 - val_loss: 0.1383 - val_acc: 0.9466\n",
      "Epoch 113/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1068 - acc: 0.9637 - val_loss: 0.1375 - val_acc: 0.9468\n",
      "Epoch 114/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1070 - acc: 0.9630 - val_loss: 0.1378 - val_acc: 0.9471\n",
      "Epoch 115/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1058 - acc: 0.9647 - val_loss: 0.1408 - val_acc: 0.9466\n",
      "Epoch 116/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1053 - acc: 0.9644 - val_loss: 0.1374 - val_acc: 0.9475\n",
      "Epoch 117/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1054 - acc: 0.9645 - val_loss: 0.1425 - val_acc: 0.9455\n",
      "Epoch 118/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1057 - acc: 0.9639 - val_loss: 0.1373 - val_acc: 0.9472\n",
      "Epoch 119/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1044 - acc: 0.9645 - val_loss: 0.1391 - val_acc: 0.9469\n",
      "Epoch 120/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1043 - acc: 0.9651 - val_loss: 0.1396 - val_acc: 0.9466\n",
      "Epoch 121/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1043 - acc: 0.9646 - val_loss: 0.1363 - val_acc: 0.9473\n",
      "Epoch 122/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1034 - acc: 0.9656 - val_loss: 0.1369 - val_acc: 0.9474\n",
      "Epoch 123/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1029 - acc: 0.9655 - val_loss: 0.1358 - val_acc: 0.9478\n",
      "Epoch 124/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1031 - acc: 0.9654 - val_loss: 0.1355 - val_acc: 0.9481\n",
      "Epoch 125/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1036 - acc: 0.9650 - val_loss: 0.1366 - val_acc: 0.9475\n",
      "Epoch 126/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1027 - acc: 0.9652 - val_loss: 0.1363 - val_acc: 0.9476\n",
      "Epoch 127/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1020 - acc: 0.9664 - val_loss: 0.1378 - val_acc: 0.9467\n",
      "Epoch 128/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1020 - acc: 0.9659 - val_loss: 0.1358 - val_acc: 0.9480\n",
      "Epoch 129/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1015 - acc: 0.9661 - val_loss: 0.1359 - val_acc: 0.9479\n",
      "Epoch 130/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1014 - acc: 0.9655 - val_loss: 0.1358 - val_acc: 0.9478\n",
      "Epoch 131/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1013 - acc: 0.9656 - val_loss: 0.1357 - val_acc: 0.9477\n",
      "Epoch 132/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1005 - acc: 0.9663 - val_loss: 0.1352 - val_acc: 0.9478\n",
      "Epoch 133/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1008 - acc: 0.9666 - val_loss: 0.1382 - val_acc: 0.9468\n",
      "Epoch 134/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1016 - acc: 0.9653 - val_loss: 0.1351 - val_acc: 0.9479\n",
      "Epoch 135/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0996 - acc: 0.9672 - val_loss: 0.1358 - val_acc: 0.9474\n",
      "Epoch 136/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1000 - acc: 0.9664 - val_loss: 0.1363 - val_acc: 0.9475\n",
      "Epoch 137/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0999 - acc: 0.9671 - val_loss: 0.1346 - val_acc: 0.9483\n",
      "Epoch 138/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0993 - acc: 0.9665 - val_loss: 0.1352 - val_acc: 0.9477\n",
      "Epoch 139/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0989 - acc: 0.9670 - val_loss: 0.1358 - val_acc: 0.9475\n",
      "Epoch 140/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0990 - acc: 0.9669 - val_loss: 0.1351 - val_acc: 0.9480\n",
      "Epoch 141/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0984 - acc: 0.9677 - val_loss: 0.1347 - val_acc: 0.9478\n",
      "Epoch 142/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0983 - acc: 0.9673 - val_loss: 0.1343 - val_acc: 0.9482\n",
      "Epoch 143/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0978 - acc: 0.9674 - val_loss: 0.1350 - val_acc: 0.9476\n",
      "Epoch 144/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0977 - acc: 0.9681 - val_loss: 0.1360 - val_acc: 0.9473\n",
      "Epoch 145/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0978 - acc: 0.9674 - val_loss: 0.1347 - val_acc: 0.9483\n",
      "Epoch 146/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0970 - acc: 0.9675 - val_loss: 0.1340 - val_acc: 0.9484\n",
      "Epoch 147/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0981 - acc: 0.9672 - val_loss: 0.1351 - val_acc: 0.9477\n",
      "Epoch 148/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0982 - acc: 0.9670 - val_loss: 0.1342 - val_acc: 0.9486\n",
      "Epoch 149/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0966 - acc: 0.9682 - val_loss: 0.1360 - val_acc: 0.9478\n",
      "Epoch 150/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0962 - acc: 0.9679 - val_loss: 0.1362 - val_acc: 0.9472\n",
      "Epoch 151/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0966 - acc: 0.9678 - val_loss: 0.1347 - val_acc: 0.9479\n",
      "Epoch 152/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0958 - acc: 0.9686 - val_loss: 0.1346 - val_acc: 0.9483\n",
      "Epoch 153/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0962 - acc: 0.9680 - val_loss: 0.1342 - val_acc: 0.9484\n",
      "Epoch 154/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0957 - acc: 0.9681 - val_loss: 0.1351 - val_acc: 0.9480\n",
      "Epoch 155/200\n",
      "17142/17142 [==============================] - 0s 26us/step - loss: 0.0963 - acc: 0.9677 - val_loss: 0.1333 - val_acc: 0.9487\n",
      "Epoch 156/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0955 - acc: 0.9683 - val_loss: 0.1352 - val_acc: 0.9471\n",
      "Epoch 157/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0949 - acc: 0.9685 - val_loss: 0.1336 - val_acc: 0.9491\n",
      "Epoch 158/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0946 - acc: 0.9685 - val_loss: 0.1341 - val_acc: 0.9480\n",
      "Epoch 159/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0952 - acc: 0.9681 - val_loss: 0.1332 - val_acc: 0.9488\n",
      "Epoch 160/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0942 - acc: 0.9692 - val_loss: 0.1341 - val_acc: 0.9481\n",
      "Epoch 161/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0935 - acc: 0.9693 - val_loss: 0.1336 - val_acc: 0.9485\n",
      "Epoch 162/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0946 - acc: 0.9682 - val_loss: 0.1346 - val_acc: 0.9485\n",
      "Epoch 163/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0935 - acc: 0.9696 - val_loss: 0.1359 - val_acc: 0.9479\n",
      "Epoch 164/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0933 - acc: 0.9700 - val_loss: 0.1342 - val_acc: 0.9487\n",
      "Epoch 165/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0932 - acc: 0.9694 - val_loss: 0.1337 - val_acc: 0.9486\n",
      "Epoch 166/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0931 - acc: 0.9686 - val_loss: 0.1347 - val_acc: 0.9482\n",
      "Epoch 167/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0927 - acc: 0.9698 - val_loss: 0.1347 - val_acc: 0.9483\n",
      "Epoch 168/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0925 - acc: 0.9698 - val_loss: 0.1343 - val_acc: 0.9484\n",
      "Epoch 169/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.0923 - acc: 0.9698 - val_loss: 0.1337 - val_acc: 0.9486\n",
      "auroc: 0.984599652794344\n",
      "auprc: 0.975145791927559\n",
      "auroc: 0.9845897752933408\n",
      "auprc: 0.9751535951071618\n",
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 6s 351us/step - loss: 0.6605 - acc: 0.6463 - val_loss: 0.6172 - val_acc: 0.6677\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.5836 - acc: 0.6914 - val_loss: 0.5499 - val_acc: 0.7185\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.5057 - acc: 0.7494 - val_loss: 0.4717 - val_acc: 0.7715\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.4348 - acc: 0.7976 - val_loss: 0.4140 - val_acc: 0.8045\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.3871 - acc: 0.8259 - val_loss: 0.3748 - val_acc: 0.8316\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.3541 - acc: 0.8472 - val_loss: 0.3467 - val_acc: 0.8476\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.3295 - acc: 0.8620 - val_loss: 0.3264 - val_acc: 0.8610\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.3081 - acc: 0.8772 - val_loss: 0.3035 - val_acc: 0.8780\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2901 - acc: 0.8874 - val_loss: 0.2875 - val_acc: 0.8848\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2762 - acc: 0.8944 - val_loss: 0.2732 - val_acc: 0.8927\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.2620 - acc: 0.9025 - val_loss: 0.2609 - val_acc: 0.8991\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2510 - acc: 0.9088 - val_loss: 0.2511 - val_acc: 0.9020\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.2417 - acc: 0.9109 - val_loss: 0.2419 - val_acc: 0.9073\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2329 - acc: 0.9156 - val_loss: 0.2341 - val_acc: 0.9114\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2248 - acc: 0.9187 - val_loss: 0.2270 - val_acc: 0.9133\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2184 - acc: 0.9198 - val_loss: 0.2201 - val_acc: 0.9177\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2123 - acc: 0.9235 - val_loss: 0.2156 - val_acc: 0.9182\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2066 - acc: 0.9251 - val_loss: 0.2107 - val_acc: 0.9206\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2013 - acc: 0.9266 - val_loss: 0.2053 - val_acc: 0.9221\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1970 - acc: 0.9283 - val_loss: 0.2048 - val_acc: 0.9217\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1939 - acc: 0.9285 - val_loss: 0.1987 - val_acc: 0.9254\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1892 - acc: 0.9305 - val_loss: 0.1963 - val_acc: 0.9245\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.1873 - acc: 0.9311 - val_loss: 0.1935 - val_acc: 0.9265\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1837 - acc: 0.9329 - val_loss: 0.1927 - val_acc: 0.9252\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1804 - acc: 0.9343 - val_loss: 0.1879 - val_acc: 0.9291\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1780 - acc: 0.9351 - val_loss: 0.1862 - val_acc: 0.9286\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1756 - acc: 0.9352 - val_loss: 0.1838 - val_acc: 0.9293\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1731 - acc: 0.9372 - val_loss: 0.1835 - val_acc: 0.9295\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1715 - acc: 0.9372 - val_loss: 0.1806 - val_acc: 0.9308\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1695 - acc: 0.9386 - val_loss: 0.1828 - val_acc: 0.9293\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1673 - acc: 0.9390 - val_loss: 0.1778 - val_acc: 0.9319\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1652 - acc: 0.9405 - val_loss: 0.1759 - val_acc: 0.9330\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1635 - acc: 0.9413 - val_loss: 0.1743 - val_acc: 0.9334\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1618 - acc: 0.9418 - val_loss: 0.1758 - val_acc: 0.9321\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1599 - acc: 0.9430 - val_loss: 0.1742 - val_acc: 0.9325\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1584 - acc: 0.9433 - val_loss: 0.1708 - val_acc: 0.9351\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1564 - acc: 0.9445 - val_loss: 0.1693 - val_acc: 0.9354\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1550 - acc: 0.9448 - val_loss: 0.1732 - val_acc: 0.9331\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1535 - acc: 0.9459 - val_loss: 0.1682 - val_acc: 0.9356\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1524 - acc: 0.9454 - val_loss: 0.1692 - val_acc: 0.9347\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1521 - acc: 0.9454 - val_loss: 0.1654 - val_acc: 0.9375\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1502 - acc: 0.9468 - val_loss: 0.1666 - val_acc: 0.9366\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1485 - acc: 0.9481 - val_loss: 0.1642 - val_acc: 0.9372\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1473 - acc: 0.9484 - val_loss: 0.1661 - val_acc: 0.9357\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1467 - acc: 0.9479 - val_loss: 0.1621 - val_acc: 0.9378\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1450 - acc: 0.9491 - val_loss: 0.1627 - val_acc: 0.9373\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1436 - acc: 0.9495 - val_loss: 0.1623 - val_acc: 0.9378\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1426 - acc: 0.9502 - val_loss: 0.1596 - val_acc: 0.9392\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1416 - acc: 0.9499 - val_loss: 0.1607 - val_acc: 0.9382\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1409 - acc: 0.9517 - val_loss: 0.1581 - val_acc: 0.9401\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1392 - acc: 0.9518 - val_loss: 0.1587 - val_acc: 0.9390\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1392 - acc: 0.9512 - val_loss: 0.1577 - val_acc: 0.9403\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1382 - acc: 0.9521 - val_loss: 0.1591 - val_acc: 0.9384\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1381 - acc: 0.9515 - val_loss: 0.1558 - val_acc: 0.9406\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1365 - acc: 0.9526 - val_loss: 0.1555 - val_acc: 0.9403\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1351 - acc: 0.9534 - val_loss: 0.1554 - val_acc: 0.9401\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1341 - acc: 0.9542 - val_loss: 0.1538 - val_acc: 0.9416\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1338 - acc: 0.9534 - val_loss: 0.1537 - val_acc: 0.9412\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1317 - acc: 0.9554 - val_loss: 0.1523 - val_acc: 0.9420\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1315 - acc: 0.9550 - val_loss: 0.1532 - val_acc: 0.9417\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1305 - acc: 0.9557 - val_loss: 0.1525 - val_acc: 0.9416\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1296 - acc: 0.9553 - val_loss: 0.1510 - val_acc: 0.9426\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1303 - acc: 0.9551 - val_loss: 0.1506 - val_acc: 0.9425\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1286 - acc: 0.9561 - val_loss: 0.1501 - val_acc: 0.9424\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1272 - acc: 0.9572 - val_loss: 0.1508 - val_acc: 0.9431\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1275 - acc: 0.9567 - val_loss: 0.1491 - val_acc: 0.9435\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1264 - acc: 0.9571 - val_loss: 0.1513 - val_acc: 0.9431\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1263 - acc: 0.9569 - val_loss: 0.1500 - val_acc: 0.9426\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1251 - acc: 0.9569 - val_loss: 0.1482 - val_acc: 0.9431\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1242 - acc: 0.9582 - val_loss: 0.1481 - val_acc: 0.9427\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1235 - acc: 0.9588 - val_loss: 0.1480 - val_acc: 0.9436\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1232 - acc: 0.9580 - val_loss: 0.1471 - val_acc: 0.9434\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1222 - acc: 0.9587 - val_loss: 0.1465 - val_acc: 0.9436\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1214 - acc: 0.9591 - val_loss: 0.1480 - val_acc: 0.9438\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1223 - acc: 0.9587 - val_loss: 0.1465 - val_acc: 0.9442\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1209 - acc: 0.9594 - val_loss: 0.1460 - val_acc: 0.9437\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1198 - acc: 0.9603 - val_loss: 0.1452 - val_acc: 0.9443\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1195 - acc: 0.9602 - val_loss: 0.1469 - val_acc: 0.9441\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1193 - acc: 0.9597 - val_loss: 0.1453 - val_acc: 0.9442\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1183 - acc: 0.9604 - val_loss: 0.1444 - val_acc: 0.9444\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1179 - acc: 0.9609 - val_loss: 0.1446 - val_acc: 0.9446\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1178 - acc: 0.9606 - val_loss: 0.1443 - val_acc: 0.9450\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1171 - acc: 0.9606 - val_loss: 0.1443 - val_acc: 0.9451\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1164 - acc: 0.9612 - val_loss: 0.1433 - val_acc: 0.9453\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1155 - acc: 0.9617 - val_loss: 0.1428 - val_acc: 0.9454\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1171 - acc: 0.9598 - val_loss: 0.1436 - val_acc: 0.9443\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1158 - acc: 0.9610 - val_loss: 0.1423 - val_acc: 0.9451\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1145 - acc: 0.9619 - val_loss: 0.1423 - val_acc: 0.9454\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1132 - acc: 0.9629 - val_loss: 0.1420 - val_acc: 0.9463\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1131 - acc: 0.9627 - val_loss: 0.1413 - val_acc: 0.9457\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1134 - acc: 0.9626 - val_loss: 0.1408 - val_acc: 0.9464\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1125 - acc: 0.9627 - val_loss: 0.1420 - val_acc: 0.9456\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1131 - acc: 0.9620 - val_loss: 0.1409 - val_acc: 0.9466\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1121 - acc: 0.9628 - val_loss: 0.1408 - val_acc: 0.9463\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1110 - acc: 0.9633 - val_loss: 0.1406 - val_acc: 0.9457\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1110 - acc: 0.9636 - val_loss: 0.1406 - val_acc: 0.9464\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1102 - acc: 0.9637 - val_loss: 0.1406 - val_acc: 0.9454\n",
      "Epoch 98/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1096 - acc: 0.9645 - val_loss: 0.1397 - val_acc: 0.9470\n",
      "Epoch 99/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1098 - acc: 0.9635 - val_loss: 0.1412 - val_acc: 0.9466\n",
      "Epoch 100/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1098 - acc: 0.9640 - val_loss: 0.1421 - val_acc: 0.9449\n",
      "Epoch 101/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1092 - acc: 0.9636 - val_loss: 0.1397 - val_acc: 0.9462\n",
      "Epoch 102/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1091 - acc: 0.9638 - val_loss: 0.1427 - val_acc: 0.9438\n",
      "Epoch 103/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1082 - acc: 0.9640 - val_loss: 0.1397 - val_acc: 0.9465\n",
      "Epoch 104/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1069 - acc: 0.9650 - val_loss: 0.1386 - val_acc: 0.9472\n",
      "Epoch 105/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1072 - acc: 0.9646 - val_loss: 0.1399 - val_acc: 0.9456\n",
      "Epoch 106/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1074 - acc: 0.9646 - val_loss: 0.1391 - val_acc: 0.9464\n",
      "Epoch 107/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1062 - acc: 0.9651 - val_loss: 0.1399 - val_acc: 0.9473\n",
      "Epoch 108/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1061 - acc: 0.9654 - val_loss: 0.1382 - val_acc: 0.9470\n",
      "Epoch 109/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1053 - acc: 0.9653 - val_loss: 0.1375 - val_acc: 0.9473\n",
      "Epoch 110/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1058 - acc: 0.9657 - val_loss: 0.1400 - val_acc: 0.9457\n",
      "Epoch 111/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1051 - acc: 0.9658 - val_loss: 0.1399 - val_acc: 0.9467\n",
      "Epoch 112/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1041 - acc: 0.9661 - val_loss: 0.1371 - val_acc: 0.9483\n",
      "Epoch 113/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1038 - acc: 0.9664 - val_loss: 0.1392 - val_acc: 0.9464\n",
      "Epoch 114/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1043 - acc: 0.9660 - val_loss: 0.1368 - val_acc: 0.9480\n",
      "Epoch 115/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1033 - acc: 0.9665 - val_loss: 0.1370 - val_acc: 0.9474\n",
      "Epoch 116/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1027 - acc: 0.9671 - val_loss: 0.1369 - val_acc: 0.9476\n",
      "Epoch 117/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1028 - acc: 0.9670 - val_loss: 0.1363 - val_acc: 0.9476\n",
      "Epoch 118/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1028 - acc: 0.9666 - val_loss: 0.1366 - val_acc: 0.9478\n",
      "Epoch 119/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1024 - acc: 0.9665 - val_loss: 0.1367 - val_acc: 0.9485\n",
      "Epoch 120/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1015 - acc: 0.9674 - val_loss: 0.1364 - val_acc: 0.9486\n",
      "Epoch 121/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1016 - acc: 0.9669 - val_loss: 0.1366 - val_acc: 0.9477\n",
      "Epoch 122/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1010 - acc: 0.9671 - val_loss: 0.1359 - val_acc: 0.9480\n",
      "Epoch 123/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1014 - acc: 0.9667 - val_loss: 0.1365 - val_acc: 0.9480\n",
      "Epoch 124/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1010 - acc: 0.9669 - val_loss: 0.1363 - val_acc: 0.9479\n",
      "Epoch 125/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1004 - acc: 0.9672 - val_loss: 0.1391 - val_acc: 0.9462\n",
      "Epoch 126/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1002 - acc: 0.9675 - val_loss: 0.1359 - val_acc: 0.9484\n",
      "Epoch 127/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0999 - acc: 0.9674 - val_loss: 0.1360 - val_acc: 0.9478\n",
      "Epoch 128/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0999 - acc: 0.9673 - val_loss: 0.1372 - val_acc: 0.9472\n",
      "Epoch 129/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0993 - acc: 0.9680 - val_loss: 0.1356 - val_acc: 0.9481\n",
      "Epoch 130/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0993 - acc: 0.9676 - val_loss: 0.1357 - val_acc: 0.9480\n",
      "Epoch 131/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0992 - acc: 0.9675 - val_loss: 0.1355 - val_acc: 0.9482\n",
      "Epoch 132/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0979 - acc: 0.9688 - val_loss: 0.1354 - val_acc: 0.9481\n",
      "Epoch 133/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0983 - acc: 0.9681 - val_loss: 0.1379 - val_acc: 0.9469\n",
      "Epoch 134/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0984 - acc: 0.9677 - val_loss: 0.1368 - val_acc: 0.9471\n",
      "Epoch 135/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0973 - acc: 0.9688 - val_loss: 0.1352 - val_acc: 0.9481\n",
      "Epoch 136/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0974 - acc: 0.9684 - val_loss: 0.1354 - val_acc: 0.9481\n",
      "Epoch 137/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0976 - acc: 0.9675 - val_loss: 0.1375 - val_acc: 0.9473\n",
      "Epoch 138/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0980 - acc: 0.9681 - val_loss: 0.1352 - val_acc: 0.9484\n",
      "Epoch 139/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0965 - acc: 0.9692 - val_loss: 0.1352 - val_acc: 0.9479\n",
      "Epoch 140/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0967 - acc: 0.9690 - val_loss: 0.1358 - val_acc: 0.9483\n",
      "Epoch 141/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0962 - acc: 0.9690 - val_loss: 0.1357 - val_acc: 0.9470\n",
      "Epoch 142/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0956 - acc: 0.9688 - val_loss: 0.1345 - val_acc: 0.9485\n",
      "Epoch 143/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0955 - acc: 0.9701 - val_loss: 0.1361 - val_acc: 0.9484\n",
      "Epoch 144/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0955 - acc: 0.9692 - val_loss: 0.1353 - val_acc: 0.9476\n",
      "Epoch 145/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0954 - acc: 0.9692 - val_loss: 0.1361 - val_acc: 0.9484\n",
      "Epoch 146/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0949 - acc: 0.9700 - val_loss: 0.1367 - val_acc: 0.9470\n",
      "Epoch 147/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0952 - acc: 0.9696 - val_loss: 0.1350 - val_acc: 0.9477\n",
      "Epoch 148/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0948 - acc: 0.9696 - val_loss: 0.1367 - val_acc: 0.9480\n",
      "Epoch 149/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0946 - acc: 0.9694 - val_loss: 0.1352 - val_acc: 0.9483\n",
      "Epoch 150/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0942 - acc: 0.9708 - val_loss: 0.1362 - val_acc: 0.9466\n",
      "Epoch 151/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0944 - acc: 0.9701 - val_loss: 0.1345 - val_acc: 0.9486\n",
      "Epoch 152/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0933 - acc: 0.9703 - val_loss: 0.1338 - val_acc: 0.9493\n",
      "Epoch 153/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0934 - acc: 0.9700 - val_loss: 0.1340 - val_acc: 0.9494\n",
      "Epoch 154/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0935 - acc: 0.9702 - val_loss: 0.1347 - val_acc: 0.9491\n",
      "Epoch 155/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0935 - acc: 0.9702 - val_loss: 0.1339 - val_acc: 0.9493\n",
      "Epoch 156/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0927 - acc: 0.9704 - val_loss: 0.1342 - val_acc: 0.9483\n",
      "Epoch 157/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0927 - acc: 0.9704 - val_loss: 0.1343 - val_acc: 0.9487\n",
      "Epoch 158/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0919 - acc: 0.9712 - val_loss: 0.1350 - val_acc: 0.9481\n",
      "Epoch 159/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0926 - acc: 0.9702 - val_loss: 0.1358 - val_acc: 0.9481\n",
      "Epoch 160/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.0924 - acc: 0.9701 - val_loss: 0.1357 - val_acc: 0.9483\n",
      "Epoch 161/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0930 - acc: 0.9702 - val_loss: 0.1339 - val_acc: 0.9490\n",
      "Epoch 162/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0920 - acc: 0.9704 - val_loss: 0.1343 - val_acc: 0.9490\n",
      "auroc: 0.9845647702587171\n",
      "auprc: 0.9749008504128817\n",
      "auroc: 0.9845647702587171\n",
      "auprc: 0.9749008504128817\n",
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 6s 366us/step - loss: 0.6727 - acc: 0.6377 - val_loss: 0.6129 - val_acc: 0.6671\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.5803 - acc: 0.6978 - val_loss: 0.5613 - val_acc: 0.7017\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.5208 - acc: 0.7459 - val_loss: 0.5059 - val_acc: 0.7470\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.4667 - acc: 0.7871 - val_loss: 0.4538 - val_acc: 0.7862\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.4229 - acc: 0.8157 - val_loss: 0.4125 - val_acc: 0.8188\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.3885 - acc: 0.8386 - val_loss: 0.3836 - val_acc: 0.8317\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.3607 - acc: 0.8535 - val_loss: 0.3558 - val_acc: 0.8552\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.3360 - acc: 0.8682 - val_loss: 0.3334 - val_acc: 0.8606\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.3140 - acc: 0.8786 - val_loss: 0.3122 - val_acc: 0.8753\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.2952 - acc: 0.8892 - val_loss: 0.2967 - val_acc: 0.8821\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2805 - acc: 0.8952 - val_loss: 0.2814 - val_acc: 0.8896\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2667 - acc: 0.9015 - val_loss: 0.2698 - val_acc: 0.8962\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.2552 - acc: 0.9063 - val_loss: 0.2613 - val_acc: 0.8992\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.2451 - acc: 0.9097 - val_loss: 0.2478 - val_acc: 0.9058\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2354 - acc: 0.9137 - val_loss: 0.2402 - val_acc: 0.9071\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2277 - acc: 0.9159 - val_loss: 0.2356 - val_acc: 0.9089\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.2203 - acc: 0.9194 - val_loss: 0.2253 - val_acc: 0.9141\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.2139 - acc: 0.9220 - val_loss: 0.2237 - val_acc: 0.9129\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2084 - acc: 0.9246 - val_loss: 0.2155 - val_acc: 0.9171\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2042 - acc: 0.9258 - val_loss: 0.2113 - val_acc: 0.9189\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1994 - acc: 0.9279 - val_loss: 0.2086 - val_acc: 0.9194\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1961 - acc: 0.9291 - val_loss: 0.2052 - val_acc: 0.9211\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1926 - acc: 0.9302 - val_loss: 0.2034 - val_acc: 0.9221\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1888 - acc: 0.9316 - val_loss: 0.1984 - val_acc: 0.9247\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1858 - acc: 0.9335 - val_loss: 0.1981 - val_acc: 0.9238\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1830 - acc: 0.9339 - val_loss: 0.1936 - val_acc: 0.9275\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1804 - acc: 0.9351 - val_loss: 0.1912 - val_acc: 0.9274\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1779 - acc: 0.9364 - val_loss: 0.1900 - val_acc: 0.9269\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1764 - acc: 0.9358 - val_loss: 0.1884 - val_acc: 0.9267\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1737 - acc: 0.9373 - val_loss: 0.1862 - val_acc: 0.9283\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1710 - acc: 0.9385 - val_loss: 0.1839 - val_acc: 0.9307\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1689 - acc: 0.9396 - val_loss: 0.1823 - val_acc: 0.9315\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1669 - acc: 0.9403 - val_loss: 0.1807 - val_acc: 0.9305\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1662 - acc: 0.9400 - val_loss: 0.1832 - val_acc: 0.9284\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1650 - acc: 0.9397 - val_loss: 0.1780 - val_acc: 0.9319\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1614 - acc: 0.9415 - val_loss: 0.1764 - val_acc: 0.9323\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1594 - acc: 0.9430 - val_loss: 0.1761 - val_acc: 0.9321\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1586 - acc: 0.9431 - val_loss: 0.1730 - val_acc: 0.9335\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1570 - acc: 0.9439 - val_loss: 0.1769 - val_acc: 0.9317\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1571 - acc: 0.9436 - val_loss: 0.1730 - val_acc: 0.9332\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1547 - acc: 0.9446 - val_loss: 0.1690 - val_acc: 0.9348\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1525 - acc: 0.9457 - val_loss: 0.1697 - val_acc: 0.9354\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1511 - acc: 0.9465 - val_loss: 0.1684 - val_acc: 0.9354\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1503 - acc: 0.9468 - val_loss: 0.1662 - val_acc: 0.9358\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1487 - acc: 0.9475 - val_loss: 0.1665 - val_acc: 0.9356\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1483 - acc: 0.9465 - val_loss: 0.1640 - val_acc: 0.9373\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1463 - acc: 0.9482 - val_loss: 0.1633 - val_acc: 0.9379\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1450 - acc: 0.9484 - val_loss: 0.1628 - val_acc: 0.9374\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1438 - acc: 0.9492 - val_loss: 0.1610 - val_acc: 0.9378\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1431 - acc: 0.9497 - val_loss: 0.1618 - val_acc: 0.9369\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1428 - acc: 0.9487 - val_loss: 0.1595 - val_acc: 0.9384\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1407 - acc: 0.9506 - val_loss: 0.1582 - val_acc: 0.9396\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1398 - acc: 0.9508 - val_loss: 0.1591 - val_acc: 0.9398\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1385 - acc: 0.9521 - val_loss: 0.1578 - val_acc: 0.9396\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1381 - acc: 0.9516 - val_loss: 0.1563 - val_acc: 0.9409\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1379 - acc: 0.9513 - val_loss: 0.1562 - val_acc: 0.9416\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1359 - acc: 0.9528 - val_loss: 0.1551 - val_acc: 0.9411\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1350 - acc: 0.9534 - val_loss: 0.1538 - val_acc: 0.9417\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1341 - acc: 0.9536 - val_loss: 0.1536 - val_acc: 0.9422\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1330 - acc: 0.9540 - val_loss: 0.1533 - val_acc: 0.9409\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1327 - acc: 0.9540 - val_loss: 0.1521 - val_acc: 0.9429\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1310 - acc: 0.9555 - val_loss: 0.1541 - val_acc: 0.9405\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1312 - acc: 0.9548 - val_loss: 0.1512 - val_acc: 0.9426\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1301 - acc: 0.9548 - val_loss: 0.1498 - val_acc: 0.9435\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1299 - acc: 0.9552 - val_loss: 0.1504 - val_acc: 0.9425\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1284 - acc: 0.9559 - val_loss: 0.1486 - val_acc: 0.9440\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1272 - acc: 0.9565 - val_loss: 0.1485 - val_acc: 0.9435\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1269 - acc: 0.9567 - val_loss: 0.1477 - val_acc: 0.9445\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1266 - acc: 0.9566 - val_loss: 0.1522 - val_acc: 0.9425\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1259 - acc: 0.9574 - val_loss: 0.1483 - val_acc: 0.9436\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1246 - acc: 0.9578 - val_loss: 0.1540 - val_acc: 0.9399\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1259 - acc: 0.9565 - val_loss: 0.1459 - val_acc: 0.9443\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1226 - acc: 0.9590 - val_loss: 0.1483 - val_acc: 0.9433\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1233 - acc: 0.9576 - val_loss: 0.1448 - val_acc: 0.9457\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1216 - acc: 0.9586 - val_loss: 0.1458 - val_acc: 0.9449\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.1213 - acc: 0.9593 - val_loss: 0.1466 - val_acc: 0.9436\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1201 - acc: 0.9599 - val_loss: 0.1431 - val_acc: 0.9458\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1197 - acc: 0.9599 - val_loss: 0.1453 - val_acc: 0.9441\n",
      "Epoch 79/200\n",
      "14500/17142 [========================>.....] - ETA: 0s - loss: 0.1188 - acc: 0.9605"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.1554 - acc: 0.9450 - val_loss: 0.1723 - val_acc: 0.9331\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1538 - acc: 0.9463 - val_loss: 0.1693 - val_acc: 0.9339\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1530 - acc: 0.9463 - val_loss: 0.1668 - val_acc: 0.9356\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1509 - acc: 0.9477 - val_loss: 0.1672 - val_acc: 0.9353\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1490 - acc: 0.9483 - val_loss: 0.1648 - val_acc: 0.9362\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1487 - acc: 0.9479 - val_loss: 0.1671 - val_acc: 0.9348\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1465 - acc: 0.9494 - val_loss: 0.1628 - val_acc: 0.9370\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1454 - acc: 0.9488 - val_loss: 0.1631 - val_acc: 0.9372\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1453 - acc: 0.9485 - val_loss: 0.1614 - val_acc: 0.9376\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1441 - acc: 0.9494 - val_loss: 0.1605 - val_acc: 0.9385\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1419 - acc: 0.9509 - val_loss: 0.1614 - val_acc: 0.9379\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1405 - acc: 0.9512 - val_loss: 0.1586 - val_acc: 0.9390\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1395 - acc: 0.9528 - val_loss: 0.1581 - val_acc: 0.9392\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1386 - acc: 0.9525 - val_loss: 0.1573 - val_acc: 0.9402\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1382 - acc: 0.9528 - val_loss: 0.1565 - val_acc: 0.9407\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1366 - acc: 0.9533 - val_loss: 0.1570 - val_acc: 0.9407\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1365 - acc: 0.9533 - val_loss: 0.1548 - val_acc: 0.9407\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1350 - acc: 0.9540 - val_loss: 0.1553 - val_acc: 0.9413\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1337 - acc: 0.9540 - val_loss: 0.1544 - val_acc: 0.9407\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1333 - acc: 0.9551 - val_loss: 0.1544 - val_acc: 0.9407\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1323 - acc: 0.9548 - val_loss: 0.1552 - val_acc: 0.9392\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1320 - acc: 0.9553 - val_loss: 0.1520 - val_acc: 0.9422\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1306 - acc: 0.9551 - val_loss: 0.1536 - val_acc: 0.9405\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1298 - acc: 0.9556 - val_loss: 0.1503 - val_acc: 0.9430\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1289 - acc: 0.9559 - val_loss: 0.1496 - val_acc: 0.9431\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1279 - acc: 0.9566 - val_loss: 0.1503 - val_acc: 0.9432\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1280 - acc: 0.9565 - val_loss: 0.1576 - val_acc: 0.9382\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1277 - acc: 0.9564 - val_loss: 0.1479 - val_acc: 0.9444\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1257 - acc: 0.9580 - val_loss: 0.1530 - val_acc: 0.9418\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1259 - acc: 0.9570 - val_loss: 0.1473 - val_acc: 0.9448\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1248 - acc: 0.9577 - val_loss: 0.1467 - val_acc: 0.9444\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1249 - acc: 0.9570 - val_loss: 0.1458 - val_acc: 0.9447\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1236 - acc: 0.9582 - val_loss: 0.1454 - val_acc: 0.9449\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1223 - acc: 0.9585 - val_loss: 0.1447 - val_acc: 0.9449\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1213 - acc: 0.9591 - val_loss: 0.1449 - val_acc: 0.9451\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1212 - acc: 0.9598 - val_loss: 0.1439 - val_acc: 0.9454\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1205 - acc: 0.9598 - val_loss: 0.1426 - val_acc: 0.9456\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1197 - acc: 0.9596 - val_loss: 0.1423 - val_acc: 0.9461\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1188 - acc: 0.9604 - val_loss: 0.1444 - val_acc: 0.9452\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1186 - acc: 0.9606 - val_loss: 0.1418 - val_acc: 0.9462\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1176 - acc: 0.9607 - val_loss: 0.1416 - val_acc: 0.9456\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1179 - acc: 0.9602 - val_loss: 0.1415 - val_acc: 0.9459\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1172 - acc: 0.9613 - val_loss: 0.1405 - val_acc: 0.9464\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1162 - acc: 0.9614 - val_loss: 0.1398 - val_acc: 0.9468\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1157 - acc: 0.9611 - val_loss: 0.1435 - val_acc: 0.9443\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1152 - acc: 0.9613 - val_loss: 0.1391 - val_acc: 0.9468\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1149 - acc: 0.9617 - val_loss: 0.1403 - val_acc: 0.9470\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1148 - acc: 0.9616 - val_loss: 0.1384 - val_acc: 0.9474\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1133 - acc: 0.9626 - val_loss: 0.1380 - val_acc: 0.9476\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1130 - acc: 0.9628 - val_loss: 0.1375 - val_acc: 0.9481\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1137 - acc: 0.9613 - val_loss: 0.1390 - val_acc: 0.9462\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1127 - acc: 0.9620 - val_loss: 0.1374 - val_acc: 0.9483\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1116 - acc: 0.9632 - val_loss: 0.1425 - val_acc: 0.9451\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1119 - acc: 0.9622 - val_loss: 0.1443 - val_acc: 0.9445\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1127 - acc: 0.9623 - val_loss: 0.1362 - val_acc: 0.9484\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1102 - acc: 0.9635 - val_loss: 0.1359 - val_acc: 0.9483\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1103 - acc: 0.9630 - val_loss: 0.1397 - val_acc: 0.9466\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1104 - acc: 0.9628 - val_loss: 0.1357 - val_acc: 0.9484\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1092 - acc: 0.9637 - val_loss: 0.1362 - val_acc: 0.9483\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1084 - acc: 0.9641 - val_loss: 0.1347 - val_acc: 0.9491\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1081 - acc: 0.9641 - val_loss: 0.1373 - val_acc: 0.9474\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1080 - acc: 0.9643 - val_loss: 0.1352 - val_acc: 0.9482\n",
      "Epoch 98/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1073 - acc: 0.9647 - val_loss: 0.1351 - val_acc: 0.9479\n",
      "Epoch 99/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1076 - acc: 0.9643 - val_loss: 0.1338 - val_acc: 0.9492\n",
      "Epoch 100/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1068 - acc: 0.9649 - val_loss: 0.1350 - val_acc: 0.9485\n",
      "Epoch 101/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1068 - acc: 0.9653 - val_loss: 0.1337 - val_acc: 0.9488\n",
      "Epoch 102/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1064 - acc: 0.9652 - val_loss: 0.1335 - val_acc: 0.9495\n",
      "Epoch 103/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1069 - acc: 0.9642 - val_loss: 0.1359 - val_acc: 0.9477\n",
      "Epoch 104/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1055 - acc: 0.9654 - val_loss: 0.1333 - val_acc: 0.9492\n",
      "Epoch 105/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1051 - acc: 0.9654 - val_loss: 0.1330 - val_acc: 0.9493\n",
      "Epoch 106/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1051 - acc: 0.9658 - val_loss: 0.1327 - val_acc: 0.9493\n",
      "Epoch 107/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1050 - acc: 0.9654 - val_loss: 0.1337 - val_acc: 0.9487\n",
      "Epoch 108/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1041 - acc: 0.9655 - val_loss: 0.1355 - val_acc: 0.9487\n",
      "Epoch 109/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1034 - acc: 0.9662 - val_loss: 0.1326 - val_acc: 0.9493\n",
      "Epoch 110/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1031 - acc: 0.9667 - val_loss: 0.1332 - val_acc: 0.9500\n",
      "Epoch 111/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1029 - acc: 0.9663 - val_loss: 0.1329 - val_acc: 0.9489\n",
      "Epoch 112/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1031 - acc: 0.9659 - val_loss: 0.1324 - val_acc: 0.9497\n",
      "Epoch 113/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1027 - acc: 0.9665 - val_loss: 0.1363 - val_acc: 0.9471\n",
      "Epoch 114/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1024 - acc: 0.9663 - val_loss: 0.1338 - val_acc: 0.9491\n",
      "Epoch 115/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1018 - acc: 0.9667 - val_loss: 0.1330 - val_acc: 0.9481\n",
      "Epoch 116/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1015 - acc: 0.9671 - val_loss: 0.1316 - val_acc: 0.9499\n",
      "Epoch 117/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1010 - acc: 0.9671 - val_loss: 0.1310 - val_acc: 0.9501\n",
      "Epoch 118/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1004 - acc: 0.9673 - val_loss: 0.1318 - val_acc: 0.9496\n",
      "Epoch 119/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1010 - acc: 0.9668 - val_loss: 0.1327 - val_acc: 0.9494\n",
      "Epoch 120/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1013 - acc: 0.9664 - val_loss: 0.1307 - val_acc: 0.9500\n",
      "Epoch 121/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0997 - acc: 0.9673 - val_loss: 0.1310 - val_acc: 0.9500\n",
      "Epoch 122/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0995 - acc: 0.9673 - val_loss: 0.1307 - val_acc: 0.9502\n",
      "Epoch 123/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0995 - acc: 0.9674 - val_loss: 0.1317 - val_acc: 0.9500\n",
      "Epoch 124/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1010 - acc: 0.9660 - val_loss: 0.1318 - val_acc: 0.9492\n",
      "Epoch 125/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0995 - acc: 0.9676 - val_loss: 0.1305 - val_acc: 0.9499\n",
      "Epoch 126/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0987 - acc: 0.9675 - val_loss: 0.1302 - val_acc: 0.9507\n",
      "Epoch 127/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0988 - acc: 0.9681 - val_loss: 0.1303 - val_acc: 0.9501\n",
      "Epoch 128/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0983 - acc: 0.9673 - val_loss: 0.1311 - val_acc: 0.9491\n",
      "Epoch 129/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0985 - acc: 0.9674 - val_loss: 0.1323 - val_acc: 0.9489\n",
      "Epoch 130/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0981 - acc: 0.9676 - val_loss: 0.1293 - val_acc: 0.9507\n",
      "Epoch 131/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0981 - acc: 0.9674 - val_loss: 0.1307 - val_acc: 0.9500\n",
      "Epoch 132/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0973 - acc: 0.9681 - val_loss: 0.1300 - val_acc: 0.9502\n",
      "Epoch 133/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0977 - acc: 0.9682 - val_loss: 0.1291 - val_acc: 0.9506\n",
      "Epoch 134/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0968 - acc: 0.9684 - val_loss: 0.1312 - val_acc: 0.9490\n",
      "Epoch 135/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0973 - acc: 0.9677 - val_loss: 0.1289 - val_acc: 0.9505\n",
      "Epoch 136/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0963 - acc: 0.9687 - val_loss: 0.1290 - val_acc: 0.9506\n",
      "Epoch 137/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0961 - acc: 0.9685 - val_loss: 0.1292 - val_acc: 0.9503\n",
      "Epoch 138/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0960 - acc: 0.9685 - val_loss: 0.1294 - val_acc: 0.9503\n",
      "Epoch 139/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0961 - acc: 0.9687 - val_loss: 0.1295 - val_acc: 0.9501\n",
      "Epoch 140/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0956 - acc: 0.9691 - val_loss: 0.1285 - val_acc: 0.9507\n",
      "Epoch 141/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0955 - acc: 0.9687 - val_loss: 0.1327 - val_acc: 0.9481\n",
      "Epoch 142/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0968 - acc: 0.9677 - val_loss: 0.1338 - val_acc: 0.9468\n",
      "Epoch 143/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0971 - acc: 0.9681 - val_loss: 0.1313 - val_acc: 0.9487\n",
      "Epoch 144/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0964 - acc: 0.9684 - val_loss: 0.1300 - val_acc: 0.9491\n",
      "Epoch 145/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0951 - acc: 0.9689 - val_loss: 0.1293 - val_acc: 0.9502\n",
      "Epoch 146/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0943 - acc: 0.9691 - val_loss: 0.1300 - val_acc: 0.9503\n",
      "Epoch 147/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0948 - acc: 0.9684 - val_loss: 0.1319 - val_acc: 0.9492\n",
      "Epoch 148/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0946 - acc: 0.9685 - val_loss: 0.1312 - val_acc: 0.9501\n",
      "Epoch 149/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0940 - acc: 0.9692 - val_loss: 0.1279 - val_acc: 0.9501\n",
      "Epoch 150/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0942 - acc: 0.9693 - val_loss: 0.1282 - val_acc: 0.9505\n",
      "Epoch 151/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0940 - acc: 0.9698 - val_loss: 0.1293 - val_acc: 0.9496\n",
      "Epoch 152/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0936 - acc: 0.9693 - val_loss: 0.1278 - val_acc: 0.9510\n",
      "Epoch 153/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0931 - acc: 0.9702 - val_loss: 0.1305 - val_acc: 0.9490\n",
      "Epoch 154/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0937 - acc: 0.9695 - val_loss: 0.1287 - val_acc: 0.9502\n",
      "Epoch 155/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0927 - acc: 0.9702 - val_loss: 0.1285 - val_acc: 0.9507\n",
      "Epoch 156/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0922 - acc: 0.9703 - val_loss: 0.1289 - val_acc: 0.9499\n",
      "Epoch 157/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0922 - acc: 0.9701 - val_loss: 0.1319 - val_acc: 0.9498\n",
      "Epoch 158/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0927 - acc: 0.9700 - val_loss: 0.1284 - val_acc: 0.9510\n",
      "Epoch 159/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0917 - acc: 0.9702 - val_loss: 0.1280 - val_acc: 0.9510\n",
      "Epoch 160/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0918 - acc: 0.9701 - val_loss: 0.1274 - val_acc: 0.9507\n",
      "Epoch 161/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0916 - acc: 0.9703 - val_loss: 0.1276 - val_acc: 0.9510\n",
      "Epoch 162/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.0912 - acc: 0.9706 - val_loss: 0.1295 - val_acc: 0.9499\n",
      "Epoch 163/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0915 - acc: 0.9703 - val_loss: 0.1274 - val_acc: 0.9508\n",
      "Epoch 164/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0920 - acc: 0.9704 - val_loss: 0.1300 - val_acc: 0.9507\n",
      "Epoch 165/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0921 - acc: 0.9698 - val_loss: 0.1303 - val_acc: 0.9492\n",
      "Epoch 166/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0909 - acc: 0.9705 - val_loss: 0.1275 - val_acc: 0.9506\n",
      "Epoch 167/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0909 - acc: 0.9707 - val_loss: 0.1271 - val_acc: 0.9514\n",
      "Epoch 168/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0916 - acc: 0.9702 - val_loss: 0.1298 - val_acc: 0.9511\n",
      "Epoch 169/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0912 - acc: 0.9705 - val_loss: 0.1273 - val_acc: 0.9512\n",
      "Epoch 170/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0901 - acc: 0.9712 - val_loss: 0.1274 - val_acc: 0.9508\n",
      "Epoch 171/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0900 - acc: 0.9705 - val_loss: 0.1272 - val_acc: 0.9511\n",
      "Epoch 172/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0897 - acc: 0.9709 - val_loss: 0.1288 - val_acc: 0.9505\n",
      "Epoch 173/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0902 - acc: 0.9709 - val_loss: 0.1296 - val_acc: 0.9500\n",
      "Epoch 174/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0901 - acc: 0.9703 - val_loss: 0.1268 - val_acc: 0.9514\n",
      "Epoch 175/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0898 - acc: 0.9708 - val_loss: 0.1270 - val_acc: 0.9515\n",
      "Epoch 176/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0896 - acc: 0.9712 - val_loss: 0.1296 - val_acc: 0.9499\n",
      "Epoch 177/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0904 - acc: 0.9702 - val_loss: 0.1267 - val_acc: 0.9514\n",
      "Epoch 178/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0898 - acc: 0.9711 - val_loss: 0.1271 - val_acc: 0.9519\n",
      "Epoch 179/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0892 - acc: 0.9709 - val_loss: 0.1280 - val_acc: 0.9510\n",
      "Epoch 180/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.0890 - acc: 0.9713 - val_loss: 0.1266 - val_acc: 0.9518\n",
      "Epoch 181/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0887 - acc: 0.9716 - val_loss: 0.1277 - val_acc: 0.9512\n",
      "Epoch 182/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0882 - acc: 0.9717 - val_loss: 0.1272 - val_acc: 0.9514\n",
      "Epoch 183/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0884 - acc: 0.9710 - val_loss: 0.1270 - val_acc: 0.9509\n",
      "Epoch 184/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0887 - acc: 0.9708 - val_loss: 0.1269 - val_acc: 0.9514\n",
      "Epoch 185/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0879 - acc: 0.9721 - val_loss: 0.1270 - val_acc: 0.9514\n",
      "Epoch 186/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0881 - acc: 0.9714 - val_loss: 0.1305 - val_acc: 0.9499\n",
      "Epoch 187/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0881 - acc: 0.9714 - val_loss: 0.1293 - val_acc: 0.9503\n",
      "Epoch 188/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.0879 - acc: 0.9718 - val_loss: 0.1270 - val_acc: 0.9514\n",
      "Epoch 189/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.0883 - acc: 0.9709 - val_loss: 0.1287 - val_acc: 0.9504\n",
      "Epoch 190/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.0891 - acc: 0.9708 - val_loss: 0.1288 - val_acc: 0.9509\n",
      "auroc: 0.9862001661420446\n",
      "auprc: 0.9776458242248292\n",
      "auroc: 0.9861964209826839\n",
      "auprc: 0.9776141917100803\n",
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 8s 438us/step - loss: 0.6676 - acc: 0.6368 - val_loss: 0.6149 - val_acc: 0.6766\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.5873 - acc: 0.6920 - val_loss: 0.5675 - val_acc: 0.7047\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.5368 - acc: 0.7302 - val_loss: 0.5191 - val_acc: 0.7359\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.4875 - acc: 0.7637 - val_loss: 0.4707 - val_acc: 0.7723\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.4406 - acc: 0.7996 - val_loss: 0.4270 - val_acc: 0.8009\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.3976 - acc: 0.8310 - val_loss: 0.3859 - val_acc: 0.8334\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.3583 - acc: 0.8568 - val_loss: 0.3469 - val_acc: 0.8599\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.3250 - acc: 0.8778 - val_loss: 0.3167 - val_acc: 0.8751\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2980 - acc: 0.8903 - val_loss: 0.2931 - val_acc: 0.8880\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.2771 - acc: 0.8990 - val_loss: 0.2769 - val_acc: 0.8949\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.2610 - acc: 0.9050 - val_loss: 0.2643 - val_acc: 0.8948\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.2488 - acc: 0.9088 - val_loss: 0.2521 - val_acc: 0.9045\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2373 - acc: 0.9133 - val_loss: 0.2396 - val_acc: 0.9083\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.2286 - acc: 0.9167 - val_loss: 0.2309 - val_acc: 0.9133\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.2210 - acc: 0.9198 - val_loss: 0.2247 - val_acc: 0.9147\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.2149 - acc: 0.9224 - val_loss: 0.2189 - val_acc: 0.9169\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.2093 - acc: 0.9240 - val_loss: 0.2175 - val_acc: 0.9169\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.2049 - acc: 0.9249 - val_loss: 0.2097 - val_acc: 0.9205\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.1996 - acc: 0.9273 - val_loss: 0.2072 - val_acc: 0.9209\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.1956 - acc: 0.9291 - val_loss: 0.2028 - val_acc: 0.9240\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1924 - acc: 0.9306 - val_loss: 0.1996 - val_acc: 0.9239\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1887 - acc: 0.9321 - val_loss: 0.1982 - val_acc: 0.9242\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1851 - acc: 0.9327 - val_loss: 0.1962 - val_acc: 0.9233\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1824 - acc: 0.9340 - val_loss: 0.1919 - val_acc: 0.9268\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1802 - acc: 0.9353 - val_loss: 0.1900 - val_acc: 0.9274\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1780 - acc: 0.9351 - val_loss: 0.1893 - val_acc: 0.9267\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1751 - acc: 0.9373 - val_loss: 0.1859 - val_acc: 0.9291\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1721 - acc: 0.9385 - val_loss: 0.1846 - val_acc: 0.9291\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.1701 - acc: 0.9391 - val_loss: 0.1824 - val_acc: 0.9307\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1686 - acc: 0.9397 - val_loss: 0.1837 - val_acc: 0.9279\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1669 - acc: 0.9391 - val_loss: 0.1787 - val_acc: 0.9310\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1643 - acc: 0.9413 - val_loss: 0.1785 - val_acc: 0.9305\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1626 - acc: 0.9419 - val_loss: 0.1781 - val_acc: 0.9309\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1615 - acc: 0.9422 - val_loss: 0.1755 - val_acc: 0.9320\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1598 - acc: 0.9427 - val_loss: 0.1735 - val_acc: 0.9325\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1588 - acc: 0.9430 - val_loss: 0.1749 - val_acc: 0.9314\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1570 - acc: 0.9439 - val_loss: 0.1715 - val_acc: 0.9329\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1556 - acc: 0.9441 - val_loss: 0.1741 - val_acc: 0.9327\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1538 - acc: 0.9451 - val_loss: 0.1684 - val_acc: 0.9349\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1524 - acc: 0.9451 - val_loss: 0.1675 - val_acc: 0.9350\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1507 - acc: 0.9465 - val_loss: 0.1663 - val_acc: 0.9358\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1496 - acc: 0.9466 - val_loss: 0.1659 - val_acc: 0.9352\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1485 - acc: 0.9466 - val_loss: 0.1638 - val_acc: 0.9367\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1470 - acc: 0.9478 - val_loss: 0.1630 - val_acc: 0.9378\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1453 - acc: 0.9489 - val_loss: 0.1630 - val_acc: 0.9367\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1446 - acc: 0.9487 - val_loss: 0.1620 - val_acc: 0.9381\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1440 - acc: 0.9491 - val_loss: 0.1624 - val_acc: 0.9379\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1423 - acc: 0.9501 - val_loss: 0.1592 - val_acc: 0.9385\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1413 - acc: 0.9508 - val_loss: 0.1579 - val_acc: 0.9399\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1407 - acc: 0.9501 - val_loss: 0.1586 - val_acc: 0.9390\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1393 - acc: 0.9505 - val_loss: 0.1572 - val_acc: 0.9391\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1381 - acc: 0.9518 - val_loss: 0.1586 - val_acc: 0.9382\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1382 - acc: 0.9516 - val_loss: 0.1555 - val_acc: 0.9406\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1363 - acc: 0.9520 - val_loss: 0.1544 - val_acc: 0.9418\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1349 - acc: 0.9535 - val_loss: 0.1542 - val_acc: 0.9416\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1345 - acc: 0.9531 - val_loss: 0.1526 - val_acc: 0.9416\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1347 - acc: 0.9529 - val_loss: 0.1524 - val_acc: 0.9421\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1330 - acc: 0.9537 - val_loss: 0.1512 - val_acc: 0.9422\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1319 - acc: 0.9540 - val_loss: 0.1515 - val_acc: 0.9421\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1310 - acc: 0.9547 - val_loss: 0.1525 - val_acc: 0.9417\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1301 - acc: 0.9550 - val_loss: 0.1506 - val_acc: 0.9412\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.1306 - acc: 0.9544 - val_loss: 0.1512 - val_acc: 0.9419\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1282 - acc: 0.9557 - val_loss: 0.1478 - val_acc: 0.9439\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1278 - acc: 0.9558 - val_loss: 0.1471 - val_acc: 0.9438\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1266 - acc: 0.9564 - val_loss: 0.1474 - val_acc: 0.9434\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1259 - acc: 0.9567 - val_loss: 0.1462 - val_acc: 0.9443\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1265 - acc: 0.9563 - val_loss: 0.1470 - val_acc: 0.9444\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1253 - acc: 0.9574 - val_loss: 0.1486 - val_acc: 0.9425\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1253 - acc: 0.9563 - val_loss: 0.1446 - val_acc: 0.9450\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1238 - acc: 0.9574 - val_loss: 0.1478 - val_acc: 0.9441\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1234 - acc: 0.9577 - val_loss: 0.1437 - val_acc: 0.9457\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1217 - acc: 0.9586 - val_loss: 0.1432 - val_acc: 0.9458\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1212 - acc: 0.9588 - val_loss: 0.1434 - val_acc: 0.9454\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1205 - acc: 0.9594 - val_loss: 0.1428 - val_acc: 0.9463\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1203 - acc: 0.9588 - val_loss: 0.1421 - val_acc: 0.9467\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1199 - acc: 0.9592 - val_loss: 0.1416 - val_acc: 0.9467\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1191 - acc: 0.9597 - val_loss: 0.1421 - val_acc: 0.9452\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1180 - acc: 0.9600 - val_loss: 0.1420 - val_acc: 0.9460\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1175 - acc: 0.9607 - val_loss: 0.1420 - val_acc: 0.9462\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1174 - acc: 0.9605 - val_loss: 0.1405 - val_acc: 0.9469\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.1164 - acc: 0.9613 - val_loss: 0.1400 - val_acc: 0.9467\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1164 - acc: 0.9610 - val_loss: 0.1405 - val_acc: 0.9465\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1155 - acc: 0.9615 - val_loss: 0.1391 - val_acc: 0.9482\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1156 - acc: 0.9613 - val_loss: 0.1385 - val_acc: 0.9480\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1146 - acc: 0.9615 - val_loss: 0.1392 - val_acc: 0.9479\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1138 - acc: 0.9630 - val_loss: 0.1385 - val_acc: 0.9478\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1136 - acc: 0.9622 - val_loss: 0.1377 - val_acc: 0.9478\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1139 - acc: 0.9617 - val_loss: 0.1378 - val_acc: 0.9484\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1130 - acc: 0.9622 - val_loss: 0.1374 - val_acc: 0.9479\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 1s 32us/step - loss: 0.1126 - acc: 0.9625 - val_loss: 0.1372 - val_acc: 0.9481\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1128 - acc: 0.9620 - val_loss: 0.1406 - val_acc: 0.9468\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1115 - acc: 0.9631 - val_loss: 0.1364 - val_acc: 0.9487\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1111 - acc: 0.9633 - val_loss: 0.1365 - val_acc: 0.9491\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1100 - acc: 0.9634 - val_loss: 0.1361 - val_acc: 0.9485\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1094 - acc: 0.9638 - val_loss: 0.1377 - val_acc: 0.9477\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1096 - acc: 0.9638 - val_loss: 0.1353 - val_acc: 0.9486\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1088 - acc: 0.9646 - val_loss: 0.1372 - val_acc: 0.9470\n",
      "Epoch 98/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1090 - acc: 0.9638 - val_loss: 0.1363 - val_acc: 0.9479\n",
      "Epoch 99/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1080 - acc: 0.9645 - val_loss: 0.1348 - val_acc: 0.9488\n",
      "Epoch 100/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1077 - acc: 0.9642 - val_loss: 0.1351 - val_acc: 0.9481\n",
      "Epoch 101/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1073 - acc: 0.9644 - val_loss: 0.1368 - val_acc: 0.9479\n",
      "Epoch 102/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1082 - acc: 0.9636 - val_loss: 0.1348 - val_acc: 0.9483\n",
      "Epoch 103/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1067 - acc: 0.9654 - val_loss: 0.1336 - val_acc: 0.9492\n",
      "Epoch 104/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1067 - acc: 0.9653 - val_loss: 0.1341 - val_acc: 0.9490\n",
      "Epoch 105/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1053 - acc: 0.9660 - val_loss: 0.1341 - val_acc: 0.9496\n",
      "Epoch 106/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1054 - acc: 0.9655 - val_loss: 0.1330 - val_acc: 0.9488\n",
      "Epoch 107/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1052 - acc: 0.9655 - val_loss: 0.1329 - val_acc: 0.9491\n",
      "Epoch 108/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1042 - acc: 0.9655 - val_loss: 0.1331 - val_acc: 0.9499\n",
      "Epoch 109/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1051 - acc: 0.9654 - val_loss: 0.1331 - val_acc: 0.9491\n",
      "Epoch 110/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1040 - acc: 0.9657 - val_loss: 0.1332 - val_acc: 0.9494\n",
      "Epoch 111/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1034 - acc: 0.9663 - val_loss: 0.1323 - val_acc: 0.9492\n",
      "Epoch 112/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1040 - acc: 0.9662 - val_loss: 0.1322 - val_acc: 0.9497\n",
      "Epoch 113/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1043 - acc: 0.9656 - val_loss: 0.1332 - val_acc: 0.9496\n",
      "Epoch 114/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1025 - acc: 0.9661 - val_loss: 0.1325 - val_acc: 0.9497\n",
      "Epoch 115/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1024 - acc: 0.9662 - val_loss: 0.1315 - val_acc: 0.9502\n",
      "Epoch 116/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1028 - acc: 0.9667 - val_loss: 0.1318 - val_acc: 0.9497\n",
      "Epoch 117/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1015 - acc: 0.9673 - val_loss: 0.1311 - val_acc: 0.9499\n",
      "Epoch 118/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1014 - acc: 0.9669 - val_loss: 0.1307 - val_acc: 0.9499\n",
      "Epoch 119/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1009 - acc: 0.9677 - val_loss: 0.1329 - val_acc: 0.9492\n",
      "Epoch 120/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1005 - acc: 0.9670 - val_loss: 0.1323 - val_acc: 0.9494\n",
      "Epoch 121/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1013 - acc: 0.9668 - val_loss: 0.1305 - val_acc: 0.9500\n",
      "Epoch 122/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1005 - acc: 0.9674 - val_loss: 0.1333 - val_acc: 0.9497\n",
      "Epoch 123/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.1009 - acc: 0.9666 - val_loss: 0.1314 - val_acc: 0.9493\n",
      "Epoch 124/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.1001 - acc: 0.9676 - val_loss: 0.1310 - val_acc: 0.9501\n",
      "Epoch 125/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.0995 - acc: 0.9673 - val_loss: 0.1311 - val_acc: 0.9500\n",
      "Epoch 126/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.1012 - acc: 0.9656 - val_loss: 0.1305 - val_acc: 0.9496\n",
      "Epoch 127/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.0993 - acc: 0.9673 - val_loss: 0.1306 - val_acc: 0.9500\n",
      "Epoch 128/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0987 - acc: 0.9676 - val_loss: 0.1317 - val_acc: 0.9501\n",
      "Epoch 129/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0986 - acc: 0.9681 - val_loss: 0.1364 - val_acc: 0.9488\n",
      "Epoch 130/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0987 - acc: 0.9676 - val_loss: 0.1308 - val_acc: 0.9506\n",
      "Epoch 131/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0979 - acc: 0.9678 - val_loss: 0.1308 - val_acc: 0.9504\n",
      "Epoch 132/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0990 - acc: 0.9676 - val_loss: 0.1302 - val_acc: 0.9508\n",
      "Epoch 133/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0979 - acc: 0.9680 - val_loss: 0.1305 - val_acc: 0.9508\n",
      "Epoch 134/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0974 - acc: 0.9683 - val_loss: 0.1293 - val_acc: 0.9502\n",
      "Epoch 135/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0970 - acc: 0.9678 - val_loss: 0.1298 - val_acc: 0.9504\n",
      "Epoch 136/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0964 - acc: 0.9689 - val_loss: 0.1295 - val_acc: 0.9502\n",
      "Epoch 137/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0970 - acc: 0.9682 - val_loss: 0.1318 - val_acc: 0.9500\n",
      "Epoch 138/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.0959 - acc: 0.9685 - val_loss: 0.1301 - val_acc: 0.9505\n",
      "Epoch 139/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0956 - acc: 0.9690 - val_loss: 0.1291 - val_acc: 0.9504\n",
      "Epoch 140/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0959 - acc: 0.9682 - val_loss: 0.1306 - val_acc: 0.9498\n",
      "Epoch 141/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0956 - acc: 0.9686 - val_loss: 0.1290 - val_acc: 0.9506\n",
      "Epoch 142/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0955 - acc: 0.9688 - val_loss: 0.1291 - val_acc: 0.9511\n",
      "Epoch 143/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0958 - acc: 0.9680 - val_loss: 0.1301 - val_acc: 0.9506\n",
      "Epoch 144/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0954 - acc: 0.9688 - val_loss: 0.1294 - val_acc: 0.9502\n",
      "Epoch 145/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0949 - acc: 0.9688 - val_loss: 0.1285 - val_acc: 0.9512\n",
      "Epoch 146/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.0945 - acc: 0.9693 - val_loss: 0.1311 - val_acc: 0.9504\n",
      "Epoch 147/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0947 - acc: 0.9687 - val_loss: 0.1288 - val_acc: 0.9508\n",
      "Epoch 148/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0938 - acc: 0.9691 - val_loss: 0.1289 - val_acc: 0.9504\n",
      "Epoch 149/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.0942 - acc: 0.9691 - val_loss: 0.1284 - val_acc: 0.9506\n",
      "Epoch 150/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0941 - acc: 0.9692 - val_loss: 0.1323 - val_acc: 0.9501\n",
      "Epoch 151/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0934 - acc: 0.9691 - val_loss: 0.1281 - val_acc: 0.9509\n",
      "Epoch 152/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0944 - acc: 0.9690 - val_loss: 0.1291 - val_acc: 0.9505\n",
      "Epoch 153/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0937 - acc: 0.9692 - val_loss: 0.1283 - val_acc: 0.9513\n",
      "Epoch 154/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0937 - acc: 0.9691 - val_loss: 0.1288 - val_acc: 0.9501\n",
      "Epoch 155/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0929 - acc: 0.9694 - val_loss: 0.1283 - val_acc: 0.9503\n",
      "Epoch 156/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0926 - acc: 0.9698 - val_loss: 0.1276 - val_acc: 0.9518\n",
      "Epoch 157/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0929 - acc: 0.9696 - val_loss: 0.1294 - val_acc: 0.9508\n",
      "Epoch 158/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0930 - acc: 0.9694 - val_loss: 0.1293 - val_acc: 0.9508\n",
      "Epoch 159/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0925 - acc: 0.9697 - val_loss: 0.1287 - val_acc: 0.9506\n",
      "Epoch 160/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0925 - acc: 0.9695 - val_loss: 0.1291 - val_acc: 0.9504\n",
      "Epoch 161/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0919 - acc: 0.9696 - val_loss: 0.1292 - val_acc: 0.9514\n",
      "Epoch 162/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0914 - acc: 0.9701 - val_loss: 0.1286 - val_acc: 0.9502\n",
      "Epoch 163/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0913 - acc: 0.9703 - val_loss: 0.1283 - val_acc: 0.9516\n",
      "Epoch 164/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0911 - acc: 0.9707 - val_loss: 0.1298 - val_acc: 0.9502\n",
      "Epoch 165/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0913 - acc: 0.9698 - val_loss: 0.1278 - val_acc: 0.9509\n",
      "Epoch 166/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0907 - acc: 0.9701 - val_loss: 0.1273 - val_acc: 0.9515\n",
      "Epoch 167/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0901 - acc: 0.9702 - val_loss: 0.1279 - val_acc: 0.9513\n",
      "Epoch 168/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0902 - acc: 0.9705 - val_loss: 0.1272 - val_acc: 0.9514\n",
      "Epoch 169/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0899 - acc: 0.9705 - val_loss: 0.1276 - val_acc: 0.9513\n",
      "Epoch 170/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0907 - acc: 0.9705 - val_loss: 0.1274 - val_acc: 0.9513\n",
      "Epoch 171/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0906 - acc: 0.9698 - val_loss: 0.1268 - val_acc: 0.9515\n",
      "Epoch 172/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0905 - acc: 0.9696 - val_loss: 0.1278 - val_acc: 0.9515\n",
      "Epoch 173/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.0894 - acc: 0.9714 - val_loss: 0.1270 - val_acc: 0.9514\n",
      "Epoch 174/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.0906 - acc: 0.9701 - val_loss: 0.1282 - val_acc: 0.9512\n",
      "Epoch 175/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.0894 - acc: 0.9708 - val_loss: 0.1272 - val_acc: 0.9510\n",
      "Epoch 176/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.0889 - acc: 0.9709 - val_loss: 0.1267 - val_acc: 0.9520\n",
      "Epoch 177/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.0899 - acc: 0.9706 - val_loss: 0.1272 - val_acc: 0.9520\n",
      "Epoch 178/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.0889 - acc: 0.9706 - val_loss: 0.1289 - val_acc: 0.9506\n",
      "Epoch 179/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0886 - acc: 0.9711 - val_loss: 0.1274 - val_acc: 0.9515\n",
      "Epoch 180/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.0888 - acc: 0.9711 - val_loss: 0.1268 - val_acc: 0.9512\n",
      "Epoch 181/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0880 - acc: 0.9714 - val_loss: 0.1268 - val_acc: 0.9518\n",
      "Epoch 182/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0880 - acc: 0.9713 - val_loss: 0.1270 - val_acc: 0.9513\n",
      "Epoch 183/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0876 - acc: 0.9716 - val_loss: 0.1264 - val_acc: 0.9517\n",
      "Epoch 184/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0879 - acc: 0.9712 - val_loss: 0.1272 - val_acc: 0.9521\n",
      "Epoch 185/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0877 - acc: 0.9713 - val_loss: 0.1267 - val_acc: 0.9511\n",
      "Epoch 186/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0879 - acc: 0.9715 - val_loss: 0.1284 - val_acc: 0.9512\n",
      "Epoch 187/200\n",
      "17142/17142 [==============================] - 1s 30us/step - loss: 0.0879 - acc: 0.9710 - val_loss: 0.1272 - val_acc: 0.9520\n",
      "Epoch 188/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0876 - acc: 0.9715 - val_loss: 0.1275 - val_acc: 0.9508\n",
      "Epoch 189/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0878 - acc: 0.9716 - val_loss: 0.1261 - val_acc: 0.9517\n",
      "Epoch 190/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0874 - acc: 0.9711 - val_loss: 0.1276 - val_acc: 0.9517\n",
      "Epoch 191/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.0875 - acc: 0.9715 - val_loss: 0.1272 - val_acc: 0.9514\n",
      "Epoch 192/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0873 - acc: 0.9710 - val_loss: 0.1270 - val_acc: 0.9512\n",
      "Epoch 193/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0867 - acc: 0.9718 - val_loss: 0.1264 - val_acc: 0.9515\n",
      "Epoch 194/200\n",
      "17142/17142 [==============================] - 1s 29us/step - loss: 0.0868 - acc: 0.9716 - val_loss: 0.1273 - val_acc: 0.9518\n",
      "Epoch 195/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.0875 - acc: 0.9718 - val_loss: 0.1276 - val_acc: 0.9511\n",
      "Epoch 196/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0865 - acc: 0.9711 - val_loss: 0.1260 - val_acc: 0.9515\n",
      "Epoch 197/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0864 - acc: 0.9719 - val_loss: 0.1261 - val_acc: 0.9516\n",
      "Epoch 198/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0867 - acc: 0.9715 - val_loss: 0.1283 - val_acc: 0.9518\n",
      "Epoch 199/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0861 - acc: 0.9716 - val_loss: 0.1262 - val_acc: 0.9521\n",
      "Epoch 200/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.0863 - acc: 0.9713 - val_loss: 0.1269 - val_acc: 0.9517\n",
      "auroc: 0.9860060578631885\n",
      "auprc: 0.9778061608597716\n",
      "auroc: 0.9859972011823931\n",
      "auprc: 0.9778043224046344\n",
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 8s 452us/step - loss: 0.8934 - acc: 0.5941 - val_loss: 0.6500 - val_acc: 0.6613\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.6025 - acc: 0.6841 - val_loss: 0.5791 - val_acc: 0.6956\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.5508 - acc: 0.7243 - val_loss: 0.5299 - val_acc: 0.7334\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.4983 - acc: 0.7675 - val_loss: 0.4761 - val_acc: 0.7778\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.4461 - acc: 0.8068 - val_loss: 0.4271 - val_acc: 0.8181\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.4019 - acc: 0.8348 - val_loss: 0.3891 - val_acc: 0.8368\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.3686 - acc: 0.8539 - val_loss: 0.3593 - val_acc: 0.8563\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.3414 - acc: 0.8698 - val_loss: 0.3397 - val_acc: 0.8616\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.3201 - acc: 0.8814 - val_loss: 0.3164 - val_acc: 0.8799\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.3021 - acc: 0.8896 - val_loss: 0.2982 - val_acc: 0.8882\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2873 - acc: 0.8963 - val_loss: 0.2845 - val_acc: 0.8925\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.2745 - acc: 0.9011 - val_loss: 0.2737 - val_acc: 0.8950\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2639 - acc: 0.9051 - val_loss: 0.2640 - val_acc: 0.9007\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2543 - acc: 0.9085 - val_loss: 0.2535 - val_acc: 0.9050\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 0s 27us/step - loss: 0.2462 - acc: 0.9111 - val_loss: 0.2478 - val_acc: 0.9083\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2386 - acc: 0.9142 - val_loss: 0.2407 - val_acc: 0.9106\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.2327 - acc: 0.9165 - val_loss: 0.2335 - val_acc: 0.9135\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 0s 29us/step - loss: 0.2263 - acc: 0.9185 - val_loss: 0.2288 - val_acc: 0.9148\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2207 - acc: 0.9207 - val_loss: 0.2256 - val_acc: 0.9146\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2164 - acc: 0.9222 - val_loss: 0.2192 - val_acc: 0.9184\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2114 - acc: 0.9242 - val_loss: 0.2158 - val_acc: 0.9191\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 0s 28us/step - loss: 0.2076 - acc: 0.9254 - val_loss: 0.2130 - val_acc: 0.9192\n",
      "Epoch 23/200\n",
      "15000/17142 [=========================>....] - ETA: 0s - loss: 0.2048 - acc: 0.9268"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for curr_seed in all_seeds: \n",
    "    model, early_stopping_callback, auroc_callback = train_model(model_wrapper = SIAMESE_WRAPPER, \n",
    "                                                                 aug = None, \n",
    "                                                                 curr_seed = curr_seed, \n",
    "                                                                 x = x_train_200, \n",
    "                                                                 y = y_train_200_mutate, \n",
    "                                                                 val_data = (x_val_200, y_val_200))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"Siamese_200_auROC\", curr_seed = curr_seed,\n",
    "             callback = auroc_callback, model = model, val_data = (x_val_200, y_val_200))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"Siamese_200_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_val_200, y_val_200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETERS = {\n",
    "    'filters': 20,\n",
    "    'kernel_size': 21,\n",
    "    'input_length':200,\n",
    "    'pool_size': 20, \n",
    "    'strides': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "RC_WRAPPER = RCArch(**PARAMETERS)\n",
    "REG_WRAPPER = RegArch(**PARAMETERS)\n",
    "SIAMESE_WRAPPER = SiameseArch(**PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K \n",
    "\n",
    "import keras.layers as kl\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dropout \n",
    "from keras.layers.core import Flatten\n",
    "\n",
    "from keras.engine import Layer\n",
    "from keras.engine.base_layer import InputSpec\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras_genomics.layers import RevCompConv1D\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "  \n",
    "    \n",
    "class RevCompSumPool(Layer): \n",
    "    def __init__(self, **kwargs): \n",
    "        super(RevCompSumPoolAvg, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.num_input_chan = input_shape[2]\n",
    "        super(RevCompSumPoolAvg, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs): \n",
    "        inputs = (inputs[:,:,:int(self.num_input_chan/2)] + inputs[:,:,int(self.num_input_chan/2):][:,::-1,::-1])\n",
    "        return inputs\n",
    "      \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], int(input_shape[2]/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "PARAMETERS = {\n",
    "    'filters': 20,\n",
    "    'kernel_size': 21,\n",
    "    'input_length':1000,\n",
    "    'pool_size': 20, \n",
    "    'strides': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import CustomObjectScope\n",
    "from keras.models import load_model\n",
    "with CustomObjectScope({'RevCompSumPool': RevCompSumPool}):\n",
    "    loaded_model = load_model('MyProfileModel.h5')\n",
    "    for model in model_names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "RC_WRAPPER = RCArch(**PARAMETERS)\n",
    "REG_WRAPPER = RegArch(**PARAMETERS)\n",
    "SIAMESE_WRAPPER = SiameseArch(**PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 3s 94us/step - loss: 0.7744 - acc: 0.5952 - val_loss: 0.6875 - val_acc: 0.6327\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6692 - acc: 0.6343 - val_loss: 0.6775 - val_acc: 0.6243\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6536 - acc: 0.6434 - val_loss: 0.6692 - val_acc: 0.6371\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6424 - acc: 0.6516 - val_loss: 0.6634 - val_acc: 0.6284\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6314 - acc: 0.6602 - val_loss: 0.6548 - val_acc: 0.6408\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6226 - acc: 0.6653 - val_loss: 0.6520 - val_acc: 0.6504\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6129 - acc: 0.6721 - val_loss: 0.6425 - val_acc: 0.6567\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6036 - acc: 0.6785 - val_loss: 0.6340 - val_acc: 0.6585\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5953 - acc: 0.6839 - val_loss: 0.6333 - val_acc: 0.6657\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5863 - acc: 0.6918 - val_loss: 0.6191 - val_acc: 0.6721\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5756 - acc: 0.6995 - val_loss: 0.6138 - val_acc: 0.6707\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5676 - acc: 0.7048 - val_loss: 0.6022 - val_acc: 0.6810\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5581 - acc: 0.7129 - val_loss: 0.6075 - val_acc: 0.6826\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5557 - acc: 0.7145 - val_loss: 0.5867 - val_acc: 0.6951\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5399 - acc: 0.7259 - val_loss: 0.5745 - val_acc: 0.6996\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5311 - acc: 0.7332 - val_loss: 0.5652 - val_acc: 0.7078\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5227 - acc: 0.7393 - val_loss: 0.5578 - val_acc: 0.7116\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5155 - acc: 0.7446 - val_loss: 0.5549 - val_acc: 0.7152\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5088 - acc: 0.7490 - val_loss: 0.5459 - val_acc: 0.7215\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5009 - acc: 0.7542 - val_loss: 0.5437 - val_acc: 0.7246\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4945 - acc: 0.7589 - val_loss: 0.5390 - val_acc: 0.7248\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4899 - acc: 0.7609 - val_loss: 0.5354 - val_acc: 0.7296\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4822 - acc: 0.7670 - val_loss: 0.5239 - val_acc: 0.7351\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4785 - acc: 0.7686 - val_loss: 0.5300 - val_acc: 0.7343\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4715 - acc: 0.7736 - val_loss: 0.5239 - val_acc: 0.7375\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4663 - acc: 0.7778 - val_loss: 0.5209 - val_acc: 0.7389\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4631 - acc: 0.7793 - val_loss: 0.5096 - val_acc: 0.7453\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4576 - acc: 0.7828 - val_loss: 0.5171 - val_acc: 0.7412\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4534 - acc: 0.7861 - val_loss: 0.5003 - val_acc: 0.7505\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4501 - acc: 0.7882 - val_loss: 0.4980 - val_acc: 0.7524\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4473 - acc: 0.7903 - val_loss: 0.4991 - val_acc: 0.7516\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4427 - acc: 0.7936 - val_loss: 0.4976 - val_acc: 0.7517\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4408 - acc: 0.7938 - val_loss: 0.4942 - val_acc: 0.7560\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4358 - acc: 0.7972 - val_loss: 0.4888 - val_acc: 0.7590\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4310 - acc: 0.8016 - val_loss: 0.4867 - val_acc: 0.7611\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4287 - acc: 0.8017 - val_loss: 0.4862 - val_acc: 0.7616\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4257 - acc: 0.8036 - val_loss: 0.4811 - val_acc: 0.7650\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4223 - acc: 0.8061 - val_loss: 0.4808 - val_acc: 0.7653\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4212 - acc: 0.8071 - val_loss: 0.4786 - val_acc: 0.7654\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4163 - acc: 0.8103 - val_loss: 0.4763 - val_acc: 0.7685\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4158 - acc: 0.8109 - val_loss: 0.4759 - val_acc: 0.7686\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4117 - acc: 0.8134 - val_loss: 0.4722 - val_acc: 0.7712\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4099 - acc: 0.8138 - val_loss: 0.4754 - val_acc: 0.7687\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4088 - acc: 0.8139 - val_loss: 0.4714 - val_acc: 0.7709\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4054 - acc: 0.8179 - val_loss: 0.4702 - val_acc: 0.7732\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4049 - acc: 0.8174 - val_loss: 0.4714 - val_acc: 0.7719\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4006 - acc: 0.8208 - val_loss: 0.4652 - val_acc: 0.7762\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4008 - acc: 0.8197 - val_loss: 0.4644 - val_acc: 0.7779\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3992 - acc: 0.8208 - val_loss: 0.4642 - val_acc: 0.7773\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3949 - acc: 0.8230 - val_loss: 0.4664 - val_acc: 0.7757\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3949 - acc: 0.8233 - val_loss: 0.4620 - val_acc: 0.7793\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3925 - acc: 0.8249 - val_loss: 0.4656 - val_acc: 0.7761\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3927 - acc: 0.8234 - val_loss: 0.4779 - val_acc: 0.7697\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3941 - acc: 0.8222 - val_loss: 0.4627 - val_acc: 0.7793\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3888 - acc: 0.8264 - val_loss: 0.4594 - val_acc: 0.7800\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3869 - acc: 0.8272 - val_loss: 0.4600 - val_acc: 0.7788\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3864 - acc: 0.8279 - val_loss: 0.4569 - val_acc: 0.7822\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3844 - acc: 0.8298 - val_loss: 0.4583 - val_acc: 0.7816\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3817 - acc: 0.8313 - val_loss: 0.4578 - val_acc: 0.7806\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3812 - acc: 0.8310 - val_loss: 0.4595 - val_acc: 0.7808\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3791 - acc: 0.8317 - val_loss: 0.4566 - val_acc: 0.7823\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3770 - acc: 0.8328 - val_loss: 0.4674 - val_acc: 0.7766\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3785 - acc: 0.8309 - val_loss: 0.4538 - val_acc: 0.7833\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3756 - acc: 0.8343 - val_loss: 0.4539 - val_acc: 0.7839\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3731 - acc: 0.8354 - val_loss: 0.4544 - val_acc: 0.7831\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3748 - acc: 0.8336 - val_loss: 0.4617 - val_acc: 0.7793\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3748 - acc: 0.8334 - val_loss: 0.4632 - val_acc: 0.7792\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3720 - acc: 0.8355 - val_loss: 0.4588 - val_acc: 0.7809\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3684 - acc: 0.8385 - val_loss: 0.4553 - val_acc: 0.7844\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3693 - acc: 0.8373 - val_loss: 0.4536 - val_acc: 0.7858\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3688 - acc: 0.8374 - val_loss: 0.4546 - val_acc: 0.7843\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3688 - acc: 0.8359 - val_loss: 0.4526 - val_acc: 0.7843\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3654 - acc: 0.8390 - val_loss: 0.4498 - val_acc: 0.7872\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3648 - acc: 0.8398 - val_loss: 0.4511 - val_acc: 0.7868\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3637 - acc: 0.8400 - val_loss: 0.4485 - val_acc: 0.7885\n",
      "Epoch 76/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3619 - acc: 0.8411 - val_loss: 0.4546 - val_acc: 0.7841\n",
      "Epoch 77/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3616 - acc: 0.8415 - val_loss: 0.4502 - val_acc: 0.7872\n",
      "Epoch 78/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3643 - acc: 0.8383 - val_loss: 0.4488 - val_acc: 0.7871\n",
      "Epoch 79/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3591 - acc: 0.8426 - val_loss: 0.4506 - val_acc: 0.7864\n",
      "Epoch 80/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.3581 - acc: 0.8433 - val_loss: 0.4548 - val_acc: 0.7843\n",
      "Epoch 81/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3588 - acc: 0.8431 - val_loss: 0.4481 - val_acc: 0.7881\n",
      "Epoch 82/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3569 - acc: 0.8437 - val_loss: 0.4507 - val_acc: 0.7867\n",
      "Epoch 83/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3566 - acc: 0.8440 - val_loss: 0.4569 - val_acc: 0.7837\n",
      "Epoch 84/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3560 - acc: 0.8438 - val_loss: 0.4479 - val_acc: 0.7882\n",
      "Epoch 85/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3549 - acc: 0.8439 - val_loss: 0.4553 - val_acc: 0.7861\n",
      "Epoch 86/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3548 - acc: 0.8448 - val_loss: 0.4509 - val_acc: 0.7875\n",
      "Epoch 87/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3526 - acc: 0.8454 - val_loss: 0.4506 - val_acc: 0.7863\n",
      "Epoch 88/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3508 - acc: 0.8470 - val_loss: 0.4453 - val_acc: 0.7898\n",
      "Epoch 89/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3501 - acc: 0.8484 - val_loss: 0.4453 - val_acc: 0.7899\n",
      "Epoch 90/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3489 - acc: 0.8478 - val_loss: 0.4622 - val_acc: 0.7813\n",
      "Epoch 91/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3532 - acc: 0.8446 - val_loss: 0.4580 - val_acc: 0.7855\n",
      "Epoch 92/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3502 - acc: 0.8467 - val_loss: 0.4469 - val_acc: 0.7902\n",
      "Epoch 93/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3486 - acc: 0.8486 - val_loss: 0.4489 - val_acc: 0.7883\n",
      "Epoch 94/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3467 - acc: 0.8492 - val_loss: 0.4511 - val_acc: 0.7894\n",
      "Epoch 95/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3461 - acc: 0.8493 - val_loss: 0.4447 - val_acc: 0.7904\n",
      "Epoch 96/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3462 - acc: 0.8490 - val_loss: 0.4450 - val_acc: 0.7902\n",
      "Epoch 97/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3517 - acc: 0.8444 - val_loss: 0.4509 - val_acc: 0.7887\n",
      "Epoch 98/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3437 - acc: 0.8498 - val_loss: 0.4476 - val_acc: 0.7892\n",
      "Epoch 99/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3471 - acc: 0.8480 - val_loss: 0.4463 - val_acc: 0.7901\n",
      "Epoch 100/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3433 - acc: 0.8507 - val_loss: 0.4489 - val_acc: 0.7885\n",
      "Epoch 101/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3442 - acc: 0.8501 - val_loss: 0.4517 - val_acc: 0.7870\n",
      "Epoch 102/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.3424 - acc: 0.8508 - val_loss: 0.4471 - val_acc: 0.7894\n",
      "Epoch 103/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3414 - acc: 0.8509 - val_loss: 0.4527 - val_acc: 0.7864\n",
      "Epoch 104/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3413 - acc: 0.8505 - val_loss: 0.4462 - val_acc: 0.7903\n",
      "Epoch 105/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3404 - acc: 0.8525 - val_loss: 0.4514 - val_acc: 0.7893\n",
      "auroc: 0.8442194142107967\n",
      "auprc: 0.7486116812487053\n",
      "auroc: 0.8433941089885962\n",
      "auprc: 0.7469356554241244\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 3s 92us/step - loss: 0.7368 - acc: 0.6024 - val_loss: 0.6930 - val_acc: 0.6192\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6710 - acc: 0.6311 - val_loss: 0.6782 - val_acc: 0.6343\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.6566 - acc: 0.6416 - val_loss: 0.6743 - val_acc: 0.6247\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6472 - acc: 0.6476 - val_loss: 0.6686 - val_acc: 0.6246\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.6381 - acc: 0.6541 - val_loss: 0.6637 - val_acc: 0.6361\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.6305 - acc: 0.6595 - val_loss: 0.6558 - val_acc: 0.6465\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.6217 - acc: 0.6665 - val_loss: 0.6507 - val_acc: 0.6479\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.6123 - acc: 0.6734 - val_loss: 0.6378 - val_acc: 0.6574\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5989 - acc: 0.6846 - val_loss: 0.6263 - val_acc: 0.6679\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.5883 - acc: 0.6922 - val_loss: 0.6161 - val_acc: 0.6698\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5744 - acc: 0.7009 - val_loss: 0.6030 - val_acc: 0.6763\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.5648 - acc: 0.7081 - val_loss: 0.5879 - val_acc: 0.6908\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5492 - acc: 0.7206 - val_loss: 0.5822 - val_acc: 0.6979\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5396 - acc: 0.7271 - val_loss: 0.5760 - val_acc: 0.7031\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5295 - acc: 0.7353 - val_loss: 0.5647 - val_acc: 0.7107\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.5229 - acc: 0.7389 - val_loss: 0.5504 - val_acc: 0.7168\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5157 - acc: 0.7451 - val_loss: 0.5437 - val_acc: 0.7210\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.5094 - acc: 0.7477 - val_loss: 0.5471 - val_acc: 0.7232\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.5015 - acc: 0.7536 - val_loss: 0.5319 - val_acc: 0.7294\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4936 - acc: 0.7587 - val_loss: 0.5265 - val_acc: 0.7310\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4874 - acc: 0.7632 - val_loss: 0.5216 - val_acc: 0.7337\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4823 - acc: 0.7658 - val_loss: 0.5194 - val_acc: 0.7361\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4792 - acc: 0.7681 - val_loss: 0.5160 - val_acc: 0.7373\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4736 - acc: 0.7715 - val_loss: 0.5147 - val_acc: 0.7398\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4680 - acc: 0.7751 - val_loss: 0.5070 - val_acc: 0.7469\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4624 - acc: 0.7806 - val_loss: 0.5067 - val_acc: 0.7475\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4595 - acc: 0.7822 - val_loss: 0.5009 - val_acc: 0.7509\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4552 - acc: 0.7848 - val_loss: 0.4929 - val_acc: 0.7537\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4541 - acc: 0.7854 - val_loss: 0.5005 - val_acc: 0.7514\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4460 - acc: 0.7912 - val_loss: 0.4958 - val_acc: 0.7547\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4435 - acc: 0.7921 - val_loss: 0.4963 - val_acc: 0.7545\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4397 - acc: 0.7948 - val_loss: 0.4901 - val_acc: 0.7583\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4366 - acc: 0.7971 - val_loss: 0.4836 - val_acc: 0.7611\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4342 - acc: 0.7980 - val_loss: 0.4823 - val_acc: 0.7620\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4301 - acc: 0.8012 - val_loss: 0.4750 - val_acc: 0.7666\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4256 - acc: 0.8049 - val_loss: 0.4731 - val_acc: 0.7687\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4260 - acc: 0.8044 - val_loss: 0.4721 - val_acc: 0.7691\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4236 - acc: 0.8059 - val_loss: 0.4729 - val_acc: 0.7708\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4220 - acc: 0.8058 - val_loss: 0.4695 - val_acc: 0.7713\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4209 - acc: 0.8062 - val_loss: 0.4729 - val_acc: 0.7685\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4137 - acc: 0.8116 - val_loss: 0.4721 - val_acc: 0.7698\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4107 - acc: 0.8144 - val_loss: 0.4640 - val_acc: 0.7758\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4081 - acc: 0.8160 - val_loss: 0.4658 - val_acc: 0.7731\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4076 - acc: 0.8156 - val_loss: 0.4632 - val_acc: 0.7769\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4041 - acc: 0.8177 - val_loss: 0.4654 - val_acc: 0.7750\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4045 - acc: 0.8182 - val_loss: 0.4631 - val_acc: 0.7747\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4008 - acc: 0.8209 - val_loss: 0.4591 - val_acc: 0.7773\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3980 - acc: 0.8225 - val_loss: 0.4615 - val_acc: 0.7773\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3987 - acc: 0.8219 - val_loss: 0.4568 - val_acc: 0.7789\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3962 - acc: 0.8232 - val_loss: 0.4577 - val_acc: 0.7802\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3940 - acc: 0.8242 - val_loss: 0.4584 - val_acc: 0.7794\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3940 - acc: 0.8231 - val_loss: 0.4593 - val_acc: 0.7784\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3930 - acc: 0.8243 - val_loss: 0.4566 - val_acc: 0.7805\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3914 - acc: 0.8245 - val_loss: 0.4523 - val_acc: 0.7813\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3878 - acc: 0.8271 - val_loss: 0.4517 - val_acc: 0.7823\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3873 - acc: 0.8272 - val_loss: 0.4545 - val_acc: 0.7829\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3852 - acc: 0.8288 - val_loss: 0.4648 - val_acc: 0.7778\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3847 - acc: 0.8285 - val_loss: 0.4525 - val_acc: 0.7837\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3827 - acc: 0.8302 - val_loss: 0.4529 - val_acc: 0.7807\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3814 - acc: 0.8309 - val_loss: 0.4528 - val_acc: 0.7846\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3800 - acc: 0.8318 - val_loss: 0.4512 - val_acc: 0.7848\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3805 - acc: 0.8303 - val_loss: 0.4523 - val_acc: 0.7833\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3823 - acc: 0.8287 - val_loss: 0.4505 - val_acc: 0.7830\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3769 - acc: 0.8333 - val_loss: 0.4517 - val_acc: 0.7847\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3763 - acc: 0.8332 - val_loss: 0.4534 - val_acc: 0.7829\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3771 - acc: 0.8330 - val_loss: 0.4474 - val_acc: 0.7859\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3723 - acc: 0.8358 - val_loss: 0.4576 - val_acc: 0.7792\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3733 - acc: 0.8357 - val_loss: 0.4487 - val_acc: 0.7856\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3710 - acc: 0.8357 - val_loss: 0.4472 - val_acc: 0.7850\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3708 - acc: 0.8362 - val_loss: 0.4455 - val_acc: 0.7882\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3676 - acc: 0.8383 - val_loss: 0.4487 - val_acc: 0.7862\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3688 - acc: 0.8377 - val_loss: 0.4467 - val_acc: 0.7869\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3680 - acc: 0.8368 - val_loss: 0.4508 - val_acc: 0.7853\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3669 - acc: 0.8380 - val_loss: 0.4465 - val_acc: 0.7866\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3685 - acc: 0.8361 - val_loss: 0.4545 - val_acc: 0.7836\n",
      "Epoch 76/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3683 - acc: 0.8367 - val_loss: 0.4451 - val_acc: 0.7884\n",
      "Epoch 77/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3631 - acc: 0.8400 - val_loss: 0.4457 - val_acc: 0.7877\n",
      "Epoch 78/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3623 - acc: 0.8403 - val_loss: 0.4439 - val_acc: 0.7896\n",
      "Epoch 79/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3629 - acc: 0.8395 - val_loss: 0.4472 - val_acc: 0.7874\n",
      "Epoch 80/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3616 - acc: 0.8407 - val_loss: 0.4483 - val_acc: 0.7857\n",
      "Epoch 81/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3617 - acc: 0.8413 - val_loss: 0.4455 - val_acc: 0.7885\n",
      "Epoch 82/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3622 - acc: 0.8387 - val_loss: 0.4453 - val_acc: 0.7890\n",
      "Epoch 83/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3627 - acc: 0.8399 - val_loss: 0.4496 - val_acc: 0.7883\n",
      "Epoch 84/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3593 - acc: 0.8415 - val_loss: 0.4435 - val_acc: 0.7898\n",
      "Epoch 85/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3574 - acc: 0.8433 - val_loss: 0.4517 - val_acc: 0.7844\n",
      "Epoch 86/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3573 - acc: 0.8430 - val_loss: 0.4462 - val_acc: 0.7881\n",
      "Epoch 87/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3567 - acc: 0.8430 - val_loss: 0.4496 - val_acc: 0.7853\n",
      "Epoch 88/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3560 - acc: 0.8438 - val_loss: 0.4428 - val_acc: 0.7894\n",
      "Epoch 89/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3550 - acc: 0.8445 - val_loss: 0.4437 - val_acc: 0.7894\n",
      "Epoch 90/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3550 - acc: 0.8455 - val_loss: 0.4495 - val_acc: 0.7860\n",
      "Epoch 91/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3551 - acc: 0.8433 - val_loss: 0.4480 - val_acc: 0.7874\n",
      "Epoch 92/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3539 - acc: 0.8447 - val_loss: 0.4476 - val_acc: 0.7887\n",
      "Epoch 93/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3569 - acc: 0.8425 - val_loss: 0.4477 - val_acc: 0.7869\n",
      "Epoch 94/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3514 - acc: 0.8459 - val_loss: 0.4434 - val_acc: 0.7904\n",
      "Epoch 95/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3524 - acc: 0.8457 - val_loss: 0.4474 - val_acc: 0.7880\n",
      "Epoch 96/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3523 - acc: 0.8455 - val_loss: 0.4468 - val_acc: 0.7897\n",
      "Epoch 97/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3514 - acc: 0.8458 - val_loss: 0.4471 - val_acc: 0.7896\n",
      "Epoch 98/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3540 - acc: 0.8440 - val_loss: 0.4446 - val_acc: 0.7906\n",
      "auroc: 0.8445402343202749\n",
      "auprc: 0.7488615467119214\n",
      "auroc: 0.8439336916788859\n",
      "auprc: 0.7475332377748211\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 3s 90us/step - loss: 0.7321 - acc: 0.6063 - val_loss: 0.6866 - val_acc: 0.6193\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6666 - acc: 0.6343 - val_loss: 0.6766 - val_acc: 0.6271\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6525 - acc: 0.6447 - val_loss: 0.6715 - val_acc: 0.6317\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6406 - acc: 0.6539 - val_loss: 0.6631 - val_acc: 0.6360\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6329 - acc: 0.6573 - val_loss: 0.6637 - val_acc: 0.6332\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6214 - acc: 0.6667 - val_loss: 0.6479 - val_acc: 0.6495\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6099 - acc: 0.6743 - val_loss: 0.6397 - val_acc: 0.6592\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6002 - acc: 0.6823 - val_loss: 0.6244 - val_acc: 0.6636\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5871 - acc: 0.6916 - val_loss: 0.6233 - val_acc: 0.6770\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5746 - acc: 0.7013 - val_loss: 0.6085 - val_acc: 0.6887\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5648 - acc: 0.7082 - val_loss: 0.6061 - val_acc: 0.6887\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5549 - acc: 0.7156 - val_loss: 0.5908 - val_acc: 0.6979\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5422 - acc: 0.7253 - val_loss: 0.5717 - val_acc: 0.7075\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5330 - acc: 0.7320 - val_loss: 0.5741 - val_acc: 0.7001\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5282 - acc: 0.7349 - val_loss: 0.5717 - val_acc: 0.7110\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5205 - acc: 0.7399 - val_loss: 0.5530 - val_acc: 0.7113\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.5106 - acc: 0.7473 - val_loss: 0.5427 - val_acc: 0.7235\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.5049 - acc: 0.7524 - val_loss: 0.5461 - val_acc: 0.7249\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.5006 - acc: 0.7533 - val_loss: 0.5328 - val_acc: 0.7283\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4936 - acc: 0.7587 - val_loss: 0.5394 - val_acc: 0.7257\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4895 - acc: 0.7619 - val_loss: 0.5418 - val_acc: 0.7308\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4861 - acc: 0.7636 - val_loss: 0.5245 - val_acc: 0.7303\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4809 - acc: 0.7665 - val_loss: 0.5185 - val_acc: 0.7389\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4756 - acc: 0.7713 - val_loss: 0.5209 - val_acc: 0.7361\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4728 - acc: 0.7733 - val_loss: 0.5121 - val_acc: 0.7409\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4670 - acc: 0.7768 - val_loss: 0.5118 - val_acc: 0.7384\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4643 - acc: 0.7782 - val_loss: 0.5078 - val_acc: 0.7430\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4607 - acc: 0.7812 - val_loss: 0.5039 - val_acc: 0.7465\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4562 - acc: 0.7846 - val_loss: 0.5012 - val_acc: 0.7475\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4543 - acc: 0.7849 - val_loss: 0.4999 - val_acc: 0.7490\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4509 - acc: 0.7878 - val_loss: 0.4987 - val_acc: 0.7486\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4505 - acc: 0.7866 - val_loss: 0.4978 - val_acc: 0.7503\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4472 - acc: 0.7896 - val_loss: 0.4989 - val_acc: 0.7523\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4434 - acc: 0.7921 - val_loss: 0.4949 - val_acc: 0.7519\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4395 - acc: 0.7956 - val_loss: 0.4998 - val_acc: 0.7494\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4411 - acc: 0.7925 - val_loss: 0.4920 - val_acc: 0.7560\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4366 - acc: 0.7974 - val_loss: 0.4889 - val_acc: 0.7582\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4315 - acc: 0.8006 - val_loss: 0.4896 - val_acc: 0.7585\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4302 - acc: 0.8007 - val_loss: 0.4858 - val_acc: 0.7611\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4273 - acc: 0.8028 - val_loss: 0.4845 - val_acc: 0.7612\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4257 - acc: 0.8038 - val_loss: 0.4946 - val_acc: 0.7569\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4264 - acc: 0.8025 - val_loss: 0.4845 - val_acc: 0.7612\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4223 - acc: 0.8054 - val_loss: 0.4816 - val_acc: 0.7627\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4199 - acc: 0.8079 - val_loss: 0.4842 - val_acc: 0.7615\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4200 - acc: 0.8072 - val_loss: 0.4858 - val_acc: 0.7602\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4156 - acc: 0.8090 - val_loss: 0.4796 - val_acc: 0.7656\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4130 - acc: 0.8112 - val_loss: 0.4853 - val_acc: 0.7616\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4127 - acc: 0.8113 - val_loss: 0.4756 - val_acc: 0.7684\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4110 - acc: 0.8121 - val_loss: 0.4766 - val_acc: 0.7670\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4076 - acc: 0.8140 - val_loss: 0.4754 - val_acc: 0.7674\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4087 - acc: 0.8134 - val_loss: 0.4752 - val_acc: 0.7673\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4039 - acc: 0.8169 - val_loss: 0.4772 - val_acc: 0.7671\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4028 - acc: 0.8171 - val_loss: 0.4750 - val_acc: 0.7686\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4045 - acc: 0.8154 - val_loss: 0.4734 - val_acc: 0.7693\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4019 - acc: 0.8167 - val_loss: 0.4714 - val_acc: 0.7688\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3979 - acc: 0.8200 - val_loss: 0.4756 - val_acc: 0.7682\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3967 - acc: 0.8206 - val_loss: 0.4869 - val_acc: 0.7624\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3955 - acc: 0.8206 - val_loss: 0.4717 - val_acc: 0.7698\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3950 - acc: 0.8208 - val_loss: 0.4702 - val_acc: 0.7713\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3929 - acc: 0.8222 - val_loss: 0.4835 - val_acc: 0.7657\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3933 - acc: 0.8217 - val_loss: 0.4715 - val_acc: 0.7686\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3924 - acc: 0.8226 - val_loss: 0.4691 - val_acc: 0.7722\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3877 - acc: 0.8253 - val_loss: 0.4702 - val_acc: 0.7701\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3890 - acc: 0.8252 - val_loss: 0.4710 - val_acc: 0.7685\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3877 - acc: 0.8249 - val_loss: 0.4729 - val_acc: 0.7686\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3866 - acc: 0.8259 - val_loss: 0.4657 - val_acc: 0.7731\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3835 - acc: 0.8279 - val_loss: 0.4656 - val_acc: 0.7726\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3873 - acc: 0.8246 - val_loss: 0.4691 - val_acc: 0.7715\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3838 - acc: 0.8260 - val_loss: 0.4635 - val_acc: 0.7752\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3801 - acc: 0.8291 - val_loss: 0.4659 - val_acc: 0.7732\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3806 - acc: 0.8286 - val_loss: 0.4770 - val_acc: 0.7657\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3786 - acc: 0.8300 - val_loss: 0.4651 - val_acc: 0.7740\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3770 - acc: 0.8295 - val_loss: 0.4691 - val_acc: 0.7729\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3756 - acc: 0.8315 - val_loss: 0.4671 - val_acc: 0.7728\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3748 - acc: 0.8310 - val_loss: 0.4679 - val_acc: 0.7735\n",
      "Epoch 76/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3748 - acc: 0.8312 - val_loss: 0.4658 - val_acc: 0.7719\n",
      "Epoch 77/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3724 - acc: 0.8329 - val_loss: 0.4642 - val_acc: 0.7746\n",
      "Epoch 78/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3717 - acc: 0.8327 - val_loss: 0.4803 - val_acc: 0.7678\n",
      "Epoch 79/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3724 - acc: 0.8323 - val_loss: 0.4821 - val_acc: 0.7681\n",
      "auroc: 0.8268164057327744\n",
      "auprc: 0.7173604674098842\n",
      "auroc: 0.8249481529352362\n",
      "auprc: 0.7137811280519312\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 3s 94us/step - loss: 0.7282 - acc: 0.6064 - val_loss: 0.6815 - val_acc: 0.6191\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6702 - acc: 0.6301 - val_loss: 0.6775 - val_acc: 0.6311\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.6573 - acc: 0.6404 - val_loss: 0.6708 - val_acc: 0.6346\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6504 - acc: 0.6442 - val_loss: 0.6677 - val_acc: 0.6375\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6426 - acc: 0.6504 - val_loss: 0.6640 - val_acc: 0.6399\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6350 - acc: 0.6558 - val_loss: 0.6589 - val_acc: 0.6406\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6304 - acc: 0.6595 - val_loss: 0.6555 - val_acc: 0.6360\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6213 - acc: 0.6648 - val_loss: 0.6463 - val_acc: 0.6490\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6129 - acc: 0.6719 - val_loss: 0.6459 - val_acc: 0.6589\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6059 - acc: 0.6776 - val_loss: 0.6284 - val_acc: 0.6649\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5916 - acc: 0.6885 - val_loss: 0.6167 - val_acc: 0.6693\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.5806 - acc: 0.6970 - val_loss: 0.6077 - val_acc: 0.6725\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.5702 - acc: 0.7030 - val_loss: 0.5946 - val_acc: 0.6856\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.5573 - acc: 0.7129 - val_loss: 0.5852 - val_acc: 0.6972\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5476 - acc: 0.7194 - val_loss: 0.5922 - val_acc: 0.6935\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.5380 - acc: 0.7256 - val_loss: 0.5683 - val_acc: 0.7079\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5270 - acc: 0.7325 - val_loss: 0.5555 - val_acc: 0.7150\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5180 - acc: 0.7386 - val_loss: 0.5526 - val_acc: 0.7146\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5109 - acc: 0.7446 - val_loss: 0.5405 - val_acc: 0.7207\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5036 - acc: 0.7497 - val_loss: 0.5485 - val_acc: 0.7162\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4959 - acc: 0.7549 - val_loss: 0.5353 - val_acc: 0.7268\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4885 - acc: 0.7600 - val_loss: 0.5299 - val_acc: 0.7299\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4826 - acc: 0.7641 - val_loss: 0.5283 - val_acc: 0.7307\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4776 - acc: 0.7681 - val_loss: 0.5109 - val_acc: 0.7430\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4713 - acc: 0.7711 - val_loss: 0.5066 - val_acc: 0.7461\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4656 - acc: 0.7763 - val_loss: 0.5074 - val_acc: 0.7432\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4603 - acc: 0.7801 - val_loss: 0.4991 - val_acc: 0.7500\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4565 - acc: 0.7823 - val_loss: 0.4962 - val_acc: 0.7523\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4526 - acc: 0.7858 - val_loss: 0.4967 - val_acc: 0.7507\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4499 - acc: 0.7876 - val_loss: 0.4978 - val_acc: 0.7525\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4445 - acc: 0.7914 - val_loss: 0.4896 - val_acc: 0.7561\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4432 - acc: 0.7921 - val_loss: 0.4891 - val_acc: 0.7588\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4384 - acc: 0.7947 - val_loss: 0.4840 - val_acc: 0.7610\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4350 - acc: 0.7979 - val_loss: 0.4816 - val_acc: 0.7625\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4330 - acc: 0.7984 - val_loss: 0.4821 - val_acc: 0.7638\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4301 - acc: 0.7999 - val_loss: 0.4846 - val_acc: 0.7610\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4262 - acc: 0.8035 - val_loss: 0.4807 - val_acc: 0.7617\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4235 - acc: 0.8057 - val_loss: 0.4820 - val_acc: 0.7631\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4239 - acc: 0.8033 - val_loss: 0.4785 - val_acc: 0.7639\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4193 - acc: 0.8078 - val_loss: 0.4727 - val_acc: 0.7687\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4166 - acc: 0.8099 - val_loss: 0.4698 - val_acc: 0.7706\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4137 - acc: 0.8115 - val_loss: 0.4725 - val_acc: 0.7686\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.4112 - acc: 0.8137 - val_loss: 0.4735 - val_acc: 0.7691\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4093 - acc: 0.8143 - val_loss: 0.4673 - val_acc: 0.7722\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4105 - acc: 0.8127 - val_loss: 0.4836 - val_acc: 0.7633\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.4076 - acc: 0.8161 - val_loss: 0.4654 - val_acc: 0.7743\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4064 - acc: 0.8150 - val_loss: 0.4764 - val_acc: 0.7680\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4044 - acc: 0.8161 - val_loss: 0.4619 - val_acc: 0.7754\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4016 - acc: 0.8188 - val_loss: 0.4624 - val_acc: 0.7757\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4004 - acc: 0.8188 - val_loss: 0.4676 - val_acc: 0.7723\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3967 - acc: 0.8214 - val_loss: 0.4628 - val_acc: 0.7743\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3952 - acc: 0.8230 - val_loss: 0.4586 - val_acc: 0.7787\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3934 - acc: 0.8233 - val_loss: 0.4602 - val_acc: 0.7771\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3932 - acc: 0.8235 - val_loss: 0.4578 - val_acc: 0.7786\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3907 - acc: 0.8250 - val_loss: 0.4590 - val_acc: 0.7782\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3901 - acc: 0.8259 - val_loss: 0.4612 - val_acc: 0.7758\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3907 - acc: 0.8244 - val_loss: 0.4620 - val_acc: 0.7766\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3876 - acc: 0.8271 - val_loss: 0.4611 - val_acc: 0.7772\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3851 - acc: 0.8290 - val_loss: 0.4623 - val_acc: 0.7772\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3841 - acc: 0.8299 - val_loss: 0.4549 - val_acc: 0.7809\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3859 - acc: 0.8279 - val_loss: 0.4559 - val_acc: 0.7803\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.3829 - acc: 0.8295 - val_loss: 0.4545 - val_acc: 0.7818\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3806 - acc: 0.8307 - val_loss: 0.4571 - val_acc: 0.7798\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3818 - acc: 0.8304 - val_loss: 0.4572 - val_acc: 0.7792\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3789 - acc: 0.8318 - val_loss: 0.4527 - val_acc: 0.7821\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3790 - acc: 0.8312 - val_loss: 0.4566 - val_acc: 0.7792\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3817 - acc: 0.8286 - val_loss: 0.4622 - val_acc: 0.7773\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3786 - acc: 0.8308 - val_loss: 0.4521 - val_acc: 0.7833\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3742 - acc: 0.8346 - val_loss: 0.4591 - val_acc: 0.7799\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3745 - acc: 0.8340 - val_loss: 0.4554 - val_acc: 0.7824\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3761 - acc: 0.8324 - val_loss: 0.4614 - val_acc: 0.7789\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3718 - acc: 0.8358 - val_loss: 0.4536 - val_acc: 0.7822\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3702 - acc: 0.8369 - val_loss: 0.4553 - val_acc: 0.7812\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3688 - acc: 0.8373 - val_loss: 0.4508 - val_acc: 0.7844\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3697 - acc: 0.8360 - val_loss: 0.4555 - val_acc: 0.7822\n",
      "Epoch 76/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3690 - acc: 0.8369 - val_loss: 0.4675 - val_acc: 0.7756\n",
      "Epoch 77/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3668 - acc: 0.8382 - val_loss: 0.4553 - val_acc: 0.7828\n",
      "Epoch 78/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3669 - acc: 0.8380 - val_loss: 0.4586 - val_acc: 0.7793\n",
      "Epoch 79/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3673 - acc: 0.8375 - val_loss: 0.4690 - val_acc: 0.7772\n",
      "Epoch 80/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3658 - acc: 0.8381 - val_loss: 0.4503 - val_acc: 0.7854\n",
      "Epoch 81/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.3654 - acc: 0.8376 - val_loss: 0.4505 - val_acc: 0.7858\n",
      "Epoch 82/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3634 - acc: 0.8397 - val_loss: 0.4494 - val_acc: 0.7867\n",
      "Epoch 83/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3638 - acc: 0.8388 - val_loss: 0.4510 - val_acc: 0.7846\n",
      "Epoch 84/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3611 - acc: 0.8411 - val_loss: 0.4550 - val_acc: 0.7823\n",
      "Epoch 85/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3600 - acc: 0.8418 - val_loss: 0.4498 - val_acc: 0.7856\n",
      "Epoch 86/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3602 - acc: 0.8412 - val_loss: 0.4485 - val_acc: 0.7872\n",
      "Epoch 87/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3613 - acc: 0.8406 - val_loss: 0.4484 - val_acc: 0.7875\n",
      "Epoch 88/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3611 - acc: 0.8401 - val_loss: 0.4524 - val_acc: 0.7843\n",
      "Epoch 89/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3570 - acc: 0.8432 - val_loss: 0.4527 - val_acc: 0.7855\n",
      "Epoch 90/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3564 - acc: 0.8437 - val_loss: 0.4489 - val_acc: 0.7872\n",
      "Epoch 91/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3566 - acc: 0.8429 - val_loss: 0.4535 - val_acc: 0.7840\n",
      "Epoch 92/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3610 - acc: 0.8388 - val_loss: 0.4498 - val_acc: 0.7874\n",
      "Epoch 93/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3564 - acc: 0.8428 - val_loss: 0.4477 - val_acc: 0.7873\n",
      "Epoch 94/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3544 - acc: 0.8442 - val_loss: 0.4572 - val_acc: 0.7831\n",
      "Epoch 95/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3525 - acc: 0.8451 - val_loss: 0.4518 - val_acc: 0.7869\n",
      "Epoch 96/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3528 - acc: 0.8444 - val_loss: 0.4521 - val_acc: 0.7860\n",
      "Epoch 97/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3549 - acc: 0.8429 - val_loss: 0.4564 - val_acc: 0.7834\n",
      "Epoch 98/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3510 - acc: 0.8458 - val_loss: 0.4491 - val_acc: 0.7876\n",
      "Epoch 99/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3527 - acc: 0.8448 - val_loss: 0.4488 - val_acc: 0.7887\n",
      "Epoch 100/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3520 - acc: 0.8453 - val_loss: 0.4628 - val_acc: 0.7824\n",
      "Epoch 101/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3517 - acc: 0.8448 - val_loss: 0.4604 - val_acc: 0.7833\n",
      "Epoch 102/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3513 - acc: 0.8465 - val_loss: 0.4533 - val_acc: 0.7849\n",
      "Epoch 103/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3498 - acc: 0.8465 - val_loss: 0.4516 - val_acc: 0.7875\n",
      "auroc: 0.8421438118633006\n",
      "auprc: 0.7455164168318289\n",
      "auroc: 0.8408222963502165\n",
      "auprc: 0.7431055580837654\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 3s 97us/step - loss: 0.7590 - acc: 0.6002 - val_loss: 0.6874 - val_acc: 0.6348\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6669 - acc: 0.6337 - val_loss: 0.6753 - val_acc: 0.6257\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6544 - acc: 0.6428 - val_loss: 0.6691 - val_acc: 0.6379\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.6446 - acc: 0.6493 - val_loss: 0.6654 - val_acc: 0.6349\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6359 - acc: 0.6563 - val_loss: 0.6594 - val_acc: 0.6387\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6272 - acc: 0.6617 - val_loss: 0.6536 - val_acc: 0.6484\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6196 - acc: 0.6655 - val_loss: 0.6512 - val_acc: 0.6583\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6141 - acc: 0.6711 - val_loss: 0.6420 - val_acc: 0.6457\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6033 - acc: 0.6779 - val_loss: 0.6335 - val_acc: 0.6520\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5930 - acc: 0.6859 - val_loss: 0.6228 - val_acc: 0.6689\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5847 - acc: 0.6933 - val_loss: 0.6174 - val_acc: 0.6653\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.5751 - acc: 0.6992 - val_loss: 0.6130 - val_acc: 0.6799\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.5652 - acc: 0.7067 - val_loss: 0.5972 - val_acc: 0.6842\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.5573 - acc: 0.7129 - val_loss: 0.5945 - val_acc: 0.6925\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.5494 - acc: 0.7192 - val_loss: 0.5871 - val_acc: 0.6968\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5402 - acc: 0.7266 - val_loss: 0.5765 - val_acc: 0.6977\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5322 - acc: 0.7320 - val_loss: 0.5659 - val_acc: 0.7065\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.5238 - acc: 0.7385 - val_loss: 0.5683 - val_acc: 0.7088\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5160 - acc: 0.7448 - val_loss: 0.5482 - val_acc: 0.7187\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5076 - acc: 0.7516 - val_loss: 0.5489 - val_acc: 0.7181\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4999 - acc: 0.7571 - val_loss: 0.5336 - val_acc: 0.7305\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4931 - acc: 0.7614 - val_loss: 0.5306 - val_acc: 0.7347\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4864 - acc: 0.7659 - val_loss: 0.5229 - val_acc: 0.7370\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4840 - acc: 0.7663 - val_loss: 0.5308 - val_acc: 0.7365\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4776 - acc: 0.7723 - val_loss: 0.5157 - val_acc: 0.7453\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.4701 - acc: 0.7775 - val_loss: 0.5104 - val_acc: 0.7469\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4665 - acc: 0.7794 - val_loss: 0.5098 - val_acc: 0.7478\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4620 - acc: 0.7820 - val_loss: 0.5109 - val_acc: 0.7480\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4595 - acc: 0.7832 - val_loss: 0.5068 - val_acc: 0.7505\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4546 - acc: 0.7870 - val_loss: 0.5104 - val_acc: 0.7485\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4522 - acc: 0.7881 - val_loss: 0.4944 - val_acc: 0.7572\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.4476 - acc: 0.7914 - val_loss: 0.4975 - val_acc: 0.7550\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4470 - acc: 0.7920 - val_loss: 0.4969 - val_acc: 0.7564\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4391 - acc: 0.7972 - val_loss: 0.4897 - val_acc: 0.7595\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4371 - acc: 0.7990 - val_loss: 0.4915 - val_acc: 0.7583\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4337 - acc: 0.8003 - val_loss: 0.4887 - val_acc: 0.7604\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4323 - acc: 0.8008 - val_loss: 0.4947 - val_acc: 0.7572\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.4302 - acc: 0.8015 - val_loss: 0.4871 - val_acc: 0.7601\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4282 - acc: 0.8032 - val_loss: 0.4854 - val_acc: 0.7626\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4253 - acc: 0.8050 - val_loss: 0.4762 - val_acc: 0.7684\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4250 - acc: 0.8052 - val_loss: 0.4777 - val_acc: 0.7674\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4204 - acc: 0.8083 - val_loss: 0.4745 - val_acc: 0.7690\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4194 - acc: 0.8079 - val_loss: 0.4811 - val_acc: 0.7648\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4164 - acc: 0.8100 - val_loss: 0.4711 - val_acc: 0.7717\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4127 - acc: 0.8135 - val_loss: 0.4724 - val_acc: 0.7692\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4129 - acc: 0.8120 - val_loss: 0.4720 - val_acc: 0.7699\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4099 - acc: 0.8139 - val_loss: 0.4693 - val_acc: 0.7715\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4125 - acc: 0.8125 - val_loss: 0.4816 - val_acc: 0.7656\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4090 - acc: 0.8147 - val_loss: 0.4716 - val_acc: 0.7716\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4050 - acc: 0.8171 - val_loss: 0.4671 - val_acc: 0.7733\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4026 - acc: 0.8197 - val_loss: 0.4647 - val_acc: 0.7753\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4004 - acc: 0.8203 - val_loss: 0.4635 - val_acc: 0.7753\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4008 - acc: 0.8201 - val_loss: 0.4685 - val_acc: 0.7743\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3985 - acc: 0.8209 - val_loss: 0.4624 - val_acc: 0.7764\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3963 - acc: 0.8236 - val_loss: 0.4626 - val_acc: 0.7774\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3961 - acc: 0.8227 - val_loss: 0.4622 - val_acc: 0.7771\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3947 - acc: 0.8239 - val_loss: 0.4610 - val_acc: 0.7775\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3909 - acc: 0.8260 - val_loss: 0.4590 - val_acc: 0.7804\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3901 - acc: 0.8271 - val_loss: 0.4597 - val_acc: 0.7788\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3900 - acc: 0.8261 - val_loss: 0.4701 - val_acc: 0.7716\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3905 - acc: 0.8260 - val_loss: 0.4608 - val_acc: 0.7792\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3873 - acc: 0.8280 - val_loss: 0.4582 - val_acc: 0.7804\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3866 - acc: 0.8278 - val_loss: 0.4585 - val_acc: 0.7795\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3848 - acc: 0.8293 - val_loss: 0.4561 - val_acc: 0.7823\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3854 - acc: 0.8291 - val_loss: 0.4597 - val_acc: 0.7792\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3825 - acc: 0.8303 - val_loss: 0.4581 - val_acc: 0.7813\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3820 - acc: 0.8310 - val_loss: 0.4559 - val_acc: 0.7806\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3830 - acc: 0.8294 - val_loss: 0.4538 - val_acc: 0.7839\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3805 - acc: 0.8309 - val_loss: 0.4559 - val_acc: 0.7818\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3822 - acc: 0.8299 - val_loss: 0.4571 - val_acc: 0.7820\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3778 - acc: 0.8324 - val_loss: 0.4638 - val_acc: 0.7761\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3765 - acc: 0.8341 - val_loss: 0.4527 - val_acc: 0.7842\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3753 - acc: 0.8341 - val_loss: 0.4569 - val_acc: 0.7824\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3744 - acc: 0.8355 - val_loss: 0.4608 - val_acc: 0.7799\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3748 - acc: 0.8347 - val_loss: 0.4568 - val_acc: 0.7813\n",
      "Epoch 76/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3749 - acc: 0.8347 - val_loss: 0.4585 - val_acc: 0.7802\n",
      "Epoch 77/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3712 - acc: 0.8373 - val_loss: 0.4540 - val_acc: 0.7830\n",
      "Epoch 78/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3701 - acc: 0.8372 - val_loss: 0.4639 - val_acc: 0.7781\n",
      "Epoch 79/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3700 - acc: 0.8385 - val_loss: 0.4510 - val_acc: 0.7849\n",
      "Epoch 80/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3679 - acc: 0.8394 - val_loss: 0.4498 - val_acc: 0.7856\n",
      "Epoch 81/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3668 - acc: 0.8394 - val_loss: 0.4599 - val_acc: 0.7793\n",
      "Epoch 82/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3708 - acc: 0.8364 - val_loss: 0.4601 - val_acc: 0.7830\n",
      "Epoch 83/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3657 - acc: 0.8398 - val_loss: 0.4497 - val_acc: 0.7855\n",
      "Epoch 84/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3648 - acc: 0.8405 - val_loss: 0.4510 - val_acc: 0.7851\n",
      "Epoch 85/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3655 - acc: 0.8400 - val_loss: 0.4541 - val_acc: 0.7856\n",
      "Epoch 86/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3639 - acc: 0.8409 - val_loss: 0.4496 - val_acc: 0.7862\n",
      "Epoch 87/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3647 - acc: 0.8404 - val_loss: 0.4495 - val_acc: 0.7855\n",
      "Epoch 88/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.3618 - acc: 0.8420 - val_loss: 0.4638 - val_acc: 0.7800\n",
      "Epoch 89/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3620 - acc: 0.8416 - val_loss: 0.4535 - val_acc: 0.7847\n",
      "Epoch 90/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3598 - acc: 0.8431 - val_loss: 0.4521 - val_acc: 0.7846\n",
      "Epoch 91/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3592 - acc: 0.8442 - val_loss: 0.4544 - val_acc: 0.7841\n",
      "Epoch 92/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3603 - acc: 0.8428 - val_loss: 0.4504 - val_acc: 0.7881\n",
      "Epoch 93/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3589 - acc: 0.8438 - val_loss: 0.4523 - val_acc: 0.7850\n",
      "Epoch 94/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3569 - acc: 0.8449 - val_loss: 0.4565 - val_acc: 0.7823\n",
      "Epoch 95/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3556 - acc: 0.8456 - val_loss: 0.4492 - val_acc: 0.7883\n",
      "Epoch 96/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3549 - acc: 0.8463 - val_loss: 0.4632 - val_acc: 0.7799\n",
      "Epoch 97/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3553 - acc: 0.8455 - val_loss: 0.4549 - val_acc: 0.7854\n",
      "Epoch 98/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3563 - acc: 0.8449 - val_loss: 0.4526 - val_acc: 0.7850\n",
      "Epoch 99/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3530 - acc: 0.8473 - val_loss: 0.4473 - val_acc: 0.7876\n",
      "Epoch 100/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3527 - acc: 0.8471 - val_loss: 0.4472 - val_acc: 0.7885\n",
      "Epoch 101/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3512 - acc: 0.8485 - val_loss: 0.4478 - val_acc: 0.7877\n",
      "Epoch 102/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3514 - acc: 0.8478 - val_loss: 0.4524 - val_acc: 0.7859\n",
      "Epoch 103/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3502 - acc: 0.8485 - val_loss: 0.4480 - val_acc: 0.7890\n",
      "Epoch 104/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3509 - acc: 0.8482 - val_loss: 0.4479 - val_acc: 0.7877\n",
      "Epoch 105/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3494 - acc: 0.8491 - val_loss: 0.4578 - val_acc: 0.7839\n",
      "Epoch 106/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3492 - acc: 0.8482 - val_loss: 0.4490 - val_acc: 0.7893\n",
      "Epoch 107/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3493 - acc: 0.8477 - val_loss: 0.4520 - val_acc: 0.7862\n",
      "Epoch 108/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3492 - acc: 0.8491 - val_loss: 0.4531 - val_acc: 0.7856\n",
      "Epoch 109/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3465 - acc: 0.8500 - val_loss: 0.4495 - val_acc: 0.7888\n",
      "Epoch 110/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3466 - acc: 0.8500 - val_loss: 0.4464 - val_acc: 0.7896\n",
      "Epoch 111/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3456 - acc: 0.8501 - val_loss: 0.4481 - val_acc: 0.7894\n",
      "Epoch 112/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3464 - acc: 0.8495 - val_loss: 0.4544 - val_acc: 0.7865\n",
      "Epoch 113/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3440 - acc: 0.8518 - val_loss: 0.4483 - val_acc: 0.7897\n",
      "Epoch 114/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.3446 - acc: 0.8512 - val_loss: 0.4484 - val_acc: 0.7892\n",
      "Epoch 115/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3430 - acc: 0.8523 - val_loss: 0.4463 - val_acc: 0.7899\n",
      "Epoch 116/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3432 - acc: 0.8514 - val_loss: 0.4483 - val_acc: 0.7906\n",
      "Epoch 117/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3473 - acc: 0.8473 - val_loss: 0.4831 - val_acc: 0.7731\n",
      "Epoch 118/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3426 - acc: 0.8516 - val_loss: 0.4499 - val_acc: 0.7889\n",
      "Epoch 119/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3417 - acc: 0.8517 - val_loss: 0.4519 - val_acc: 0.7864\n",
      "Epoch 120/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3423 - acc: 0.8510 - val_loss: 0.4589 - val_acc: 0.7858\n",
      "Epoch 121/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3389 - acc: 0.8539 - val_loss: 0.4526 - val_acc: 0.7868\n",
      "Epoch 122/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3399 - acc: 0.8525 - val_loss: 0.4476 - val_acc: 0.7901\n",
      "Epoch 123/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3383 - acc: 0.8532 - val_loss: 0.4492 - val_acc: 0.7888\n",
      "Epoch 124/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3377 - acc: 0.8546 - val_loss: 0.4569 - val_acc: 0.7871\n",
      "Epoch 125/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.3374 - acc: 0.8546 - val_loss: 0.4554 - val_acc: 0.7877\n",
      "auroc: 0.8448690846883138\n",
      "auprc: 0.7527417812948384\n",
      "auroc: 0.8440444789581281\n",
      "auprc: 0.7513129854644115\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 3s 100us/step - loss: 0.7160 - acc: 0.6079 - val_loss: 0.6872 - val_acc: 0.6378\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6677 - acc: 0.6329 - val_loss: 0.6767 - val_acc: 0.6222\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6542 - acc: 0.6428 - val_loss: 0.6734 - val_acc: 0.6295\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6455 - acc: 0.6495 - val_loss: 0.6709 - val_acc: 0.6377\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6384 - acc: 0.6536 - val_loss: 0.6661 - val_acc: 0.6470\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6303 - acc: 0.6595 - val_loss: 0.6540 - val_acc: 0.6367\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6192 - acc: 0.6669 - val_loss: 0.6475 - val_acc: 0.6435\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.6084 - acc: 0.6738 - val_loss: 0.6378 - val_acc: 0.6498\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5963 - acc: 0.6838 - val_loss: 0.6212 - val_acc: 0.6685\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5830 - acc: 0.6941 - val_loss: 0.6076 - val_acc: 0.6749\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5700 - acc: 0.7043 - val_loss: 0.6023 - val_acc: 0.6775\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5583 - acc: 0.7128 - val_loss: 0.5827 - val_acc: 0.6846\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5442 - acc: 0.7237 - val_loss: 0.5719 - val_acc: 0.6966\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5347 - acc: 0.7297 - val_loss: 0.5762 - val_acc: 0.7010\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5211 - acc: 0.7400 - val_loss: 0.5600 - val_acc: 0.7132\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5146 - acc: 0.7442 - val_loss: 0.5442 - val_acc: 0.7184\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5047 - acc: 0.7498 - val_loss: 0.5382 - val_acc: 0.7192\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5007 - acc: 0.7524 - val_loss: 0.5316 - val_acc: 0.7296\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4894 - acc: 0.7612 - val_loss: 0.5234 - val_acc: 0.7310\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4855 - acc: 0.7622 - val_loss: 0.5262 - val_acc: 0.7313\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4790 - acc: 0.7679 - val_loss: 0.5126 - val_acc: 0.7396\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4720 - acc: 0.7723 - val_loss: 0.5156 - val_acc: 0.7409\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4677 - acc: 0.7752 - val_loss: 0.5082 - val_acc: 0.7445\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4620 - acc: 0.7790 - val_loss: 0.5024 - val_acc: 0.7467\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4598 - acc: 0.7817 - val_loss: 0.5031 - val_acc: 0.7474\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4558 - acc: 0.7831 - val_loss: 0.4986 - val_acc: 0.7507\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4510 - acc: 0.7868 - val_loss: 0.5012 - val_acc: 0.7502\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4459 - acc: 0.7904 - val_loss: 0.4866 - val_acc: 0.7576\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4423 - acc: 0.7920 - val_loss: 0.4927 - val_acc: 0.7549\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4383 - acc: 0.7954 - val_loss: 0.4862 - val_acc: 0.7592\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4336 - acc: 0.7988 - val_loss: 0.4908 - val_acc: 0.7570\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4319 - acc: 0.7991 - val_loss: 0.4837 - val_acc: 0.7600\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4298 - acc: 0.8002 - val_loss: 0.4762 - val_acc: 0.7648\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4269 - acc: 0.8020 - val_loss: 0.4819 - val_acc: 0.7621\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4238 - acc: 0.8045 - val_loss: 0.4724 - val_acc: 0.7677\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4193 - acc: 0.8078 - val_loss: 0.4791 - val_acc: 0.7641\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4172 - acc: 0.8085 - val_loss: 0.4786 - val_acc: 0.7659\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.4154 - acc: 0.8102 - val_loss: 0.4704 - val_acc: 0.7697\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.4124 - acc: 0.8115 - val_loss: 0.4788 - val_acc: 0.7650\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4101 - acc: 0.8123 - val_loss: 0.4651 - val_acc: 0.7735\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4073 - acc: 0.8145 - val_loss: 0.4653 - val_acc: 0.7734\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4048 - acc: 0.8156 - val_loss: 0.4665 - val_acc: 0.7725\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4090 - acc: 0.8123 - val_loss: 0.4617 - val_acc: 0.7762\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4001 - acc: 0.8189 - val_loss: 0.4640 - val_acc: 0.7750\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3980 - acc: 0.8193 - val_loss: 0.4605 - val_acc: 0.7758\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3983 - acc: 0.8199 - val_loss: 0.4572 - val_acc: 0.7780\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3949 - acc: 0.8225 - val_loss: 0.4571 - val_acc: 0.7788\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3943 - acc: 0.8219 - val_loss: 0.4562 - val_acc: 0.7795\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3936 - acc: 0.8232 - val_loss: 0.4604 - val_acc: 0.7768\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3898 - acc: 0.8255 - val_loss: 0.4610 - val_acc: 0.7774\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3931 - acc: 0.8225 - val_loss: 0.4576 - val_acc: 0.7789\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3881 - acc: 0.8254 - val_loss: 0.4535 - val_acc: 0.7813\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3902 - acc: 0.8236 - val_loss: 0.4517 - val_acc: 0.7822\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3837 - acc: 0.8285 - val_loss: 0.4504 - val_acc: 0.7835\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3821 - acc: 0.8299 - val_loss: 0.4503 - val_acc: 0.7833\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3801 - acc: 0.8313 - val_loss: 0.4497 - val_acc: 0.7838\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.3787 - acc: 0.8315 - val_loss: 0.4503 - val_acc: 0.7831\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3779 - acc: 0.8321 - val_loss: 0.4529 - val_acc: 0.7815\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3762 - acc: 0.8333 - val_loss: 0.4472 - val_acc: 0.7849\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3750 - acc: 0.8338 - val_loss: 0.4502 - val_acc: 0.7835\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3747 - acc: 0.8336 - val_loss: 0.4464 - val_acc: 0.7851\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3736 - acc: 0.8341 - val_loss: 0.4461 - val_acc: 0.7859\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3712 - acc: 0.8360 - val_loss: 0.4503 - val_acc: 0.7842\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3707 - acc: 0.8364 - val_loss: 0.4468 - val_acc: 0.7855\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3706 - acc: 0.8356 - val_loss: 0.4506 - val_acc: 0.7838\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3692 - acc: 0.8358 - val_loss: 0.4507 - val_acc: 0.7841\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3695 - acc: 0.8357 - val_loss: 0.4460 - val_acc: 0.7858\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3695 - acc: 0.8361 - val_loss: 0.4485 - val_acc: 0.7855\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3675 - acc: 0.8375 - val_loss: 0.4458 - val_acc: 0.7875\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3642 - acc: 0.8398 - val_loss: 0.4480 - val_acc: 0.7855\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3653 - acc: 0.8387 - val_loss: 0.4462 - val_acc: 0.7881\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3649 - acc: 0.8382 - val_loss: 0.4431 - val_acc: 0.7888\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3620 - acc: 0.8400 - val_loss: 0.4451 - val_acc: 0.7883\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3634 - acc: 0.8387 - val_loss: 0.4437 - val_acc: 0.7894\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3593 - acc: 0.8422 - val_loss: 0.4434 - val_acc: 0.7884\n",
      "Epoch 76/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.3582 - acc: 0.8428 - val_loss: 0.4452 - val_acc: 0.7888\n",
      "Epoch 77/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3584 - acc: 0.8424 - val_loss: 0.4422 - val_acc: 0.7905\n",
      "Epoch 78/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3566 - acc: 0.8433 - val_loss: 0.4434 - val_acc: 0.7882\n",
      "Epoch 79/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3583 - acc: 0.8414 - val_loss: 0.4448 - val_acc: 0.7874\n",
      "Epoch 80/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3541 - acc: 0.8459 - val_loss: 0.4447 - val_acc: 0.7887\n",
      "Epoch 81/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3541 - acc: 0.8452 - val_loss: 0.4413 - val_acc: 0.7893\n",
      "Epoch 82/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3558 - acc: 0.8437 - val_loss: 0.4467 - val_acc: 0.7879\n",
      "Epoch 83/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3570 - acc: 0.8418 - val_loss: 0.4446 - val_acc: 0.7877\n",
      "Epoch 84/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3552 - acc: 0.8429 - val_loss: 0.4410 - val_acc: 0.7913\n",
      "Epoch 85/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3520 - acc: 0.8451 - val_loss: 0.4427 - val_acc: 0.7898\n",
      "Epoch 86/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3504 - acc: 0.8472 - val_loss: 0.4406 - val_acc: 0.7914\n",
      "Epoch 87/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3538 - acc: 0.8444 - val_loss: 0.4473 - val_acc: 0.7870\n",
      "Epoch 88/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3501 - acc: 0.8461 - val_loss: 0.4423 - val_acc: 0.7910\n",
      "Epoch 89/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3481 - acc: 0.8485 - val_loss: 0.4428 - val_acc: 0.7899\n",
      "Epoch 90/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3481 - acc: 0.8482 - val_loss: 0.4406 - val_acc: 0.7906\n",
      "Epoch 91/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3469 - acc: 0.8492 - val_loss: 0.4487 - val_acc: 0.7862\n",
      "Epoch 92/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3486 - acc: 0.8473 - val_loss: 0.4425 - val_acc: 0.7906\n",
      "Epoch 93/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3457 - acc: 0.8488 - val_loss: 0.4418 - val_acc: 0.7913\n",
      "Epoch 94/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3458 - acc: 0.8495 - val_loss: 0.4442 - val_acc: 0.7898\n",
      "Epoch 95/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.3449 - acc: 0.8502 - val_loss: 0.4437 - val_acc: 0.7917\n",
      "Epoch 96/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3477 - acc: 0.8475 - val_loss: 0.4469 - val_acc: 0.7902\n",
      "auroc: 0.8460920841895563\n",
      "auprc: 0.7530724923509172\n",
      "auroc: 0.8450351262969207\n",
      "auprc: 0.7508462016685985\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 4s 104us/step - loss: 0.7363 - acc: 0.6062 - val_loss: 0.6912 - val_acc: 0.5854\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.6664 - acc: 0.6326 - val_loss: 0.6734 - val_acc: 0.6263\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6503 - acc: 0.6432 - val_loss: 0.6683 - val_acc: 0.6324\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6389 - acc: 0.6517 - val_loss: 0.6693 - val_acc: 0.6286\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6281 - acc: 0.6601 - val_loss: 0.6559 - val_acc: 0.6359\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6173 - acc: 0.6677 - val_loss: 0.6406 - val_acc: 0.6605\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6041 - acc: 0.6780 - val_loss: 0.6320 - val_acc: 0.6604\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5937 - acc: 0.6869 - val_loss: 0.6235 - val_acc: 0.6604\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5811 - acc: 0.6957 - val_loss: 0.6052 - val_acc: 0.6775\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5686 - acc: 0.7073 - val_loss: 0.5941 - val_acc: 0.6840\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5559 - acc: 0.7165 - val_loss: 0.5854 - val_acc: 0.6948\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5456 - acc: 0.7248 - val_loss: 0.5813 - val_acc: 0.7047\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5389 - acc: 0.7278 - val_loss: 0.5758 - val_acc: 0.7035\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5272 - acc: 0.7360 - val_loss: 0.5565 - val_acc: 0.7145\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5185 - acc: 0.7415 - val_loss: 0.5525 - val_acc: 0.7192\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5110 - acc: 0.7457 - val_loss: 0.5417 - val_acc: 0.7227\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5035 - acc: 0.7516 - val_loss: 0.5369 - val_acc: 0.7267\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.4986 - acc: 0.7541 - val_loss: 0.5376 - val_acc: 0.7269\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4898 - acc: 0.7596 - val_loss: 0.5257 - val_acc: 0.7334\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.4857 - acc: 0.7612 - val_loss: 0.5236 - val_acc: 0.7358\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4787 - acc: 0.7664 - val_loss: 0.5138 - val_acc: 0.7419\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.4743 - acc: 0.7702 - val_loss: 0.5101 - val_acc: 0.7435\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4675 - acc: 0.7745 - val_loss: 0.5207 - val_acc: 0.7385\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4650 - acc: 0.7749 - val_loss: 0.5054 - val_acc: 0.7477\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4589 - acc: 0.7798 - val_loss: 0.4980 - val_acc: 0.7516\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4539 - acc: 0.7831 - val_loss: 0.4950 - val_acc: 0.7545\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4494 - acc: 0.7855 - val_loss: 0.4950 - val_acc: 0.7545\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4471 - acc: 0.7883 - val_loss: 0.4897 - val_acc: 0.7585\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4399 - acc: 0.7921 - val_loss: 0.4885 - val_acc: 0.7599\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4368 - acc: 0.7943 - val_loss: 0.4900 - val_acc: 0.7595\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4338 - acc: 0.7961 - val_loss: 0.4780 - val_acc: 0.7654\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4298 - acc: 0.7990 - val_loss: 0.4757 - val_acc: 0.7685\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4280 - acc: 0.8011 - val_loss: 0.4766 - val_acc: 0.7665\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4270 - acc: 0.8006 - val_loss: 0.4700 - val_acc: 0.7733\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4212 - acc: 0.8053 - val_loss: 0.4713 - val_acc: 0.7712\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4190 - acc: 0.8069 - val_loss: 0.4690 - val_acc: 0.7738\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4179 - acc: 0.8074 - val_loss: 0.4802 - val_acc: 0.7636\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4161 - acc: 0.8075 - val_loss: 0.4670 - val_acc: 0.7738\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4088 - acc: 0.8135 - val_loss: 0.4621 - val_acc: 0.7779\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4083 - acc: 0.8140 - val_loss: 0.4675 - val_acc: 0.7743\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4063 - acc: 0.8157 - val_loss: 0.4608 - val_acc: 0.7781\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4018 - acc: 0.8175 - val_loss: 0.4560 - val_acc: 0.7825\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4013 - acc: 0.8176 - val_loss: 0.4760 - val_acc: 0.7668\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4015 - acc: 0.8171 - val_loss: 0.4578 - val_acc: 0.7793\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3981 - acc: 0.8204 - val_loss: 0.4583 - val_acc: 0.7795\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3958 - acc: 0.8215 - val_loss: 0.4558 - val_acc: 0.7813\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3944 - acc: 0.8212 - val_loss: 0.4524 - val_acc: 0.7831\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3907 - acc: 0.8243 - val_loss: 0.4565 - val_acc: 0.7801\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3890 - acc: 0.8257 - val_loss: 0.4509 - val_acc: 0.7841\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3878 - acc: 0.8260 - val_loss: 0.4496 - val_acc: 0.7845\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3865 - acc: 0.8271 - val_loss: 0.4480 - val_acc: 0.7860\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3854 - acc: 0.8271 - val_loss: 0.4548 - val_acc: 0.7822\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3826 - acc: 0.8286 - val_loss: 0.4474 - val_acc: 0.7871\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3819 - acc: 0.8295 - val_loss: 0.4717 - val_acc: 0.7722\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3833 - acc: 0.8279 - val_loss: 0.4500 - val_acc: 0.7853\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3804 - acc: 0.8309 - val_loss: 0.4506 - val_acc: 0.7851\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3786 - acc: 0.8309 - val_loss: 0.4467 - val_acc: 0.7879\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3771 - acc: 0.8320 - val_loss: 0.4547 - val_acc: 0.7827\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3762 - acc: 0.8326 - val_loss: 0.4464 - val_acc: 0.7882\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3781 - acc: 0.8298 - val_loss: 0.4543 - val_acc: 0.7826\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3753 - acc: 0.8326 - val_loss: 0.4470 - val_acc: 0.7870\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3736 - acc: 0.8335 - val_loss: 0.4451 - val_acc: 0.7887\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3693 - acc: 0.8370 - val_loss: 0.4449 - val_acc: 0.7882\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3709 - acc: 0.8364 - val_loss: 0.4465 - val_acc: 0.7863\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3685 - acc: 0.8369 - val_loss: 0.4511 - val_acc: 0.7868\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3671 - acc: 0.8378 - val_loss: 0.4450 - val_acc: 0.7876\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3655 - acc: 0.8387 - val_loss: 0.4424 - val_acc: 0.7902\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3648 - acc: 0.8397 - val_loss: 0.4414 - val_acc: 0.7914\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3626 - acc: 0.8401 - val_loss: 0.4412 - val_acc: 0.7906\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3640 - acc: 0.8389 - val_loss: 0.4440 - val_acc: 0.7896\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3614 - acc: 0.8408 - val_loss: 0.4408 - val_acc: 0.7912\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3607 - acc: 0.8412 - val_loss: 0.4406 - val_acc: 0.7909\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3597 - acc: 0.8408 - val_loss: 0.4401 - val_acc: 0.7923\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3586 - acc: 0.8416 - val_loss: 0.4493 - val_acc: 0.7865\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3597 - acc: 0.8411 - val_loss: 0.4418 - val_acc: 0.7910\n",
      "Epoch 76/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3600 - acc: 0.8399 - val_loss: 0.4434 - val_acc: 0.7902\n",
      "Epoch 77/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3588 - acc: 0.8398 - val_loss: 0.4465 - val_acc: 0.7885\n",
      "Epoch 78/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3557 - acc: 0.8425 - val_loss: 0.4477 - val_acc: 0.7879\n",
      "Epoch 79/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3551 - acc: 0.8427 - val_loss: 0.4398 - val_acc: 0.7919\n",
      "Epoch 80/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3539 - acc: 0.8431 - val_loss: 0.4422 - val_acc: 0.7910\n",
      "Epoch 81/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3545 - acc: 0.8436 - val_loss: 0.4424 - val_acc: 0.7910\n",
      "Epoch 82/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3536 - acc: 0.8436 - val_loss: 0.4407 - val_acc: 0.7913\n",
      "Epoch 83/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3513 - acc: 0.8457 - val_loss: 0.4450 - val_acc: 0.7895\n",
      "Epoch 84/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3521 - acc: 0.8446 - val_loss: 0.4443 - val_acc: 0.7901\n",
      "Epoch 85/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3511 - acc: 0.8454 - val_loss: 0.4406 - val_acc: 0.7918\n",
      "Epoch 86/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3494 - acc: 0.8457 - val_loss: 0.4407 - val_acc: 0.7926\n",
      "Epoch 87/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3491 - acc: 0.8459 - val_loss: 0.4432 - val_acc: 0.7908\n",
      "Epoch 88/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3489 - acc: 0.8464 - val_loss: 0.4447 - val_acc: 0.7908\n",
      "Epoch 89/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3473 - acc: 0.8475 - val_loss: 0.4499 - val_acc: 0.7867\n",
      "auroc: 0.8474019708565046\n",
      "auprc: 0.7527307325266497\n",
      "auroc: 0.8464389140107177\n",
      "auprc: 0.7509430730603421\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 4s 105us/step - loss: 0.7687 - acc: 0.5951 - val_loss: 0.6913 - val_acc: 0.6298\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6709 - acc: 0.6316 - val_loss: 0.6820 - val_acc: 0.6234\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6559 - acc: 0.6405 - val_loss: 0.6719 - val_acc: 0.6278\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6457 - acc: 0.6486 - val_loss: 0.6681 - val_acc: 0.6310\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6363 - acc: 0.6551 - val_loss: 0.6642 - val_acc: 0.6353\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6303 - acc: 0.6595 - val_loss: 0.6559 - val_acc: 0.6434\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6198 - acc: 0.6660 - val_loss: 0.6477 - val_acc: 0.6483\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6105 - acc: 0.6733 - val_loss: 0.6396 - val_acc: 0.6528\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6005 - acc: 0.6819 - val_loss: 0.6328 - val_acc: 0.6509\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.5904 - acc: 0.6873 - val_loss: 0.6183 - val_acc: 0.6693\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5779 - acc: 0.6973 - val_loss: 0.6059 - val_acc: 0.6771\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5674 - acc: 0.7061 - val_loss: 0.6071 - val_acc: 0.6754\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5590 - acc: 0.7119 - val_loss: 0.5874 - val_acc: 0.6890\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.5453 - acc: 0.7217 - val_loss: 0.5844 - val_acc: 0.6888\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5347 - acc: 0.7306 - val_loss: 0.5638 - val_acc: 0.7078\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5252 - acc: 0.7378 - val_loss: 0.5598 - val_acc: 0.7101\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5161 - acc: 0.7443 - val_loss: 0.5494 - val_acc: 0.7176\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5087 - acc: 0.7493 - val_loss: 0.5487 - val_acc: 0.7191\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5031 - acc: 0.7532 - val_loss: 0.5466 - val_acc: 0.7252\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4969 - acc: 0.7570 - val_loss: 0.5294 - val_acc: 0.7291\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4916 - acc: 0.7610 - val_loss: 0.5280 - val_acc: 0.7323\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4862 - acc: 0.7644 - val_loss: 0.5420 - val_acc: 0.7278\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4824 - acc: 0.7661 - val_loss: 0.5197 - val_acc: 0.7381\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4756 - acc: 0.7707 - val_loss: 0.5250 - val_acc: 0.7359\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4710 - acc: 0.7732 - val_loss: 0.5124 - val_acc: 0.7439\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4661 - acc: 0.7776 - val_loss: 0.5109 - val_acc: 0.7465\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4615 - acc: 0.7799 - val_loss: 0.5030 - val_acc: 0.7491\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4571 - acc: 0.7833 - val_loss: 0.5055 - val_acc: 0.7466\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4545 - acc: 0.7851 - val_loss: 0.4968 - val_acc: 0.7531\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.4509 - acc: 0.7878 - val_loss: 0.5028 - val_acc: 0.7502\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4469 - acc: 0.7910 - val_loss: 0.4921 - val_acc: 0.7556\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4443 - acc: 0.7919 - val_loss: 0.4903 - val_acc: 0.7583\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4407 - acc: 0.7948 - val_loss: 0.4987 - val_acc: 0.7516\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4409 - acc: 0.7931 - val_loss: 0.4889 - val_acc: 0.7590\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.4398 - acc: 0.7938 - val_loss: 0.4940 - val_acc: 0.7558\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4353 - acc: 0.7969 - val_loss: 0.4865 - val_acc: 0.7607\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4294 - acc: 0.8026 - val_loss: 0.4784 - val_acc: 0.7658\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4275 - acc: 0.8038 - val_loss: 0.4915 - val_acc: 0.7578\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4251 - acc: 0.8046 - val_loss: 0.4769 - val_acc: 0.7672\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4226 - acc: 0.8062 - val_loss: 0.4783 - val_acc: 0.7646\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4223 - acc: 0.8057 - val_loss: 0.4746 - val_acc: 0.7676\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4175 - acc: 0.8095 - val_loss: 0.4711 - val_acc: 0.7708\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4170 - acc: 0.8098 - val_loss: 0.4822 - val_acc: 0.7634\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4142 - acc: 0.8107 - val_loss: 0.4731 - val_acc: 0.7700\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4131 - acc: 0.8117 - val_loss: 0.4842 - val_acc: 0.7633\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4117 - acc: 0.8122 - val_loss: 0.4698 - val_acc: 0.7708\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4069 - acc: 0.8165 - val_loss: 0.4681 - val_acc: 0.7717\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4054 - acc: 0.8164 - val_loss: 0.4703 - val_acc: 0.7712\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4049 - acc: 0.8169 - val_loss: 0.4657 - val_acc: 0.7722\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4033 - acc: 0.8177 - val_loss: 0.4698 - val_acc: 0.7708\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4019 - acc: 0.8184 - val_loss: 0.4657 - val_acc: 0.7729\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3994 - acc: 0.8199 - val_loss: 0.4645 - val_acc: 0.7728\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3988 - acc: 0.8208 - val_loss: 0.4630 - val_acc: 0.7771\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3974 - acc: 0.8211 - val_loss: 0.4626 - val_acc: 0.7755\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3953 - acc: 0.8228 - val_loss: 0.4653 - val_acc: 0.7742\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3939 - acc: 0.8233 - val_loss: 0.4639 - val_acc: 0.7758\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3959 - acc: 0.8208 - val_loss: 0.4585 - val_acc: 0.7785\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3920 - acc: 0.8241 - val_loss: 0.4605 - val_acc: 0.7775\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3891 - acc: 0.8262 - val_loss: 0.4608 - val_acc: 0.7775\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3867 - acc: 0.8272 - val_loss: 0.4574 - val_acc: 0.7793\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3874 - acc: 0.8270 - val_loss: 0.4602 - val_acc: 0.7782\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3863 - acc: 0.8272 - val_loss: 0.4631 - val_acc: 0.7774\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3837 - acc: 0.8298 - val_loss: 0.4575 - val_acc: 0.7792\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3842 - acc: 0.8291 - val_loss: 0.4789 - val_acc: 0.7667\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3865 - acc: 0.8260 - val_loss: 0.4619 - val_acc: 0.7761\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3812 - acc: 0.8305 - val_loss: 0.4640 - val_acc: 0.7754\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3801 - acc: 0.8311 - val_loss: 0.4586 - val_acc: 0.7779\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3784 - acc: 0.8323 - val_loss: 0.4628 - val_acc: 0.7781\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3774 - acc: 0.8326 - val_loss: 0.4559 - val_acc: 0.7807\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3777 - acc: 0.8324 - val_loss: 0.4564 - val_acc: 0.7803\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3771 - acc: 0.8326 - val_loss: 0.4591 - val_acc: 0.7793\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3768 - acc: 0.8325 - val_loss: 0.4684 - val_acc: 0.7755\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3772 - acc: 0.8311 - val_loss: 0.4634 - val_acc: 0.7760\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3736 - acc: 0.8351 - val_loss: 0.4548 - val_acc: 0.7827\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3722 - acc: 0.8351 - val_loss: 0.4572 - val_acc: 0.7809\n",
      "Epoch 76/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3709 - acc: 0.8370 - val_loss: 0.4576 - val_acc: 0.7792\n",
      "Epoch 77/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3705 - acc: 0.8366 - val_loss: 0.4540 - val_acc: 0.7821\n",
      "Epoch 78/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3713 - acc: 0.8354 - val_loss: 0.4653 - val_acc: 0.7748\n",
      "Epoch 79/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3683 - acc: 0.8378 - val_loss: 0.4603 - val_acc: 0.7785\n",
      "Epoch 80/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3671 - acc: 0.8391 - val_loss: 0.4541 - val_acc: 0.7817\n",
      "Epoch 81/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3651 - acc: 0.8401 - val_loss: 0.4564 - val_acc: 0.7805\n",
      "Epoch 82/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3654 - acc: 0.8389 - val_loss: 0.4551 - val_acc: 0.7817\n",
      "Epoch 83/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3656 - acc: 0.8391 - val_loss: 0.4638 - val_acc: 0.7782\n",
      "Epoch 84/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3640 - acc: 0.8396 - val_loss: 0.4552 - val_acc: 0.7823\n",
      "Epoch 85/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3655 - acc: 0.8385 - val_loss: 0.4540 - val_acc: 0.7835\n",
      "Epoch 86/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3638 - acc: 0.8397 - val_loss: 0.4529 - val_acc: 0.7839\n",
      "Epoch 87/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3620 - acc: 0.8399 - val_loss: 0.4522 - val_acc: 0.7837\n",
      "Epoch 88/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3662 - acc: 0.8373 - val_loss: 0.4636 - val_acc: 0.7756\n",
      "Epoch 89/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3636 - acc: 0.8390 - val_loss: 0.4564 - val_acc: 0.7798\n",
      "Epoch 90/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3601 - acc: 0.8417 - val_loss: 0.4546 - val_acc: 0.7850\n",
      "Epoch 91/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3591 - acc: 0.8422 - val_loss: 0.4529 - val_acc: 0.7848\n",
      "Epoch 92/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3633 - acc: 0.8376 - val_loss: 0.4634 - val_acc: 0.7762\n",
      "Epoch 93/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3589 - acc: 0.8425 - val_loss: 0.4596 - val_acc: 0.7800\n",
      "Epoch 94/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3568 - acc: 0.8435 - val_loss: 0.4550 - val_acc: 0.7816\n",
      "Epoch 95/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3565 - acc: 0.8435 - val_loss: 0.4525 - val_acc: 0.7841\n",
      "Epoch 96/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3568 - acc: 0.8433 - val_loss: 0.4527 - val_acc: 0.7843\n",
      "Epoch 97/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3574 - acc: 0.8424 - val_loss: 0.4640 - val_acc: 0.7775\n",
      "auroc: 0.8377135017095938\n",
      "auprc: 0.7413568874052204\n",
      "auroc: 0.836981715539217\n",
      "auprc: 0.7402824913388902\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 4s 109us/step - loss: 0.8036 - acc: 0.5940 - val_loss: 0.7044 - val_acc: 0.6077\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.6727 - acc: 0.6293 - val_loss: 0.6776 - val_acc: 0.6330\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.6571 - acc: 0.6400 - val_loss: 0.6724 - val_acc: 0.6366\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.6475 - acc: 0.6475 - val_loss: 0.6680 - val_acc: 0.6305\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.6403 - acc: 0.6514 - val_loss: 0.6646 - val_acc: 0.6422\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.6347 - acc: 0.6563 - val_loss: 0.6701 - val_acc: 0.6143\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.6298 - acc: 0.6600 - val_loss: 0.6627 - val_acc: 0.6447\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.6243 - acc: 0.6634 - val_loss: 0.6564 - val_acc: 0.6444\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.6159 - acc: 0.6703 - val_loss: 0.6500 - val_acc: 0.6560\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.6087 - acc: 0.6757 - val_loss: 0.6420 - val_acc: 0.6476\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.5997 - acc: 0.6831 - val_loss: 0.6336 - val_acc: 0.6623\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.5906 - acc: 0.6898 - val_loss: 0.6211 - val_acc: 0.6703\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.5789 - acc: 0.6999 - val_loss: 0.6105 - val_acc: 0.6765\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.5678 - acc: 0.7086 - val_loss: 0.6085 - val_acc: 0.6879\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.5593 - acc: 0.7144 - val_loss: 0.5949 - val_acc: 0.6856\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.5480 - acc: 0.7225 - val_loss: 0.5827 - val_acc: 0.6959\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.5382 - acc: 0.7299 - val_loss: 0.5748 - val_acc: 0.7052\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.5305 - acc: 0.7351 - val_loss: 0.5759 - val_acc: 0.7057\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.5219 - acc: 0.7413 - val_loss: 0.5588 - val_acc: 0.7122\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.5139 - acc: 0.7464 - val_loss: 0.5540 - val_acc: 0.7157\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.5069 - acc: 0.7504 - val_loss: 0.5470 - val_acc: 0.7203\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.5015 - acc: 0.7525 - val_loss: 0.5481 - val_acc: 0.7187\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4955 - acc: 0.7583 - val_loss: 0.5346 - val_acc: 0.7275\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4901 - acc: 0.7605 - val_loss: 0.5467 - val_acc: 0.7264\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4873 - acc: 0.7616 - val_loss: 0.5259 - val_acc: 0.7324\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4789 - acc: 0.7695 - val_loss: 0.5302 - val_acc: 0.7305\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4746 - acc: 0.7712 - val_loss: 0.5190 - val_acc: 0.7362\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4703 - acc: 0.7736 - val_loss: 0.5191 - val_acc: 0.7380\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4653 - acc: 0.7777 - val_loss: 0.5150 - val_acc: 0.7411\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4639 - acc: 0.7773 - val_loss: 0.5070 - val_acc: 0.7432\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4571 - acc: 0.7829 - val_loss: 0.5035 - val_acc: 0.7453\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4535 - acc: 0.7847 - val_loss: 0.5083 - val_acc: 0.7449\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4502 - acc: 0.7864 - val_loss: 0.5161 - val_acc: 0.7434\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4470 - acc: 0.7889 - val_loss: 0.5011 - val_acc: 0.7495\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4421 - acc: 0.7919 - val_loss: 0.4929 - val_acc: 0.7552\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4395 - acc: 0.7928 - val_loss: 0.4908 - val_acc: 0.7569\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4351 - acc: 0.7971 - val_loss: 0.4895 - val_acc: 0.7586\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4332 - acc: 0.7978 - val_loss: 0.4844 - val_acc: 0.7607\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4290 - acc: 0.8006 - val_loss: 0.4825 - val_acc: 0.7618\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4261 - acc: 0.8021 - val_loss: 0.4807 - val_acc: 0.7632\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4249 - acc: 0.8032 - val_loss: 0.4799 - val_acc: 0.7649\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4197 - acc: 0.8070 - val_loss: 0.4777 - val_acc: 0.7661\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4195 - acc: 0.8059 - val_loss: 0.4808 - val_acc: 0.7648\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4249 - acc: 0.8018 - val_loss: 0.4997 - val_acc: 0.7517\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4226 - acc: 0.8024 - val_loss: 0.4872 - val_acc: 0.7561\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4131 - acc: 0.8102 - val_loss: 0.4726 - val_acc: 0.7698\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4088 - acc: 0.8146 - val_loss: 0.4703 - val_acc: 0.7706\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4070 - acc: 0.8151 - val_loss: 0.4730 - val_acc: 0.7697\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4087 - acc: 0.8126 - val_loss: 0.4731 - val_acc: 0.7691\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4054 - acc: 0.8154 - val_loss: 0.4695 - val_acc: 0.7706\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4043 - acc: 0.8163 - val_loss: 0.4798 - val_acc: 0.7640\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4008 - acc: 0.8186 - val_loss: 0.4735 - val_acc: 0.7694\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.4002 - acc: 0.8184 - val_loss: 0.4698 - val_acc: 0.7706\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3977 - acc: 0.8202 - val_loss: 0.4639 - val_acc: 0.7741\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3966 - acc: 0.8207 - val_loss: 0.4764 - val_acc: 0.7690\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3969 - acc: 0.8193 - val_loss: 0.4626 - val_acc: 0.7754\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3920 - acc: 0.8238 - val_loss: 0.4628 - val_acc: 0.7749\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3908 - acc: 0.8248 - val_loss: 0.4609 - val_acc: 0.7761\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3887 - acc: 0.8250 - val_loss: 0.4711 - val_acc: 0.7730\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3891 - acc: 0.8246 - val_loss: 0.4596 - val_acc: 0.7764\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3862 - acc: 0.8272 - val_loss: 0.4646 - val_acc: 0.7737\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3858 - acc: 0.8267 - val_loss: 0.4682 - val_acc: 0.7711\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3843 - acc: 0.8285 - val_loss: 0.4750 - val_acc: 0.7674\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3846 - acc: 0.8280 - val_loss: 0.4613 - val_acc: 0.7745\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3820 - acc: 0.8287 - val_loss: 0.4579 - val_acc: 0.7772\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3800 - acc: 0.8297 - val_loss: 0.4633 - val_acc: 0.7754\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3791 - acc: 0.8297 - val_loss: 0.4582 - val_acc: 0.7777\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3781 - acc: 0.8313 - val_loss: 0.4579 - val_acc: 0.7765\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3768 - acc: 0.8328 - val_loss: 0.4609 - val_acc: 0.7756\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3775 - acc: 0.8309 - val_loss: 0.4618 - val_acc: 0.7750\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3741 - acc: 0.8337 - val_loss: 0.4644 - val_acc: 0.7734\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3730 - acc: 0.8341 - val_loss: 0.4561 - val_acc: 0.7784\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3720 - acc: 0.8347 - val_loss: 0.4562 - val_acc: 0.7780\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3717 - acc: 0.8346 - val_loss: 0.4622 - val_acc: 0.7753\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3707 - acc: 0.8343 - val_loss: 0.4562 - val_acc: 0.7789\n",
      "Epoch 76/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3699 - acc: 0.8358 - val_loss: 0.4567 - val_acc: 0.7784\n",
      "Epoch 77/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3691 - acc: 0.8360 - val_loss: 0.4646 - val_acc: 0.7765\n",
      "Epoch 78/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3706 - acc: 0.8349 - val_loss: 0.4567 - val_acc: 0.7780\n",
      "Epoch 79/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3663 - acc: 0.8372 - val_loss: 0.4586 - val_acc: 0.7783\n",
      "Epoch 80/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3694 - acc: 0.8338 - val_loss: 0.4556 - val_acc: 0.7789\n",
      "Epoch 81/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3646 - acc: 0.8390 - val_loss: 0.4547 - val_acc: 0.7786\n",
      "Epoch 82/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3633 - acc: 0.8393 - val_loss: 0.4590 - val_acc: 0.7772\n",
      "Epoch 83/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3649 - acc: 0.8380 - val_loss: 0.4583 - val_acc: 0.7803\n",
      "Epoch 84/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3651 - acc: 0.8373 - val_loss: 0.4576 - val_acc: 0.7781\n",
      "Epoch 85/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3622 - acc: 0.8394 - val_loss: 0.4567 - val_acc: 0.7799\n",
      "Epoch 86/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3625 - acc: 0.8385 - val_loss: 0.4559 - val_acc: 0.7803\n",
      "Epoch 87/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3602 - acc: 0.8409 - val_loss: 0.4668 - val_acc: 0.7769\n",
      "Epoch 88/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3630 - acc: 0.8383 - val_loss: 0.4585 - val_acc: 0.7783\n",
      "Epoch 89/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3619 - acc: 0.8384 - val_loss: 0.4561 - val_acc: 0.7804\n",
      "Epoch 90/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3576 - acc: 0.8424 - val_loss: 0.4623 - val_acc: 0.7771\n",
      "Epoch 91/200\n",
      "34284/34284 [==============================] - 1s 41us/step - loss: 0.3594 - acc: 0.8399 - val_loss: 0.4549 - val_acc: 0.7800\n",
      "auroc: 0.8345950383995336\n",
      "auprc: 0.7316016617744582\n",
      "auroc: 0.8337908330828517\n",
      "auprc: 0.7299550534158957\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 4s 116us/step - loss: 0.7059 - acc: 0.6117 - val_loss: 0.6907 - val_acc: 0.6050\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6657 - acc: 0.6343 - val_loss: 0.6805 - val_acc: 0.6285\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.6527 - acc: 0.6446 - val_loss: 0.6731 - val_acc: 0.6245\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6410 - acc: 0.6517 - val_loss: 0.6675 - val_acc: 0.6256\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6323 - acc: 0.6581 - val_loss: 0.6591 - val_acc: 0.6425\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6217 - acc: 0.6650 - val_loss: 0.6499 - val_acc: 0.6450\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6109 - acc: 0.6724 - val_loss: 0.6487 - val_acc: 0.6489\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.6019 - acc: 0.6793 - val_loss: 0.6339 - val_acc: 0.6579\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5918 - acc: 0.6888 - val_loss: 0.6255 - val_acc: 0.6733\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5819 - acc: 0.6949 - val_loss: 0.6247 - val_acc: 0.6794\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5728 - acc: 0.7005 - val_loss: 0.6166 - val_acc: 0.6788\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5630 - acc: 0.7095 - val_loss: 0.6068 - val_acc: 0.6791\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5505 - acc: 0.7181 - val_loss: 0.5847 - val_acc: 0.6924\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5406 - acc: 0.7249 - val_loss: 0.5796 - val_acc: 0.6949\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 1s 44us/step - loss: 0.5302 - acc: 0.7316 - val_loss: 0.5743 - val_acc: 0.7008\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5232 - acc: 0.7361 - val_loss: 0.5616 - val_acc: 0.7054\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5145 - acc: 0.7426 - val_loss: 0.5531 - val_acc: 0.7113\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5073 - acc: 0.7476 - val_loss: 0.5464 - val_acc: 0.7139\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.5008 - acc: 0.7518 - val_loss: 0.5446 - val_acc: 0.7188\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.4967 - acc: 0.7537 - val_loss: 0.5365 - val_acc: 0.7197\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4891 - acc: 0.7592 - val_loss: 0.5301 - val_acc: 0.7261\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.4842 - acc: 0.7629 - val_loss: 0.5251 - val_acc: 0.7286\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4782 - acc: 0.7669 - val_loss: 0.5284 - val_acc: 0.7305\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.4736 - acc: 0.7701 - val_loss: 0.5247 - val_acc: 0.7265\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4708 - acc: 0.7721 - val_loss: 0.5173 - val_acc: 0.7393\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4648 - acc: 0.7765 - val_loss: 0.5141 - val_acc: 0.7396\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4600 - acc: 0.7808 - val_loss: 0.5055 - val_acc: 0.7427\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4566 - acc: 0.7812 - val_loss: 0.5203 - val_acc: 0.7368\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4532 - acc: 0.7852 - val_loss: 0.5061 - val_acc: 0.7469\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4508 - acc: 0.7863 - val_loss: 0.5105 - val_acc: 0.7429\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4448 - acc: 0.7909 - val_loss: 0.4970 - val_acc: 0.7521\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4408 - acc: 0.7934 - val_loss: 0.4941 - val_acc: 0.7523\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4392 - acc: 0.7937 - val_loss: 0.4906 - val_acc: 0.7530\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4352 - acc: 0.7971 - val_loss: 0.4870 - val_acc: 0.7571\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4330 - acc: 0.7982 - val_loss: 0.4859 - val_acc: 0.7584\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4297 - acc: 0.8004 - val_loss: 0.4845 - val_acc: 0.7603\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4282 - acc: 0.8017 - val_loss: 0.4799 - val_acc: 0.7617\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4234 - acc: 0.8051 - val_loss: 0.4911 - val_acc: 0.7573\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4211 - acc: 0.8068 - val_loss: 0.4836 - val_acc: 0.7619\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4197 - acc: 0.8075 - val_loss: 0.4842 - val_acc: 0.7576\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4168 - acc: 0.8091 - val_loss: 0.4780 - val_acc: 0.7644\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4152 - acc: 0.8096 - val_loss: 0.4718 - val_acc: 0.7678\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4157 - acc: 0.8083 - val_loss: 0.4788 - val_acc: 0.7620\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4110 - acc: 0.8118 - val_loss: 0.4719 - val_acc: 0.7678\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4099 - acc: 0.8122 - val_loss: 0.4756 - val_acc: 0.7653\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4074 - acc: 0.8141 - val_loss: 0.4685 - val_acc: 0.7699\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4038 - acc: 0.8173 - val_loss: 0.4686 - val_acc: 0.7707\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4030 - acc: 0.8165 - val_loss: 0.4814 - val_acc: 0.7627\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4006 - acc: 0.8186 - val_loss: 0.4624 - val_acc: 0.7744\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.4006 - acc: 0.8181 - val_loss: 0.4713 - val_acc: 0.7701\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3968 - acc: 0.8205 - val_loss: 0.4702 - val_acc: 0.7693\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3967 - acc: 0.8202 - val_loss: 0.4706 - val_acc: 0.7687\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3970 - acc: 0.8205 - val_loss: 0.4640 - val_acc: 0.7735\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3935 - acc: 0.8223 - val_loss: 0.4668 - val_acc: 0.7720\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3897 - acc: 0.8247 - val_loss: 0.4595 - val_acc: 0.7776\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3904 - acc: 0.8240 - val_loss: 0.4574 - val_acc: 0.7782\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3878 - acc: 0.8256 - val_loss: 0.4567 - val_acc: 0.7786\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3860 - acc: 0.8269 - val_loss: 0.4598 - val_acc: 0.7773\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3844 - acc: 0.8284 - val_loss: 0.4646 - val_acc: 0.7758\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3828 - acc: 0.8295 - val_loss: 0.4582 - val_acc: 0.7778\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3837 - acc: 0.8276 - val_loss: 0.4647 - val_acc: 0.7757\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3824 - acc: 0.8286 - val_loss: 0.4568 - val_acc: 0.7797\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3788 - acc: 0.8311 - val_loss: 0.4566 - val_acc: 0.7792\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3779 - acc: 0.8319 - val_loss: 0.4526 - val_acc: 0.7818\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3774 - acc: 0.8319 - val_loss: 0.4539 - val_acc: 0.7827\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3796 - acc: 0.8299 - val_loss: 0.4540 - val_acc: 0.7799\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3763 - acc: 0.8321 - val_loss: 0.4562 - val_acc: 0.7810\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3734 - acc: 0.8346 - val_loss: 0.4557 - val_acc: 0.7835\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3730 - acc: 0.8346 - val_loss: 0.4539 - val_acc: 0.7807\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3731 - acc: 0.8334 - val_loss: 0.4526 - val_acc: 0.7841\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3703 - acc: 0.8364 - val_loss: 0.4520 - val_acc: 0.7840\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3709 - acc: 0.8361 - val_loss: 0.4508 - val_acc: 0.7845\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3698 - acc: 0.8371 - val_loss: 0.4516 - val_acc: 0.7849\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3678 - acc: 0.8381 - val_loss: 0.4533 - val_acc: 0.7829\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3681 - acc: 0.8376 - val_loss: 0.4514 - val_acc: 0.7837\n",
      "Epoch 76/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3670 - acc: 0.8376 - val_loss: 0.4552 - val_acc: 0.7838\n",
      "Epoch 77/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3664 - acc: 0.8378 - val_loss: 0.4507 - val_acc: 0.7852\n",
      "Epoch 78/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3652 - acc: 0.8393 - val_loss: 0.4517 - val_acc: 0.7830\n",
      "Epoch 79/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3637 - acc: 0.8398 - val_loss: 0.4502 - val_acc: 0.7852\n",
      "Epoch 80/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3613 - acc: 0.8411 - val_loss: 0.4500 - val_acc: 0.7858\n",
      "Epoch 81/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3612 - acc: 0.8407 - val_loss: 0.4523 - val_acc: 0.7848\n",
      "Epoch 82/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3623 - acc: 0.8389 - val_loss: 0.4489 - val_acc: 0.7872\n",
      "Epoch 83/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3601 - acc: 0.8417 - val_loss: 0.4522 - val_acc: 0.7851\n",
      "Epoch 84/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3605 - acc: 0.8412 - val_loss: 0.4519 - val_acc: 0.7863\n",
      "Epoch 85/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3588 - acc: 0.8416 - val_loss: 0.4480 - val_acc: 0.7890\n",
      "Epoch 86/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3587 - acc: 0.8413 - val_loss: 0.4491 - val_acc: 0.7870\n",
      "Epoch 87/200\n",
      "34284/34284 [==============================] - 2s 44us/step - loss: 0.3578 - acc: 0.8425 - val_loss: 0.4502 - val_acc: 0.7879\n",
      "Epoch 88/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3585 - acc: 0.8409 - val_loss: 0.4538 - val_acc: 0.7835\n",
      "Epoch 89/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3567 - acc: 0.8435 - val_loss: 0.4480 - val_acc: 0.7892\n",
      "Epoch 90/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3542 - acc: 0.8448 - val_loss: 0.4503 - val_acc: 0.7875\n",
      "Epoch 91/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3557 - acc: 0.8431 - val_loss: 0.4528 - val_acc: 0.7845\n",
      "Epoch 92/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3537 - acc: 0.8446 - val_loss: 0.4564 - val_acc: 0.7828\n",
      "Epoch 93/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3528 - acc: 0.8453 - val_loss: 0.4493 - val_acc: 0.7880\n",
      "Epoch 94/200\n",
      "34284/34284 [==============================] - 1s 42us/step - loss: 0.3542 - acc: 0.8446 - val_loss: 0.4482 - val_acc: 0.7891\n",
      "Epoch 95/200\n",
      "34284/34284 [==============================] - 1s 43us/step - loss: 0.3513 - acc: 0.8458 - val_loss: 0.4503 - val_acc: 0.7878\n",
      "auroc: 0.8398766389792902\n",
      "auprc: 0.7403084430881627\n",
      "auroc: 0.8390139387440224\n",
      "auprc: 0.7388699820333106\n"
     ]
    }
   ],
   "source": [
    "for curr_seed in all_seeds: \n",
    "    model, early_stopping_callback, auroc_callback = train_model(model_wrapper = REG_WRAPPER, \n",
    "                                                                 aug = \"rev_after_all\", \n",
    "                                                                 curr_seed = curr_seed,\n",
    "                                                                 batch_size = 1000,\n",
    "                                                                 x = x_train_1000, \n",
    "                                                                 y = y_train_1000, \n",
    "                                                                 val_data = (x_val_1000, y_val_1000))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"Aug_Alt_double_1000_auROC\", curr_seed = curr_seed,\n",
    "             callback = auroc_callback, model = model, val_data = (x_val_1000, y_val_1000))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"Aug_Alt_double_1000_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_val_1000, y_val_1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seq length 200 auROC callback vs loss callback for auROC\n",
    "#seq length 200 auROC callback vs loss callback for auPRC\n",
    "#seq length 1000 auROC callback vs loss callback for auROC\n",
    "#seq length 1000 auROC callback vs loss callback for auPRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_from_file(all_seeds, seq_len): \n",
    "    auROC_auROC_results = {\n",
    "        'RC': [],\n",
    "        'Siamese': [],\n",
    "        'Aug': [], \n",
    "        'Aug_Alt': [],\n",
    "        'Aug_Alt_double': [], \n",
    "        'Standard': []\n",
    "    }\n",
    "    auROC_auPRC_results = {\n",
    "        'RC': [],\n",
    "        'Siamese': [],\n",
    "        'Aug': [], \n",
    "        'Aug_Alt': [],\n",
    "        'Aug_Alt_double': [], \n",
    "        'Standard': []\n",
    "    }\n",
    "    \n",
    "    loss_auROC_results = {\n",
    "        'RC': [],\n",
    "        'Siamese': [],\n",
    "        'Aug': [], \n",
    "        'Aug_Alt': [],\n",
    "        'Aug_Alt_double': [], \n",
    "        'Standard': []\n",
    "    }\n",
    "    loss_auPRC_results = {\n",
    "        'RC': [],\n",
    "        'Siamese': [],\n",
    "        'Aug': [], \n",
    "        'Aug_Alt': [],\n",
    "        'Aug_Alt_double': [], \n",
    "        'Standard': []\n",
    "    }\n",
    "    \n",
    "    for curr_seed in all_seeds:\n",
    "        for model_name in auROC_auROC_results: \n",
    "            f = open(\"general_results/%s/%s_%s_auROC.txt\" % (str(curr_seed), model_name, str(seq_len)))\n",
    "            lines = f.readlines()  \n",
    "            auROC_auROC_results[model_name].append(float(lines[0][7:-1]))\n",
    "            auROC_auPRC_results[model_name].append(float(lines[1][7:-1]))\n",
    "            f.close()\n",
    "            \n",
    "            f = open(\"general_results/%s/%s_%s_loss.txt\" % (str(curr_seed), model_name, str(seq_len)))\n",
    "            lines = f.readlines()  \n",
    "            loss_auROC_results[model_name].append(float(lines[0][7:-1]))\n",
    "            loss_auPRC_results[model_name].append(float(lines[1][7:-1]))\n",
    "            f.close()\n",
    "\n",
    "    return auROC_auROC_result, auROC_auPRC_results, loss_auROC_results, loss_auPRC_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stacked_graphs(result_1, result_2, y_min, y_max, metric, seq_len, save_model_name):\n",
    "    result1_means = []\n",
    "    result1_std = []\n",
    "    \n",
    "    result2_means = []\n",
    "    result2_std = []\n",
    "    \n",
    "    N = len(result_1)/2\n",
    "    for model_name in result_1: \n",
    "        result1_means.append(np.mean(result1_means[model_name]))\n",
    "        result1_std.append(np.std(result1_means[model_name]))\n",
    "    for model_name in result_2: \n",
    "        result2_means.append(np.mean(result2_means[model_name]))\n",
    "        result2_std.append(np.std(result2_means[model_name]))\n",
    "\n",
    "    ind = np.arange(N)\n",
    "    width = 0.32\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(14, 8)\n",
    "    plt.ylim([y_min,y_max])\n",
    "    rects1 = ax.bar(ind, result1_means, width, color='r', yerr = result1_std, capsize = 5)\n",
    "    rects2 = ax.bar(ind + width, result2_means, width, color='g', yerr = result2_std, capsize = 5)\n",
    "\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_xlabel('Model Arch')\n",
    "    ax.set_title('Median %s %s' % (metric, str(seq_len))\n",
    "    ax.set_xticks(ind + width / 2)\n",
    "    ax.set_xticklabels(('RC', 'Siamese', 'Aug', 'Aug_Alt', 'Aug_Alt_double', 'Standard'))\n",
    "    \n",
    "    ax.legend((rects1[0], rects2[0]), ('auROC', 'loss'))\n",
    "    plt.show\n",
    "    if save_model_name != None:\n",
    "        plt.savefig(save_model_name + \".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auROC_auROC_result_200, auROC_auPRC_results_200, loss_auROC_results_200, loss_auPRC_results_200 = get_results_from_file(all_seeds = all_seeds, seq_len = 200)\n",
    "auROC_auROC_result_1000, auROC_auPRC_results_1000, loss_auROC_results_1000, loss_auPRC_results_1000 = get_results_from_file(all_seeds = all_seeds, seq_len = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stacked_graphs(result_1 = auROC_auROC_result_200, result_2 = loss_auROC_results_200, y_min = 0.9, y_max = 1, metric = \"auROC\", seq_len = 200, save_model_name = None)\n",
    "get_stacked_graphs(result_1 = auROC_auPRC_result_200, result_2 = loss_auPRC_results_200, y_min = 0.9, y_max = 1, metric = \"auPRC\", seq_len = 200, save_model_name = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stacked_graphs(result_1 = auROC_auROC_result_1000, result_2 = loss_auROC_results_1000, y_min = 0.9, y_max = 1, metric = \"auROC\", seq_len = 1000, save_model_name = None)\n",
    "get_stacked_graphs(result_1 = auROC_auPRC_result_1000, result_2 = loss_auPRC_results_1000, y_min = 0.9, y_max = 1, metric = \"auPRC\", seq_len = 1000, save_model_name = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 4s 212us/step - loss: 0.7711 - acc: 0.5954 - val_loss: 0.6925 - val_acc: 0.6459\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.6747 - acc: 0.6305 - val_loss: 0.6779 - val_acc: 0.6221\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.6618 - acc: 0.6371 - val_loss: 0.6738 - val_acc: 0.6421\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.6512 - acc: 0.6447 - val_loss: 0.6706 - val_acc: 0.6253\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6407 - acc: 0.6520 - val_loss: 0.6622 - val_acc: 0.6323\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 1s 61us/step - loss: 0.6312 - acc: 0.6581 - val_loss: 0.6534 - val_acc: 0.6454\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.6205 - acc: 0.6659 - val_loss: 0.6460 - val_acc: 0.6534\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9af31ca0375a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                                  \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                                  \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                                  val_data = (x_val_1000, y_val_1000))\n\u001b[0m\u001b[1;32m      9\u001b[0m     save_all(filepath = \"general_results\", model_arch = \"test_RC_1000_auROC\", curr_seed = curr_seed,\n\u001b[1;32m     10\u001b[0m              callback = auroc_callback, model = model, val_data = (x_val_1000, y_val_1000))\n",
      "\u001b[0;32m~/revcomp_simulated/train_models.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model_wrapper, aug, curr_seed, batch_size, x, y, val_data)\u001b[0m\n\u001b[1;32m    138\u001b[0m     model.fit(x = x_train, y = y_train, validation_data = val_data,  \n\u001b[1;32m    139\u001b[0m               \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauroc_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m               batch_size=batch_size, epochs=200)\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauroc_callback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m                             \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/revcomp_simulated/train_models.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_epoch_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0mauroc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mauroc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_auroc_sofar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mins\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m                 \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/tf1.15.2/lib/python3.7/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for curr_seed in all_seeds: \n",
    "    model, early_stopping_callback, auroc_callback = train_model(model_wrapper = RC_WRAPPER, \n",
    "                                                                 aug = None, \n",
    "                                                                 curr_seed = curr_seed, \n",
    "                                                                 batch_size = 500,\n",
    "                                                                 x = x_train_1000, \n",
    "                                                                 y = y_train_1000, \n",
    "                                                                 val_data = (x_val_1000, y_val_1000))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"test_RC_1000_auROC\", curr_seed = curr_seed,\n",
    "             callback = auroc_callback, model = model, val_data = (x_val_1000, y_val_1000))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"test_RC_1000_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_test_1000, y_test_1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 2s 136us/step - loss: 0.7711 - acc: 0.5954 - val_loss: 0.6925 - val_acc: 0.6459\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.6747 - acc: 0.6304 - val_loss: 0.6779 - val_acc: 0.6221\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.6618 - acc: 0.6369 - val_loss: 0.6738 - val_acc: 0.6424\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6513 - acc: 0.6448 - val_loss: 0.6706 - val_acc: 0.6247\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6407 - acc: 0.6518 - val_loss: 0.6622 - val_acc: 0.6330\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6312 - acc: 0.6576 - val_loss: 0.6534 - val_acc: 0.6453\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6206 - acc: 0.6653 - val_loss: 0.6459 - val_acc: 0.6535\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6135 - acc: 0.6722 - val_loss: 0.6405 - val_acc: 0.6558\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6049 - acc: 0.6773 - val_loss: 0.6312 - val_acc: 0.6603\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5938 - acc: 0.6873 - val_loss: 0.6227 - val_acc: 0.6637\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5877 - acc: 0.6913 - val_loss: 0.6226 - val_acc: 0.6725\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5814 - acc: 0.6959 - val_loss: 0.6178 - val_acc: 0.6624\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5724 - acc: 0.7019 - val_loss: 0.6073 - val_acc: 0.6817\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.5644 - acc: 0.7080 - val_loss: 0.5980 - val_acc: 0.6821\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5571 - acc: 0.7126 - val_loss: 0.5917 - val_acc: 0.6905\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5504 - acc: 0.7186 - val_loss: 0.5928 - val_acc: 0.6801\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5446 - acc: 0.7214 - val_loss: 0.5740 - val_acc: 0.7021\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5390 - acc: 0.7275 - val_loss: 0.5703 - val_acc: 0.7004\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5278 - acc: 0.7346 - val_loss: 0.5624 - val_acc: 0.7038\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.5236 - acc: 0.7378 - val_loss: 0.5709 - val_acc: 0.7018\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5198 - acc: 0.7414 - val_loss: 0.5504 - val_acc: 0.7139\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5082 - acc: 0.7511 - val_loss: 0.5474 - val_acc: 0.7172\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5058 - acc: 0.7504 - val_loss: 0.5420 - val_acc: 0.7243\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4965 - acc: 0.7577 - val_loss: 0.5383 - val_acc: 0.7201\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.4933 - acc: 0.7604 - val_loss: 0.5305 - val_acc: 0.7290\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4893 - acc: 0.7625 - val_loss: 0.5316 - val_acc: 0.7311\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4819 - acc: 0.7677 - val_loss: 0.5284 - val_acc: 0.7344\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4803 - acc: 0.7691 - val_loss: 0.5223 - val_acc: 0.7342\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4754 - acc: 0.7720 - val_loss: 0.5167 - val_acc: 0.7407\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4738 - acc: 0.7726 - val_loss: 0.5100 - val_acc: 0.7452\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4675 - acc: 0.7772 - val_loss: 0.5162 - val_acc: 0.7422\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4638 - acc: 0.7796 - val_loss: 0.5065 - val_acc: 0.7469\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.4566 - acc: 0.7857 - val_loss: 0.5191 - val_acc: 0.7375\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 1s 58us/step - loss: 0.4596 - acc: 0.7814 - val_loss: 0.5045 - val_acc: 0.7473\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 1s 58us/step - loss: 0.4563 - acc: 0.7843 - val_loss: 0.5037 - val_acc: 0.7478\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 1s 58us/step - loss: 0.4520 - acc: 0.7875 - val_loss: 0.4943 - val_acc: 0.7551\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 1s 58us/step - loss: 0.4460 - acc: 0.7928 - val_loss: 0.5115 - val_acc: 0.7464\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.4414 - acc: 0.7937 - val_loss: 0.4912 - val_acc: 0.7582\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4410 - acc: 0.7930 - val_loss: 0.4892 - val_acc: 0.7591\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4385 - acc: 0.7967 - val_loss: 0.5013 - val_acc: 0.7539\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4352 - acc: 0.7969 - val_loss: 0.4992 - val_acc: 0.7516\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4313 - acc: 0.7998 - val_loss: 0.4811 - val_acc: 0.7643\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4281 - acc: 0.8024 - val_loss: 0.4824 - val_acc: 0.7639\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4296 - acc: 0.8000 - val_loss: 0.4779 - val_acc: 0.7665\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4221 - acc: 0.8055 - val_loss: 0.4752 - val_acc: 0.7694\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4281 - acc: 0.8008 - val_loss: 0.4907 - val_acc: 0.7577\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4211 - acc: 0.8057 - val_loss: 0.4867 - val_acc: 0.7622\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4167 - acc: 0.8092 - val_loss: 0.4707 - val_acc: 0.7723\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4144 - acc: 0.8111 - val_loss: 0.4724 - val_acc: 0.7714\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4153 - acc: 0.8100 - val_loss: 0.4715 - val_acc: 0.7711\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4103 - acc: 0.8137 - val_loss: 0.4775 - val_acc: 0.7674\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4113 - acc: 0.8116 - val_loss: 0.4733 - val_acc: 0.7712\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4077 - acc: 0.8146 - val_loss: 0.4730 - val_acc: 0.7706\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4047 - acc: 0.8175 - val_loss: 0.4647 - val_acc: 0.7775\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.4026 - acc: 0.8184 - val_loss: 0.4751 - val_acc: 0.7693\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4031 - acc: 0.8181 - val_loss: 0.4599 - val_acc: 0.7801\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3994 - acc: 0.8207 - val_loss: 0.4622 - val_acc: 0.7779\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4011 - acc: 0.8182 - val_loss: 0.4887 - val_acc: 0.7648\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3981 - acc: 0.8202 - val_loss: 0.4655 - val_acc: 0.7760\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3973 - acc: 0.8219 - val_loss: 0.4623 - val_acc: 0.7788\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3941 - acc: 0.8226 - val_loss: 0.4568 - val_acc: 0.7817\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3958 - acc: 0.8220 - val_loss: 0.4562 - val_acc: 0.7825\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3934 - acc: 0.8250 - val_loss: 0.4535 - val_acc: 0.7842\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3931 - acc: 0.8224 - val_loss: 0.4685 - val_acc: 0.7725\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3907 - acc: 0.8248 - val_loss: 0.4533 - val_acc: 0.7843\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3859 - acc: 0.8288 - val_loss: 0.4517 - val_acc: 0.7858\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3865 - acc: 0.8272 - val_loss: 0.4641 - val_acc: 0.7778\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.3855 - acc: 0.8278 - val_loss: 0.4525 - val_acc: 0.7841\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3847 - acc: 0.8285 - val_loss: 0.4503 - val_acc: 0.7856\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3807 - acc: 0.8326 - val_loss: 0.4517 - val_acc: 0.7850\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3849 - acc: 0.8293 - val_loss: 0.4486 - val_acc: 0.7890\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3787 - acc: 0.8322 - val_loss: 0.4477 - val_acc: 0.7890\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3779 - acc: 0.8327 - val_loss: 0.4488 - val_acc: 0.7879\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3775 - acc: 0.8318 - val_loss: 0.4620 - val_acc: 0.7809\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3801 - acc: 0.8297 - val_loss: 0.4652 - val_acc: 0.7778\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.3800 - acc: 0.8309 - val_loss: 0.4565 - val_acc: 0.7825\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 1s 58us/step - loss: 0.3755 - acc: 0.8330 - val_loss: 0.4580 - val_acc: 0.7823\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 1s 58us/step - loss: 0.3752 - acc: 0.8335 - val_loss: 0.4729 - val_acc: 0.7710\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3758 - acc: 0.8338 - val_loss: 0.4637 - val_acc: 0.7780\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.3748 - acc: 0.8330 - val_loss: 0.4592 - val_acc: 0.7820\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3741 - acc: 0.8344 - val_loss: 0.4451 - val_acc: 0.7906\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3682 - acc: 0.8390 - val_loss: 0.4470 - val_acc: 0.7877\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3686 - acc: 0.8375 - val_loss: 0.4474 - val_acc: 0.7880\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.3705 - acc: 0.8356 - val_loss: 0.4596 - val_acc: 0.7806\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3689 - acc: 0.8389 - val_loss: 0.4528 - val_acc: 0.7853\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.3685 - acc: 0.8386 - val_loss: 0.4449 - val_acc: 0.7884\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3645 - acc: 0.8412 - val_loss: 0.4425 - val_acc: 0.7920\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3687 - acc: 0.8370 - val_loss: 0.4551 - val_acc: 0.7849\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3640 - acc: 0.8398 - val_loss: 0.4511 - val_acc: 0.7870\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3695 - acc: 0.8354 - val_loss: 0.4576 - val_acc: 0.7812\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3621 - acc: 0.8413 - val_loss: 0.4517 - val_acc: 0.7860\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3618 - acc: 0.8411 - val_loss: 0.4493 - val_acc: 0.7881\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3653 - acc: 0.8389 - val_loss: 0.4521 - val_acc: 0.7853\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3611 - acc: 0.8415 - val_loss: 0.4430 - val_acc: 0.7931\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3651 - acc: 0.8376 - val_loss: 0.4434 - val_acc: 0.7918\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.3605 - acc: 0.8416 - val_loss: 0.4461 - val_acc: 0.7919\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3620 - acc: 0.8406 - val_loss: 0.4452 - val_acc: 0.7927\n",
      "auroc: 0.8457531028433656\n",
      "auprc: 0.7527655645678001\n",
      "auroc: 0.8436786388913239\n",
      "auprc: 0.7494199329946928\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 3s 89us/step - loss: 0.7403 - acc: 0.6046 - val_loss: 0.6777 - val_acc: 0.6195\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.6639 - acc: 0.6362 - val_loss: 0.6644 - val_acc: 0.6421\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6465 - acc: 0.6481 - val_loss: 0.6508 - val_acc: 0.6460\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6326 - acc: 0.6581 - val_loss: 0.6404 - val_acc: 0.6475\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6155 - acc: 0.6700 - val_loss: 0.6226 - val_acc: 0.6658\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.6006 - acc: 0.6807 - val_loss: 0.6099 - val_acc: 0.6738\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5856 - acc: 0.6922 - val_loss: 0.5918 - val_acc: 0.6865\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5709 - acc: 0.7031 - val_loss: 0.5780 - val_acc: 0.6911\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5555 - acc: 0.7153 - val_loss: 0.5617 - val_acc: 0.7061\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5403 - acc: 0.7253 - val_loss: 0.5453 - val_acc: 0.7215\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5285 - acc: 0.7344 - val_loss: 0.5330 - val_acc: 0.7310\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5173 - acc: 0.7419 - val_loss: 0.5223 - val_acc: 0.7389\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5064 - acc: 0.7493 - val_loss: 0.5169 - val_acc: 0.7436\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4934 - acc: 0.7574 - val_loss: 0.5036 - val_acc: 0.7483\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4831 - acc: 0.7656 - val_loss: 0.4948 - val_acc: 0.7560\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4766 - acc: 0.7696 - val_loss: 0.4901 - val_acc: 0.7593\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4676 - acc: 0.7758 - val_loss: 0.4824 - val_acc: 0.7626\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4642 - acc: 0.7779 - val_loss: 0.4739 - val_acc: 0.7698\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4533 - acc: 0.7863 - val_loss: 0.4697 - val_acc: 0.7699\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4524 - acc: 0.7854 - val_loss: 0.4636 - val_acc: 0.7758\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4453 - acc: 0.7921 - val_loss: 0.4602 - val_acc: 0.7778\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4400 - acc: 0.7945 - val_loss: 0.4645 - val_acc: 0.7768\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4378 - acc: 0.7945 - val_loss: 0.4612 - val_acc: 0.7768\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4345 - acc: 0.7976 - val_loss: 0.4673 - val_acc: 0.7701\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4301 - acc: 0.7999 - val_loss: 0.4510 - val_acc: 0.7848\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4251 - acc: 0.8032 - val_loss: 0.4439 - val_acc: 0.7880\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4203 - acc: 0.8057 - val_loss: 0.4436 - val_acc: 0.7890\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4189 - acc: 0.8053 - val_loss: 0.4412 - val_acc: 0.7905\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4157 - acc: 0.8074 - val_loss: 0.4365 - val_acc: 0.7933\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4140 - acc: 0.8095 - val_loss: 0.4391 - val_acc: 0.7930\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4121 - acc: 0.8096 - val_loss: 0.4359 - val_acc: 0.7931\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4082 - acc: 0.8130 - val_loss: 0.4296 - val_acc: 0.7991\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4062 - acc: 0.8131 - val_loss: 0.4384 - val_acc: 0.7928\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4036 - acc: 0.8146 - val_loss: 0.4286 - val_acc: 0.7992\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4031 - acc: 0.8152 - val_loss: 0.4256 - val_acc: 0.8003\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4009 - acc: 0.8168 - val_loss: 0.4241 - val_acc: 0.8026\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3970 - acc: 0.8187 - val_loss: 0.4295 - val_acc: 0.7986\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3949 - acc: 0.8196 - val_loss: 0.4210 - val_acc: 0.8038\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3944 - acc: 0.8195 - val_loss: 0.4244 - val_acc: 0.8020\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3948 - acc: 0.8199 - val_loss: 0.4199 - val_acc: 0.8048\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3898 - acc: 0.8227 - val_loss: 0.4232 - val_acc: 0.8024\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3894 - acc: 0.8236 - val_loss: 0.4170 - val_acc: 0.8068\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3855 - acc: 0.8257 - val_loss: 0.4175 - val_acc: 0.8061\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3845 - acc: 0.8257 - val_loss: 0.4176 - val_acc: 0.8061\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3834 - acc: 0.8274 - val_loss: 0.4237 - val_acc: 0.8014\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3836 - acc: 0.8257 - val_loss: 0.4196 - val_acc: 0.8030\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3807 - acc: 0.8290 - val_loss: 0.4273 - val_acc: 0.7982\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3814 - acc: 0.8269 - val_loss: 0.4119 - val_acc: 0.8094\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3836 - acc: 0.8253 - val_loss: 0.4151 - val_acc: 0.8083\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3800 - acc: 0.8285 - val_loss: 0.4101 - val_acc: 0.8109\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3778 - acc: 0.8303 - val_loss: 0.4088 - val_acc: 0.8121\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3758 - acc: 0.8306 - val_loss: 0.4114 - val_acc: 0.8091\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3732 - acc: 0.8330 - val_loss: 0.4173 - val_acc: 0.8046\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3711 - acc: 0.8332 - val_loss: 0.4113 - val_acc: 0.8097\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3706 - acc: 0.8350 - val_loss: 0.4099 - val_acc: 0.8102\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3714 - acc: 0.8331 - val_loss: 0.4147 - val_acc: 0.8064\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3704 - acc: 0.8339 - val_loss: 0.4067 - val_acc: 0.8115\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3713 - acc: 0.8326 - val_loss: 0.4197 - val_acc: 0.8040\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3669 - acc: 0.8357 - val_loss: 0.4064 - val_acc: 0.8124\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3678 - acc: 0.8347 - val_loss: 0.4144 - val_acc: 0.8066\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3650 - acc: 0.8366 - val_loss: 0.4025 - val_acc: 0.8134\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3674 - acc: 0.8356 - val_loss: 0.4160 - val_acc: 0.8072\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3656 - acc: 0.8359 - val_loss: 0.4080 - val_acc: 0.8094\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3651 - acc: 0.8355 - val_loss: 0.4238 - val_acc: 0.8018\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3647 - acc: 0.8364 - val_loss: 0.4017 - val_acc: 0.8144\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3675 - acc: 0.8340 - val_loss: 0.4009 - val_acc: 0.8134\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3603 - acc: 0.8398 - val_loss: 0.4087 - val_acc: 0.8092\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3599 - acc: 0.8405 - val_loss: 0.4046 - val_acc: 0.8118\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3581 - acc: 0.8410 - val_loss: 0.4006 - val_acc: 0.8146\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3619 - acc: 0.8371 - val_loss: 0.4014 - val_acc: 0.8141\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3595 - acc: 0.8388 - val_loss: 0.4079 - val_acc: 0.8086\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3593 - acc: 0.8389 - val_loss: 0.4111 - val_acc: 0.8080\n",
      "Epoch 73/200\n",
      " 9500/34284 [=======>......................] - ETA: 0s - loss: 0.3486 - acc: 0.8434"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3761 - acc: 0.8306 - val_loss: 0.4198 - val_acc: 0.8042\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3736 - acc: 0.8329 - val_loss: 0.4170 - val_acc: 0.8055\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3744 - acc: 0.8315 - val_loss: 0.4227 - val_acc: 0.8024\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3738 - acc: 0.8327 - val_loss: 0.4218 - val_acc: 0.8016\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3740 - acc: 0.8324 - val_loss: 0.4180 - val_acc: 0.8060\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3742 - acc: 0.8311 - val_loss: 0.4179 - val_acc: 0.8035\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3713 - acc: 0.8334 - val_loss: 0.4193 - val_acc: 0.8038\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3727 - acc: 0.8314 - val_loss: 0.4252 - val_acc: 0.7996\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3707 - acc: 0.8337 - val_loss: 0.4247 - val_acc: 0.7999\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3716 - acc: 0.8323 - val_loss: 0.4172 - val_acc: 0.8034\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3721 - acc: 0.8319 - val_loss: 0.4239 - val_acc: 0.8034\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3701 - acc: 0.8339 - val_loss: 0.4242 - val_acc: 0.8011\n",
      "auroc: 0.8653308667965959\n",
      "auprc: 0.7813335940403686\n",
      "auroc: 0.8627359541667016\n",
      "auprc: 0.7768105928746754\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 4s 103us/step - loss: 0.7068 - acc: 0.6158 - val_loss: 0.6809 - val_acc: 0.6339\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6594 - acc: 0.6389 - val_loss: 0.6709 - val_acc: 0.6219\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6443 - acc: 0.6490 - val_loss: 0.6690 - val_acc: 0.6366\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6282 - acc: 0.6597 - val_loss: 0.6493 - val_acc: 0.6392\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6067 - acc: 0.6758 - val_loss: 0.6236 - val_acc: 0.6692\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5846 - acc: 0.6946 - val_loss: 0.5996 - val_acc: 0.6834\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5605 - acc: 0.7125 - val_loss: 0.5791 - val_acc: 0.7008\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5427 - acc: 0.7245 - val_loss: 0.5643 - val_acc: 0.7114\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5240 - acc: 0.7380 - val_loss: 0.5488 - val_acc: 0.7179\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.5094 - acc: 0.7482 - val_loss: 0.5353 - val_acc: 0.7271\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4984 - acc: 0.7564 - val_loss: 0.5269 - val_acc: 0.7301\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4896 - acc: 0.7595 - val_loss: 0.5197 - val_acc: 0.7404\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4786 - acc: 0.7677 - val_loss: 0.5107 - val_acc: 0.7446\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4704 - acc: 0.7726 - val_loss: 0.5000 - val_acc: 0.7509\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4614 - acc: 0.7798 - val_loss: 0.5058 - val_acc: 0.7473\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4549 - acc: 0.7851 - val_loss: 0.4955 - val_acc: 0.7536\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4466 - acc: 0.7896 - val_loss: 0.4886 - val_acc: 0.7591\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4395 - acc: 0.7959 - val_loss: 0.4790 - val_acc: 0.7664\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4374 - acc: 0.7967 - val_loss: 0.4784 - val_acc: 0.7663\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4287 - acc: 0.8032 - val_loss: 0.4743 - val_acc: 0.7697\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4284 - acc: 0.8011 - val_loss: 0.4713 - val_acc: 0.7721\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4208 - acc: 0.8079 - val_loss: 0.4667 - val_acc: 0.7741\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4170 - acc: 0.8099 - val_loss: 0.4676 - val_acc: 0.7740\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4127 - acc: 0.8126 - val_loss: 0.4680 - val_acc: 0.7740\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4100 - acc: 0.8131 - val_loss: 0.4631 - val_acc: 0.7768\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4065 - acc: 0.8159 - val_loss: 0.4608 - val_acc: 0.7782\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4024 - acc: 0.8183 - val_loss: 0.4585 - val_acc: 0.7807\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.4009 - acc: 0.8189 - val_loss: 0.4574 - val_acc: 0.7806\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3988 - acc: 0.8194 - val_loss: 0.4583 - val_acc: 0.7811\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3919 - acc: 0.8252 - val_loss: 0.4603 - val_acc: 0.7786\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3918 - acc: 0.8243 - val_loss: 0.4539 - val_acc: 0.7820\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3920 - acc: 0.8234 - val_loss: 0.4532 - val_acc: 0.7834\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3877 - acc: 0.8261 - val_loss: 0.4552 - val_acc: 0.7823\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3846 - acc: 0.8287 - val_loss: 0.4574 - val_acc: 0.7817\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3867 - acc: 0.8256 - val_loss: 0.4502 - val_acc: 0.7848\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3806 - acc: 0.8294 - val_loss: 0.4496 - val_acc: 0.7853\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3774 - acc: 0.8308 - val_loss: 0.4714 - val_acc: 0.7748\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3817 - acc: 0.8285 - val_loss: 0.4697 - val_acc: 0.7748\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3742 - acc: 0.8344 - val_loss: 0.4475 - val_acc: 0.7851\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3724 - acc: 0.8349 - val_loss: 0.4504 - val_acc: 0.7852\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3749 - acc: 0.8320 - val_loss: 0.4478 - val_acc: 0.7848\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3687 - acc: 0.8362 - val_loss: 0.4549 - val_acc: 0.7825\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3712 - acc: 0.8345 - val_loss: 0.4490 - val_acc: 0.7862\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3664 - acc: 0.8378 - val_loss: 0.4491 - val_acc: 0.7840\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3675 - acc: 0.8367 - val_loss: 0.4560 - val_acc: 0.7830\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3660 - acc: 0.8375 - val_loss: 0.4500 - val_acc: 0.7870\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3624 - acc: 0.8403 - val_loss: 0.4527 - val_acc: 0.7811\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3646 - acc: 0.8384 - val_loss: 0.4471 - val_acc: 0.7861\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3629 - acc: 0.8390 - val_loss: 0.4503 - val_acc: 0.7851\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3614 - acc: 0.8392 - val_loss: 0.4547 - val_acc: 0.7820\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3578 - acc: 0.8423 - val_loss: 0.4561 - val_acc: 0.7832\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3568 - acc: 0.8427 - val_loss: 0.4479 - val_acc: 0.7857\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3551 - acc: 0.8439 - val_loss: 0.4468 - val_acc: 0.7882\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3544 - acc: 0.8439 - val_loss: 0.4483 - val_acc: 0.7894\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3545 - acc: 0.8433 - val_loss: 0.4488 - val_acc: 0.7877\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3536 - acc: 0.8446 - val_loss: 0.4484 - val_acc: 0.7871\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3511 - acc: 0.8455 - val_loss: 0.4462 - val_acc: 0.7894\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3525 - acc: 0.8446 - val_loss: 0.4762 - val_acc: 0.7752\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3534 - acc: 0.8444 - val_loss: 0.4512 - val_acc: 0.7862\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3515 - acc: 0.8443 - val_loss: 0.4515 - val_acc: 0.7862\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3486 - acc: 0.8468 - val_loss: 0.4593 - val_acc: 0.7851\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3493 - acc: 0.8461 - val_loss: 0.4547 - val_acc: 0.7867\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3468 - acc: 0.8475 - val_loss: 0.4464 - val_acc: 0.7905\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3447 - acc: 0.8485 - val_loss: 0.4496 - val_acc: 0.7898\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3446 - acc: 0.8488 - val_loss: 0.4495 - val_acc: 0.7872\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3443 - acc: 0.8483 - val_loss: 0.4488 - val_acc: 0.7877\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.3470 - acc: 0.8470 - val_loss: 0.4474 - val_acc: 0.7901\n",
      "auroc: 0.8426317670575091\n",
      "auprc: 0.7482134063104846\n",
      "auroc: 0.8417446196406623\n",
      "auprc: 0.7462630708496722\n",
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 3s 176us/step - loss: 0.7339 - acc: 0.6041 - val_loss: 0.6870 - val_acc: 0.6179\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6731 - acc: 0.6317 - val_loss: 0.6794 - val_acc: 0.6281\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6592 - acc: 0.6410 - val_loss: 0.6735 - val_acc: 0.6259\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6514 - acc: 0.6456 - val_loss: 0.6883 - val_acc: 0.6170\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6447 - acc: 0.6499 - val_loss: 0.6644 - val_acc: 0.6445\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6367 - acc: 0.6549 - val_loss: 0.6595 - val_acc: 0.6314\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6284 - acc: 0.6622 - val_loss: 0.6595 - val_acc: 0.6467\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6237 - acc: 0.6640 - val_loss: 0.6415 - val_acc: 0.6558\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6084 - acc: 0.6741 - val_loss: 0.6445 - val_acc: 0.6481\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5985 - acc: 0.6846 - val_loss: 0.6284 - val_acc: 0.6670\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5881 - acc: 0.6899 - val_loss: 0.6124 - val_acc: 0.6780\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5799 - acc: 0.6974 - val_loss: 0.6069 - val_acc: 0.6840\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5693 - acc: 0.7046 - val_loss: 0.5924 - val_acc: 0.6832\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5579 - acc: 0.7127 - val_loss: 0.5839 - val_acc: 0.6972\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5463 - acc: 0.7224 - val_loss: 0.5807 - val_acc: 0.7029\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5385 - acc: 0.7277 - val_loss: 0.5712 - val_acc: 0.7090\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5311 - acc: 0.7334 - val_loss: 0.5602 - val_acc: 0.7136\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5250 - acc: 0.7370 - val_loss: 0.5689 - val_acc: 0.7018\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5183 - acc: 0.7421 - val_loss: 0.5453 - val_acc: 0.7188\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5125 - acc: 0.7462 - val_loss: 0.5540 - val_acc: 0.7193\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5072 - acc: 0.7491 - val_loss: 0.5368 - val_acc: 0.7269\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5006 - acc: 0.7540 - val_loss: 0.5303 - val_acc: 0.7278\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4975 - acc: 0.7554 - val_loss: 0.5277 - val_acc: 0.7315\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4912 - acc: 0.7610 - val_loss: 0.5235 - val_acc: 0.7352\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4895 - acc: 0.7621 - val_loss: 0.5376 - val_acc: 0.7326\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4910 - acc: 0.7597 - val_loss: 0.5159 - val_acc: 0.7379\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4799 - acc: 0.7672 - val_loss: 0.5141 - val_acc: 0.7387\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4740 - acc: 0.7726 - val_loss: 0.5207 - val_acc: 0.7361\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4723 - acc: 0.7735 - val_loss: 0.5104 - val_acc: 0.7431\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4690 - acc: 0.7741 - val_loss: 0.5175 - val_acc: 0.7368\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4694 - acc: 0.7753 - val_loss: 0.5057 - val_acc: 0.7461\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4628 - acc: 0.7791 - val_loss: 0.5134 - val_acc: 0.7410\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4614 - acc: 0.7796 - val_loss: 0.5011 - val_acc: 0.7482\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4626 - acc: 0.7795 - val_loss: 0.5072 - val_acc: 0.7417\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4615 - acc: 0.7800 - val_loss: 0.4972 - val_acc: 0.7513\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4543 - acc: 0.7847 - val_loss: 0.4953 - val_acc: 0.7521\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4496 - acc: 0.7880 - val_loss: 0.4969 - val_acc: 0.7520\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4490 - acc: 0.7870 - val_loss: 0.4895 - val_acc: 0.7563\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4456 - acc: 0.7914 - val_loss: 0.4950 - val_acc: 0.7524\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4421 - acc: 0.7916 - val_loss: 0.4862 - val_acc: 0.7588\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.4431 - acc: 0.7910 - val_loss: 0.4852 - val_acc: 0.7597\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4406 - acc: 0.7920 - val_loss: 0.4899 - val_acc: 0.7552\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4436 - acc: 0.7903 - val_loss: 0.4906 - val_acc: 0.7564\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4406 - acc: 0.7915 - val_loss: 0.4997 - val_acc: 0.7533\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4330 - acc: 0.7992 - val_loss: 0.4819 - val_acc: 0.7623\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4312 - acc: 0.7990 - val_loss: 0.4847 - val_acc: 0.7606\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4292 - acc: 0.8011 - val_loss: 0.4759 - val_acc: 0.7660\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4257 - acc: 0.8036 - val_loss: 0.4818 - val_acc: 0.7632\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4236 - acc: 0.8052 - val_loss: 0.4739 - val_acc: 0.7675\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4213 - acc: 0.8067 - val_loss: 0.4856 - val_acc: 0.7608\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4269 - acc: 0.8013 - val_loss: 0.4765 - val_acc: 0.7661\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4197 - acc: 0.8071 - val_loss: 0.4711 - val_acc: 0.7689\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4193 - acc: 0.8068 - val_loss: 0.4720 - val_acc: 0.7690\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4152 - acc: 0.8092 - val_loss: 0.4694 - val_acc: 0.7702\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4129 - acc: 0.8109 - val_loss: 0.4673 - val_acc: 0.7715\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4145 - acc: 0.8100 - val_loss: 0.4667 - val_acc: 0.7715\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4106 - acc: 0.8119 - val_loss: 0.4673 - val_acc: 0.7711\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4099 - acc: 0.8136 - val_loss: 0.4776 - val_acc: 0.7652\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4100 - acc: 0.8123 - val_loss: 0.4674 - val_acc: 0.7718\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4052 - acc: 0.8148 - val_loss: 0.4722 - val_acc: 0.7687\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4082 - acc: 0.8136 - val_loss: 0.4632 - val_acc: 0.7737\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4035 - acc: 0.8165 - val_loss: 0.4634 - val_acc: 0.7743\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.4072 - acc: 0.8122 - val_loss: 0.4736 - val_acc: 0.7684\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 1s 58us/step - loss: 0.4016 - acc: 0.8170 - val_loss: 0.4693 - val_acc: 0.7716\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 1s 58us/step - loss: 0.3996 - acc: 0.8193 - val_loss: 0.4612 - val_acc: 0.7757\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.3975 - acc: 0.8207 - val_loss: 0.4680 - val_acc: 0.7703\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.3971 - acc: 0.8198 - val_loss: 0.4671 - val_acc: 0.7723\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3953 - acc: 0.8221 - val_loss: 0.4580 - val_acc: 0.7775\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3950 - acc: 0.8213 - val_loss: 0.4675 - val_acc: 0.7738\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3960 - acc: 0.8202 - val_loss: 0.4651 - val_acc: 0.7731\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3910 - acc: 0.8239 - val_loss: 0.4598 - val_acc: 0.7772\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3913 - acc: 0.8233 - val_loss: 0.4559 - val_acc: 0.7803\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3889 - acc: 0.8249 - val_loss: 0.4621 - val_acc: 0.7763\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.3896 - acc: 0.8246 - val_loss: 0.4611 - val_acc: 0.7769\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3905 - acc: 0.8232 - val_loss: 0.4694 - val_acc: 0.7715\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3874 - acc: 0.8258 - val_loss: 0.4645 - val_acc: 0.7744\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3851 - acc: 0.8278 - val_loss: 0.4582 - val_acc: 0.7785\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3882 - acc: 0.8255 - val_loss: 0.4586 - val_acc: 0.7780\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3850 - acc: 0.8269 - val_loss: 0.4668 - val_acc: 0.7738\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3838 - acc: 0.8286 - val_loss: 0.4565 - val_acc: 0.7798\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3859 - acc: 0.8262 - val_loss: 0.4619 - val_acc: 0.7772\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3833 - acc: 0.8272 - val_loss: 0.4539 - val_acc: 0.7829\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3783 - acc: 0.8322 - val_loss: 0.4611 - val_acc: 0.7774\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3775 - acc: 0.8318 - val_loss: 0.4517 - val_acc: 0.7833\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3770 - acc: 0.8321 - val_loss: 0.4522 - val_acc: 0.7839\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3745 - acc: 0.8339 - val_loss: 0.4529 - val_acc: 0.7820\n",
      "Epoch 87/200\n",
      " 3500/17142 [=====>........................] - ETA: 0s - loss: 0.3862 - acc: 0.8266"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5364 - acc: 0.7294 - val_loss: 0.5581 - val_acc: 0.7136\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5232 - acc: 0.7390 - val_loss: 0.5522 - val_acc: 0.7196\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5141 - acc: 0.7456 - val_loss: 0.5475 - val_acc: 0.7252\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5022 - acc: 0.7528 - val_loss: 0.5459 - val_acc: 0.7282\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4946 - acc: 0.7583 - val_loss: 0.5240 - val_acc: 0.7328\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4862 - acc: 0.7641 - val_loss: 0.5339 - val_acc: 0.7315\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4795 - acc: 0.7673 - val_loss: 0.5199 - val_acc: 0.7429\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4702 - acc: 0.7759 - val_loss: 0.5078 - val_acc: 0.7443\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4655 - acc: 0.7768 - val_loss: 0.5084 - val_acc: 0.7437\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4589 - acc: 0.7811 - val_loss: 0.5073 - val_acc: 0.7485\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4535 - acc: 0.7851 - val_loss: 0.4999 - val_acc: 0.7498\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4480 - acc: 0.7890 - val_loss: 0.5070 - val_acc: 0.7468\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4448 - acc: 0.7897 - val_loss: 0.4903 - val_acc: 0.7572\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4396 - acc: 0.7941 - val_loss: 0.4859 - val_acc: 0.7582\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4355 - acc: 0.7960 - val_loss: 0.4821 - val_acc: 0.7611\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4298 - acc: 0.8008 - val_loss: 0.4918 - val_acc: 0.7542\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4339 - acc: 0.7963 - val_loss: 0.4915 - val_acc: 0.7578\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4238 - acc: 0.8039 - val_loss: 0.4766 - val_acc: 0.7642\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4203 - acc: 0.8051 - val_loss: 0.4772 - val_acc: 0.7633\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4181 - acc: 0.8074 - val_loss: 0.4706 - val_acc: 0.7688\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4147 - acc: 0.8098 - val_loss: 0.4736 - val_acc: 0.7688\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4102 - acc: 0.8123 - val_loss: 0.4738 - val_acc: 0.7667\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4054 - acc: 0.8161 - val_loss: 0.4738 - val_acc: 0.7690\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4037 - acc: 0.8152 - val_loss: 0.4717 - val_acc: 0.7704\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4018 - acc: 0.8174 - val_loss: 0.4630 - val_acc: 0.7734\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3990 - acc: 0.8191 - val_loss: 0.4693 - val_acc: 0.7704\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3960 - acc: 0.8203 - val_loss: 0.4629 - val_acc: 0.7740\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3936 - acc: 0.8222 - val_loss: 0.4607 - val_acc: 0.7761\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3907 - acc: 0.8243 - val_loss: 0.4604 - val_acc: 0.7758\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3873 - acc: 0.8263 - val_loss: 0.4677 - val_acc: 0.7719\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3869 - acc: 0.8266 - val_loss: 0.4591 - val_acc: 0.7770\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3838 - acc: 0.8280 - val_loss: 0.4644 - val_acc: 0.7738\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3827 - acc: 0.8288 - val_loss: 0.4638 - val_acc: 0.7757\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3790 - acc: 0.8315 - val_loss: 0.4602 - val_acc: 0.7777\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3767 - acc: 0.8324 - val_loss: 0.4590 - val_acc: 0.7785\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3765 - acc: 0.8319 - val_loss: 0.4564 - val_acc: 0.7797\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3730 - acc: 0.8351 - val_loss: 0.4541 - val_acc: 0.7797\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3731 - acc: 0.8335 - val_loss: 0.4567 - val_acc: 0.7799\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3695 - acc: 0.8364 - val_loss: 0.4532 - val_acc: 0.7807\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3677 - acc: 0.8377 - val_loss: 0.4652 - val_acc: 0.7752\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3685 - acc: 0.8361 - val_loss: 0.4680 - val_acc: 0.7754\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3654 - acc: 0.8384 - val_loss: 0.4590 - val_acc: 0.7786\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3633 - acc: 0.8392 - val_loss: 0.4568 - val_acc: 0.7797\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3635 - acc: 0.8392 - val_loss: 0.4560 - val_acc: 0.7825\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3603 - acc: 0.8417 - val_loss: 0.4529 - val_acc: 0.7840\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3579 - acc: 0.8429 - val_loss: 0.4512 - val_acc: 0.7848\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3591 - acc: 0.8413 - val_loss: 0.4550 - val_acc: 0.7818\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3570 - acc: 0.8432 - val_loss: 0.4543 - val_acc: 0.7848\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3575 - acc: 0.8417 - val_loss: 0.4667 - val_acc: 0.7773\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3529 - acc: 0.8451 - val_loss: 0.4617 - val_acc: 0.7812\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3537 - acc: 0.8442 - val_loss: 0.4562 - val_acc: 0.7833\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3527 - acc: 0.8445 - val_loss: 0.4523 - val_acc: 0.7830\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3515 - acc: 0.8453 - val_loss: 0.4627 - val_acc: 0.7805\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3478 - acc: 0.8476 - val_loss: 0.4540 - val_acc: 0.7862\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3492 - acc: 0.8458 - val_loss: 0.4577 - val_acc: 0.7810\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3525 - acc: 0.8441 - val_loss: 0.4599 - val_acc: 0.7801\n",
      "auroc: 0.83976624503827\n",
      "auprc: 0.7377014325760656\n",
      "auroc: 0.8380982925683483\n",
      "auprc: 0.7341992787555648\n",
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 3s 199us/step - loss: 0.7281 - acc: 0.6058 - val_loss: 0.6851 - val_acc: 0.6389\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6774 - acc: 0.6256 - val_loss: 0.6878 - val_acc: 0.6102\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6662 - acc: 0.6321 - val_loss: 0.6754 - val_acc: 0.6232\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6559 - acc: 0.6398 - val_loss: 0.6893 - val_acc: 0.6191\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6505 - acc: 0.6452 - val_loss: 0.6689 - val_acc: 0.6420\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.6430 - acc: 0.6492 - val_loss: 0.6655 - val_acc: 0.6270\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6378 - acc: 0.6534 - val_loss: 0.6588 - val_acc: 0.6387\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.6331 - acc: 0.6568 - val_loss: 0.6563 - val_acc: 0.6558\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6270 - acc: 0.6595 - val_loss: 0.6464 - val_acc: 0.6507\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6175 - acc: 0.6689 - val_loss: 0.6423 - val_acc: 0.6437\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6072 - acc: 0.6753 - val_loss: 0.6317 - val_acc: 0.6615\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5954 - acc: 0.6837 - val_loss: 0.6230 - val_acc: 0.6728\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.5870 - acc: 0.6898 - val_loss: 0.6259 - val_acc: 0.6710\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5822 - acc: 0.6959 - val_loss: 0.6085 - val_acc: 0.6849\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5719 - acc: 0.7017 - val_loss: 0.6031 - val_acc: 0.6774\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5627 - acc: 0.7114 - val_loss: 0.5868 - val_acc: 0.6929\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.5536 - acc: 0.7158 - val_loss: 0.5763 - val_acc: 0.6966\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5426 - acc: 0.7235 - val_loss: 0.5712 - val_acc: 0.7041\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.5368 - acc: 0.7279 - val_loss: 0.5675 - val_acc: 0.7062\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5290 - acc: 0.7320 - val_loss: 0.5566 - val_acc: 0.7115\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5255 - acc: 0.7329 - val_loss: 0.5520 - val_acc: 0.7138\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5155 - acc: 0.7403 - val_loss: 0.5471 - val_acc: 0.7171\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5098 - acc: 0.7441 - val_loss: 0.5411 - val_acc: 0.7228\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5049 - acc: 0.7485 - val_loss: 0.5355 - val_acc: 0.7237\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4990 - acc: 0.7527 - val_loss: 0.5301 - val_acc: 0.7295\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4927 - acc: 0.7577 - val_loss: 0.5267 - val_acc: 0.7296\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4899 - acc: 0.7593 - val_loss: 0.5198 - val_acc: 0.7359\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4823 - acc: 0.7643 - val_loss: 0.5210 - val_acc: 0.7351\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4804 - acc: 0.7651 - val_loss: 0.5194 - val_acc: 0.7366\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4769 - acc: 0.7682 - val_loss: 0.5086 - val_acc: 0.7437\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4708 - acc: 0.7736 - val_loss: 0.5055 - val_acc: 0.7465\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4684 - acc: 0.7736 - val_loss: 0.5134 - val_acc: 0.7401\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4640 - acc: 0.7780 - val_loss: 0.5015 - val_acc: 0.7474\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4579 - acc: 0.7819 - val_loss: 0.4991 - val_acc: 0.7495\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4554 - acc: 0.7841 - val_loss: 0.4950 - val_acc: 0.7533\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4571 - acc: 0.7825 - val_loss: 0.5092 - val_acc: 0.7446\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4548 - acc: 0.7829 - val_loss: 0.4910 - val_acc: 0.7567\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4499 - acc: 0.7873 - val_loss: 0.4972 - val_acc: 0.7527\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4445 - acc: 0.7920 - val_loss: 0.4866 - val_acc: 0.7596\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4489 - acc: 0.7870 - val_loss: 0.4904 - val_acc: 0.7571\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4439 - acc: 0.7909 - val_loss: 0.4909 - val_acc: 0.7560\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4400 - acc: 0.7933 - val_loss: 0.4863 - val_acc: 0.7577\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4344 - acc: 0.7995 - val_loss: 0.4797 - val_acc: 0.7643\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4311 - acc: 0.8008 - val_loss: 0.4848 - val_acc: 0.7610\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4311 - acc: 0.8008 - val_loss: 0.4786 - val_acc: 0.7648\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4278 - acc: 0.8035 - val_loss: 0.4796 - val_acc: 0.7646\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4291 - acc: 0.8009 - val_loss: 0.4737 - val_acc: 0.7679\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4225 - acc: 0.8075 - val_loss: 0.4741 - val_acc: 0.7683\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4222 - acc: 0.8074 - val_loss: 0.4747 - val_acc: 0.7677\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4206 - acc: 0.8077 - val_loss: 0.4771 - val_acc: 0.7648\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4179 - acc: 0.8109 - val_loss: 0.4743 - val_acc: 0.7685\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4173 - acc: 0.8104 - val_loss: 0.4671 - val_acc: 0.7723\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4222 - acc: 0.8044 - val_loss: 0.4709 - val_acc: 0.7700\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4147 - acc: 0.8115 - val_loss: 0.4724 - val_acc: 0.7688\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4137 - acc: 0.8116 - val_loss: 0.4717 - val_acc: 0.7705\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4114 - acc: 0.8135 - val_loss: 0.4861 - val_acc: 0.7604\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4110 - acc: 0.8128 - val_loss: 0.4646 - val_acc: 0.7730\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4089 - acc: 0.8143 - val_loss: 0.4661 - val_acc: 0.7732\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4097 - acc: 0.8131 - val_loss: 0.4636 - val_acc: 0.7751\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4085 - acc: 0.8142 - val_loss: 0.4611 - val_acc: 0.7753\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4070 - acc: 0.8147 - val_loss: 0.4717 - val_acc: 0.7706\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4087 - acc: 0.8142 - val_loss: 0.4604 - val_acc: 0.7758\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4042 - acc: 0.8173 - val_loss: 0.4628 - val_acc: 0.7757\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4004 - acc: 0.8196 - val_loss: 0.4797 - val_acc: 0.7690\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4033 - acc: 0.8156 - val_loss: 0.4605 - val_acc: 0.7743\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4016 - acc: 0.8173 - val_loss: 0.4637 - val_acc: 0.7726\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3960 - acc: 0.8230 - val_loss: 0.4606 - val_acc: 0.7763\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3969 - acc: 0.8216 - val_loss: 0.4666 - val_acc: 0.7733\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3932 - acc: 0.8243 - val_loss: 0.4575 - val_acc: 0.7763\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3965 - acc: 0.8213 - val_loss: 0.4557 - val_acc: 0.7776\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3934 - acc: 0.8237 - val_loss: 0.4609 - val_acc: 0.7765\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3932 - acc: 0.8235 - val_loss: 0.4556 - val_acc: 0.7780\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3902 - acc: 0.8263 - val_loss: 0.4571 - val_acc: 0.7787\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3875 - acc: 0.8278 - val_loss: 0.4544 - val_acc: 0.7792\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3860 - acc: 0.8296 - val_loss: 0.4550 - val_acc: 0.7800\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3881 - acc: 0.8264 - val_loss: 0.4574 - val_acc: 0.7782\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.3853 - acc: 0.8283 - val_loss: 0.4533 - val_acc: 0.7801\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3876 - acc: 0.8276 - val_loss: 0.4555 - val_acc: 0.7798\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.3889 - acc: 0.8249 - val_loss: 0.4636 - val_acc: 0.7734\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3858 - acc: 0.8272 - val_loss: 0.4591 - val_acc: 0.7792\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3870 - acc: 0.8254 - val_loss: 0.4594 - val_acc: 0.7773\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3818 - acc: 0.8302 - val_loss: 0.4554 - val_acc: 0.7806\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3787 - acc: 0.8320 - val_loss: 0.4545 - val_acc: 0.7797\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3785 - acc: 0.8325 - val_loss: 0.4553 - val_acc: 0.7796\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3806 - acc: 0.8303 - val_loss: 0.4542 - val_acc: 0.7798\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3780 - acc: 0.8321 - val_loss: 0.4562 - val_acc: 0.7816\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3797 - acc: 0.8314 - val_loss: 0.4590 - val_acc: 0.7786\n",
      "auroc: 0.8368214531683934\n",
      "auprc: 0.7359360579364799\n",
      "auroc: 0.8346168242835486\n",
      "auprc: 0.7320703203517329\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 4s 127us/step - loss: 0.7065 - acc: 0.6119 - val_loss: 0.6763 - val_acc: 0.6383\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.6740 - acc: 0.6299 - val_loss: 0.6701 - val_acc: 0.6345\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6570 - acc: 0.6420 - val_loss: 0.6590 - val_acc: 0.6425\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 45us/step - loss: 0.6450 - acc: 0.6499 - val_loss: 0.6476 - val_acc: 0.6490\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.6277 - acc: 0.6606 - val_loss: 0.6276 - val_acc: 0.6573\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6073 - acc: 0.6767 - val_loss: 0.6072 - val_acc: 0.6799\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5865 - acc: 0.6900 - val_loss: 0.6013 - val_acc: 0.6941\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5698 - acc: 0.7026 - val_loss: 0.5765 - val_acc: 0.7063\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5511 - acc: 0.7147 - val_loss: 0.5511 - val_acc: 0.7142\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5351 - acc: 0.7251 - val_loss: 0.5427 - val_acc: 0.7198\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5255 - acc: 0.7319 - val_loss: 0.5322 - val_acc: 0.7288\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5159 - acc: 0.7390 - val_loss: 0.5399 - val_acc: 0.7126\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5075 - acc: 0.7450 - val_loss: 0.5152 - val_acc: 0.7382\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4963 - acc: 0.7530 - val_loss: 0.5049 - val_acc: 0.7447\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4879 - acc: 0.7603 - val_loss: 0.4990 - val_acc: 0.7501\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4790 - acc: 0.7674 - val_loss: 0.4936 - val_acc: 0.7534\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4730 - acc: 0.7715 - val_loss: 0.4859 - val_acc: 0.7587\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4682 - acc: 0.7743 - val_loss: 0.4800 - val_acc: 0.7646\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4637 - acc: 0.7782 - val_loss: 0.4816 - val_acc: 0.7621\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4569 - acc: 0.7826 - val_loss: 0.4715 - val_acc: 0.7705\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4502 - acc: 0.7883 - val_loss: 0.4670 - val_acc: 0.7737\n",
      "Epoch 22/200\n",
      "24500/34284 [====================>.........] - ETA: 0s - loss: 0.4492 - acc: 0.7874"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.6443 - acc: 0.6510 - val_loss: 0.6651 - val_acc: 0.6477\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.6369 - acc: 0.6545 - val_loss: 0.6651 - val_acc: 0.6413\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.6317 - acc: 0.6586 - val_loss: 0.6573 - val_acc: 0.6261\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.6230 - acc: 0.6636 - val_loss: 0.6553 - val_acc: 0.6358\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.6174 - acc: 0.6692 - val_loss: 0.6444 - val_acc: 0.6556\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.6073 - acc: 0.6775 - val_loss: 0.6410 - val_acc: 0.6599\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.6012 - acc: 0.6797 - val_loss: 0.6310 - val_acc: 0.6673\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.5925 - acc: 0.6864 - val_loss: 0.6262 - val_acc: 0.6703\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.5843 - acc: 0.6928 - val_loss: 0.6219 - val_acc: 0.6765\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 1s 58us/step - loss: 0.5755 - acc: 0.6984 - val_loss: 0.6113 - val_acc: 0.6799\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.5686 - acc: 0.7049 - val_loss: 0.6016 - val_acc: 0.6842\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.5633 - acc: 0.7072 - val_loss: 0.5975 - val_acc: 0.6946\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.5565 - acc: 0.7131 - val_loss: 0.5831 - val_acc: 0.6943\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.5467 - acc: 0.7205 - val_loss: 0.5826 - val_acc: 0.7038\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.5400 - acc: 0.7261 - val_loss: 0.5766 - val_acc: 0.7044\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.5305 - acc: 0.7328 - val_loss: 0.5717 - val_acc: 0.7036\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.5232 - acc: 0.7381 - val_loss: 0.5610 - val_acc: 0.7169\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.5185 - acc: 0.7413 - val_loss: 0.5581 - val_acc: 0.7136\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.5132 - acc: 0.7450 - val_loss: 0.5519 - val_acc: 0.7172\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.5080 - acc: 0.7514 - val_loss: 0.5401 - val_acc: 0.7260\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.5016 - acc: 0.7544 - val_loss: 0.5320 - val_acc: 0.7322\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4939 - acc: 0.7610 - val_loss: 0.5279 - val_acc: 0.7349\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4918 - acc: 0.7597 - val_loss: 0.5475 - val_acc: 0.7189\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.4885 - acc: 0.7619 - val_loss: 0.5212 - val_acc: 0.7388\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4794 - acc: 0.7689 - val_loss: 0.5263 - val_acc: 0.7318\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.4837 - acc: 0.7651 - val_loss: 0.5188 - val_acc: 0.7381\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4738 - acc: 0.7742 - val_loss: 0.5180 - val_acc: 0.7418\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4719 - acc: 0.7750 - val_loss: 0.5211 - val_acc: 0.7388\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4704 - acc: 0.7728 - val_loss: 0.5117 - val_acc: 0.7457\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4669 - acc: 0.7765 - val_loss: 0.5302 - val_acc: 0.7406\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4640 - acc: 0.7802 - val_loss: 0.5034 - val_acc: 0.7508\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.4567 - acc: 0.7850 - val_loss: 0.5042 - val_acc: 0.7497\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4561 - acc: 0.7840 - val_loss: 0.5044 - val_acc: 0.7503\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4537 - acc: 0.7880 - val_loss: 0.5032 - val_acc: 0.7501\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4512 - acc: 0.7904 - val_loss: 0.5042 - val_acc: 0.7504\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.4483 - acc: 0.7900 - val_loss: 0.4947 - val_acc: 0.7557\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4457 - acc: 0.7919 - val_loss: 0.4928 - val_acc: 0.7572\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4438 - acc: 0.7929 - val_loss: 0.4992 - val_acc: 0.7543\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 1s 58us/step - loss: 0.4423 - acc: 0.7940 - val_loss: 0.4958 - val_acc: 0.7547\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4421 - acc: 0.7935 - val_loss: 0.4916 - val_acc: 0.7596\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4407 - acc: 0.7942 - val_loss: 0.4904 - val_acc: 0.7584\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4358 - acc: 0.7982 - val_loss: 0.4897 - val_acc: 0.7571\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.4385 - acc: 0.7960 - val_loss: 0.4859 - val_acc: 0.7621\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4315 - acc: 0.8003 - val_loss: 0.4942 - val_acc: 0.7563\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4306 - acc: 0.8014 - val_loss: 0.4831 - val_acc: 0.7638\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4294 - acc: 0.8024 - val_loss: 0.4904 - val_acc: 0.7559\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.4273 - acc: 0.8023 - val_loss: 0.4831 - val_acc: 0.7649\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4232 - acc: 0.8066 - val_loss: 0.4937 - val_acc: 0.7596\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 1s 58us/step - loss: 0.4307 - acc: 0.7982 - val_loss: 0.4917 - val_acc: 0.7610\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4255 - acc: 0.8034 - val_loss: 0.4833 - val_acc: 0.7639\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4208 - acc: 0.8070 - val_loss: 0.4932 - val_acc: 0.7611\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 1s 58us/step - loss: 0.4205 - acc: 0.8077 - val_loss: 0.4800 - val_acc: 0.7663\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4201 - acc: 0.8059 - val_loss: 0.4842 - val_acc: 0.7635\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4147 - acc: 0.8106 - val_loss: 0.4776 - val_acc: 0.7663\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4132 - acc: 0.8125 - val_loss: 0.4815 - val_acc: 0.7673\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.4148 - acc: 0.8104 - val_loss: 0.4809 - val_acc: 0.7662\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4144 - acc: 0.8110 - val_loss: 0.4812 - val_acc: 0.7658\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4112 - acc: 0.8126 - val_loss: 0.4767 - val_acc: 0.7681\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4099 - acc: 0.8129 - val_loss: 0.4769 - val_acc: 0.7670\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4125 - acc: 0.8113 - val_loss: 0.4798 - val_acc: 0.7636\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4123 - acc: 0.8105 - val_loss: 0.4741 - val_acc: 0.7715\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4088 - acc: 0.8138 - val_loss: 0.4856 - val_acc: 0.7654\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4092 - acc: 0.8121 - val_loss: 0.4727 - val_acc: 0.7687\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4035 - acc: 0.8173 - val_loss: 0.4880 - val_acc: 0.7630\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4034 - acc: 0.8164 - val_loss: 0.4723 - val_acc: 0.7712\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4020 - acc: 0.8184 - val_loss: 0.4728 - val_acc: 0.7720\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.4018 - acc: 0.8177 - val_loss: 0.4742 - val_acc: 0.7717\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3997 - acc: 0.8196 - val_loss: 0.4765 - val_acc: 0.7704\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.4024 - acc: 0.8176 - val_loss: 0.4750 - val_acc: 0.7706\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.3984 - acc: 0.8202 - val_loss: 0.4743 - val_acc: 0.7720\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.4000 - acc: 0.8190 - val_loss: 0.4698 - val_acc: 0.7737\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.4012 - acc: 0.8170 - val_loss: 0.4707 - val_acc: 0.7725\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3997 - acc: 0.8185 - val_loss: 0.4701 - val_acc: 0.7721\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3942 - acc: 0.8224 - val_loss: 0.4795 - val_acc: 0.7670\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3933 - acc: 0.8230 - val_loss: 0.4966 - val_acc: 0.7603\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3962 - acc: 0.8206 - val_loss: 0.4757 - val_acc: 0.7703\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3971 - acc: 0.8196 - val_loss: 0.4715 - val_acc: 0.7691\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3944 - acc: 0.8213 - val_loss: 0.4732 - val_acc: 0.7680\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3935 - acc: 0.8208 - val_loss: 0.4732 - val_acc: 0.7713\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.3937 - acc: 0.8212 - val_loss: 0.4680 - val_acc: 0.7730\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3912 - acc: 0.8239 - val_loss: 0.4722 - val_acc: 0.7697\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3911 - acc: 0.8233 - val_loss: 0.4693 - val_acc: 0.7748\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.3877 - acc: 0.8273 - val_loss: 0.4698 - val_acc: 0.7706\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3869 - acc: 0.8257 - val_loss: 0.4698 - val_acc: 0.7727\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3902 - acc: 0.8249 - val_loss: 0.4676 - val_acc: 0.7740\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.3888 - acc: 0.8242 - val_loss: 0.4722 - val_acc: 0.7714\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3858 - acc: 0.8264 - val_loss: 0.4651 - val_acc: 0.7744\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.3831 - acc: 0.8285 - val_loss: 0.4809 - val_acc: 0.7687\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3860 - acc: 0.8256 - val_loss: 0.4662 - val_acc: 0.7765\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3818 - acc: 0.8298 - val_loss: 0.4680 - val_acc: 0.7762\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3816 - acc: 0.8304 - val_loss: 0.4666 - val_acc: 0.7765\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3816 - acc: 0.8294 - val_loss: 0.4811 - val_acc: 0.7694\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3872 - acc: 0.8250 - val_loss: 0.4748 - val_acc: 0.7735\n",
      "Epoch 98/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3822 - acc: 0.8269 - val_loss: 0.4654 - val_acc: 0.7754\n",
      "Epoch 99/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3815 - acc: 0.8289 - val_loss: 0.4707 - val_acc: 0.7709\n",
      "Epoch 100/200\n",
      "17142/17142 [==============================] - 1s 60us/step - loss: 0.3816 - acc: 0.8284 - val_loss: 0.4865 - val_acc: 0.7690\n",
      "Epoch 101/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.3878 - acc: 0.8238 - val_loss: 0.4810 - val_acc: 0.7710\n",
      "auroc: 0.8270589322172771\n",
      "auprc: 0.7244651667573759\n",
      "auroc: 0.825890362732394\n",
      "auprc: 0.7225086194645839\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 5s 135us/step - loss: 0.7224 - acc: 0.6108 - val_loss: 0.6757 - val_acc: 0.6339\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6664 - acc: 0.6353 - val_loss: 0.6709 - val_acc: 0.6167\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.6515 - acc: 0.6447 - val_loss: 0.6599 - val_acc: 0.6306\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.6383 - acc: 0.6536 - val_loss: 0.6441 - val_acc: 0.6577\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.6220 - acc: 0.6662 - val_loss: 0.6284 - val_acc: 0.6611\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.6052 - acc: 0.6775 - val_loss: 0.6126 - val_acc: 0.6799\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5849 - acc: 0.6929 - val_loss: 0.5934 - val_acc: 0.6871\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5675 - acc: 0.7077 - val_loss: 0.5790 - val_acc: 0.7013\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5522 - acc: 0.7170 - val_loss: 0.5625 - val_acc: 0.7110\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5368 - acc: 0.7276 - val_loss: 0.5434 - val_acc: 0.7180\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5244 - acc: 0.7362 - val_loss: 0.5294 - val_acc: 0.7327\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5122 - acc: 0.7449 - val_loss: 0.5214 - val_acc: 0.7367\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5013 - acc: 0.7521 - val_loss: 0.5122 - val_acc: 0.7458\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4923 - acc: 0.7590 - val_loss: 0.4994 - val_acc: 0.7546\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4841 - acc: 0.7641 - val_loss: 0.5000 - val_acc: 0.7535\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4774 - acc: 0.7687 - val_loss: 0.4885 - val_acc: 0.7592\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4672 - acc: 0.7767 - val_loss: 0.4939 - val_acc: 0.7553\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4614 - acc: 0.7810 - val_loss: 0.4823 - val_acc: 0.7648\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4563 - acc: 0.7837 - val_loss: 0.4746 - val_acc: 0.7702\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4510 - acc: 0.7869 - val_loss: 0.4645 - val_acc: 0.7780\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4436 - acc: 0.7927 - val_loss: 0.4620 - val_acc: 0.7794\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4393 - acc: 0.7949 - val_loss: 0.4552 - val_acc: 0.7849\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4366 - acc: 0.7966 - val_loss: 0.4619 - val_acc: 0.7786\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4343 - acc: 0.7965 - val_loss: 0.4489 - val_acc: 0.7874\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4273 - acc: 0.8026 - val_loss: 0.4535 - val_acc: 0.7835\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4248 - acc: 0.8046 - val_loss: 0.4429 - val_acc: 0.7927\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4242 - acc: 0.8048 - val_loss: 0.4555 - val_acc: 0.7819\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4185 - acc: 0.8078 - val_loss: 0.4382 - val_acc: 0.7954\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4165 - acc: 0.8095 - val_loss: 0.4397 - val_acc: 0.7932\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4164 - acc: 0.8084 - val_loss: 0.4475 - val_acc: 0.7889\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4122 - acc: 0.8107 - val_loss: 0.4343 - val_acc: 0.7983\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4094 - acc: 0.8138 - val_loss: 0.4382 - val_acc: 0.7943\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4050 - acc: 0.8160 - val_loss: 0.4344 - val_acc: 0.7959\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4044 - acc: 0.8157 - val_loss: 0.4279 - val_acc: 0.8014\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4025 - acc: 0.8174 - val_loss: 0.4268 - val_acc: 0.8011\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3991 - acc: 0.8195 - val_loss: 0.4261 - val_acc: 0.8004\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4009 - acc: 0.8181 - val_loss: 0.4403 - val_acc: 0.7920\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3957 - acc: 0.8208 - val_loss: 0.4227 - val_acc: 0.8035\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3939 - acc: 0.8228 - val_loss: 0.4229 - val_acc: 0.8037\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3931 - acc: 0.8236 - val_loss: 0.4235 - val_acc: 0.8023\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3917 - acc: 0.8232 - val_loss: 0.4270 - val_acc: 0.8019\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3918 - acc: 0.8234 - val_loss: 0.4300 - val_acc: 0.7985\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3935 - acc: 0.8221 - val_loss: 0.4346 - val_acc: 0.7972\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3907 - acc: 0.8231 - val_loss: 0.4202 - val_acc: 0.8046\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3871 - acc: 0.8249 - val_loss: 0.4154 - val_acc: 0.8078\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3875 - acc: 0.8254 - val_loss: 0.4148 - val_acc: 0.8080\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3832 - acc: 0.8284 - val_loss: 0.4149 - val_acc: 0.8074\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3850 - acc: 0.8264 - val_loss: 0.4180 - val_acc: 0.8076\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3822 - acc: 0.8289 - val_loss: 0.4166 - val_acc: 0.8056\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3807 - acc: 0.8298 - val_loss: 0.4141 - val_acc: 0.8082\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3821 - acc: 0.8271 - val_loss: 0.4175 - val_acc: 0.8066\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3781 - acc: 0.8303 - val_loss: 0.4144 - val_acc: 0.8096\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3773 - acc: 0.8315 - val_loss: 0.4184 - val_acc: 0.8067\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3770 - acc: 0.8305 - val_loss: 0.4176 - val_acc: 0.8052\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3751 - acc: 0.8327 - val_loss: 0.4140 - val_acc: 0.8076\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3777 - acc: 0.8303 - val_loss: 0.4137 - val_acc: 0.8081\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3751 - acc: 0.8321 - val_loss: 0.4122 - val_acc: 0.8086\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3742 - acc: 0.8329 - val_loss: 0.4113 - val_acc: 0.8109\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3719 - acc: 0.8339 - val_loss: 0.4170 - val_acc: 0.8059\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3724 - acc: 0.8339 - val_loss: 0.4109 - val_acc: 0.8096\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3700 - acc: 0.8357 - val_loss: 0.4117 - val_acc: 0.8106\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3699 - acc: 0.8355 - val_loss: 0.4071 - val_acc: 0.8113\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3686 - acc: 0.8365 - val_loss: 0.4061 - val_acc: 0.8121\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3684 - acc: 0.8360 - val_loss: 0.4055 - val_acc: 0.8139\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3700 - acc: 0.8345 - val_loss: 0.4046 - val_acc: 0.8135\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3663 - acc: 0.8370 - val_loss: 0.4151 - val_acc: 0.8073\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3674 - acc: 0.8356 - val_loss: 0.4050 - val_acc: 0.8131\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3664 - acc: 0.8366 - val_loss: 0.4026 - val_acc: 0.8157\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3638 - acc: 0.8387 - val_loss: 0.4030 - val_acc: 0.8134\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3654 - acc: 0.8373 - val_loss: 0.4013 - val_acc: 0.8150\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3622 - acc: 0.8397 - val_loss: 0.4094 - val_acc: 0.8110\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3623 - acc: 0.8393 - val_loss: 0.4085 - val_acc: 0.8103\n",
      "Epoch 73/200\n",
      " 9500/34284 [=======>......................] - ETA: 0s - loss: 0.3658 - acc: 0.8370"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4795 - acc: 0.7666 - val_loss: 0.4898 - val_acc: 0.7559\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4699 - acc: 0.7737 - val_loss: 0.4861 - val_acc: 0.7589\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4638 - acc: 0.7778 - val_loss: 0.4779 - val_acc: 0.7670\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4571 - acc: 0.7831 - val_loss: 0.4724 - val_acc: 0.7696\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4506 - acc: 0.7881 - val_loss: 0.4671 - val_acc: 0.7720\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4487 - acc: 0.7881 - val_loss: 0.4689 - val_acc: 0.7718\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4436 - acc: 0.7915 - val_loss: 0.4554 - val_acc: 0.7819\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4356 - acc: 0.7973 - val_loss: 0.4539 - val_acc: 0.7831\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4332 - acc: 0.7980 - val_loss: 0.4493 - val_acc: 0.7854\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4283 - acc: 0.8012 - val_loss: 0.4500 - val_acc: 0.7845\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4243 - acc: 0.8040 - val_loss: 0.4412 - val_acc: 0.7904\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4190 - acc: 0.8075 - val_loss: 0.4469 - val_acc: 0.7863\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4154 - acc: 0.8099 - val_loss: 0.4356 - val_acc: 0.7954\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4140 - acc: 0.8095 - val_loss: 0.4331 - val_acc: 0.7957\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4128 - acc: 0.8089 - val_loss: 0.4402 - val_acc: 0.7907\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4083 - acc: 0.8125 - val_loss: 0.4264 - val_acc: 0.8006\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4052 - acc: 0.8159 - val_loss: 0.4272 - val_acc: 0.7990\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4007 - acc: 0.8185 - val_loss: 0.4399 - val_acc: 0.7901\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3989 - acc: 0.8196 - val_loss: 0.4290 - val_acc: 0.7970\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3990 - acc: 0.8180 - val_loss: 0.4180 - val_acc: 0.8037\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3940 - acc: 0.8219 - val_loss: 0.4181 - val_acc: 0.8048\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3932 - acc: 0.8216 - val_loss: 0.4223 - val_acc: 0.8030\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3934 - acc: 0.8207 - val_loss: 0.4206 - val_acc: 0.8024\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3916 - acc: 0.8227 - val_loss: 0.4118 - val_acc: 0.8076\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3885 - acc: 0.8242 - val_loss: 0.4096 - val_acc: 0.8098\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3883 - acc: 0.8235 - val_loss: 0.4082 - val_acc: 0.8106\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3836 - acc: 0.8267 - val_loss: 0.4080 - val_acc: 0.8103\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3843 - acc: 0.8255 - val_loss: 0.4079 - val_acc: 0.8097\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3831 - acc: 0.8260 - val_loss: 0.4063 - val_acc: 0.8110\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3823 - acc: 0.8274 - val_loss: 0.4070 - val_acc: 0.8107\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3791 - acc: 0.8285 - val_loss: 0.4070 - val_acc: 0.8119\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3775 - acc: 0.8293 - val_loss: 0.4063 - val_acc: 0.8122\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3778 - acc: 0.8285 - val_loss: 0.4011 - val_acc: 0.8141\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3749 - acc: 0.8310 - val_loss: 0.4001 - val_acc: 0.8148\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3725 - acc: 0.8324 - val_loss: 0.4062 - val_acc: 0.8109\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3713 - acc: 0.8339 - val_loss: 0.4002 - val_acc: 0.8146\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3734 - acc: 0.8311 - val_loss: 0.4022 - val_acc: 0.8147\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3705 - acc: 0.8330 - val_loss: 0.4098 - val_acc: 0.8106\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3691 - acc: 0.8342 - val_loss: 0.3993 - val_acc: 0.8157\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3676 - acc: 0.8347 - val_loss: 0.3991 - val_acc: 0.8171\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3639 - acc: 0.8384 - val_loss: 0.4027 - val_acc: 0.8129\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3675 - acc: 0.8344 - val_loss: 0.3972 - val_acc: 0.8164\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3657 - acc: 0.8352 - val_loss: 0.3973 - val_acc: 0.8162\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3632 - acc: 0.8371 - val_loss: 0.4036 - val_acc: 0.8131\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3631 - acc: 0.8369 - val_loss: 0.3948 - val_acc: 0.8183\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3647 - acc: 0.8354 - val_loss: 0.4168 - val_acc: 0.8047\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3639 - acc: 0.8373 - val_loss: 0.3981 - val_acc: 0.8154\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3592 - acc: 0.8397 - val_loss: 0.4141 - val_acc: 0.8080\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3602 - acc: 0.8390 - val_loss: 0.3935 - val_acc: 0.8204\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3627 - acc: 0.8358 - val_loss: 0.3917 - val_acc: 0.8197\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3585 - acc: 0.8384 - val_loss: 0.3959 - val_acc: 0.8167\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3581 - acc: 0.8391 - val_loss: 0.4022 - val_acc: 0.8137\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3575 - acc: 0.8398 - val_loss: 0.3955 - val_acc: 0.8193\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3549 - acc: 0.8415 - val_loss: 0.4056 - val_acc: 0.8106\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3549 - acc: 0.8418 - val_loss: 0.3935 - val_acc: 0.8194\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3543 - acc: 0.8424 - val_loss: 0.3982 - val_acc: 0.8179\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3526 - acc: 0.8435 - val_loss: 0.3943 - val_acc: 0.8174\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3527 - acc: 0.8423 - val_loss: 0.3936 - val_acc: 0.8176\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3528 - acc: 0.8427 - val_loss: 0.3934 - val_acc: 0.8179\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3542 - acc: 0.8415 - val_loss: 0.3969 - val_acc: 0.8176\n",
      "auroc: 0.8819468347528505\n",
      "auprc: 0.807333972641838\n",
      "auroc: 0.8801286005324805\n",
      "auprc: 0.8040932459089588\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 5s 154us/step - loss: 0.7009 - acc: 0.6153 - val_loss: 0.6833 - val_acc: 0.6202\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.6598 - acc: 0.6388 - val_loss: 0.6813 - val_acc: 0.6203\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.6454 - acc: 0.6491 - val_loss: 0.6717 - val_acc: 0.6303\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.6388 - acc: 0.6528 - val_loss: 0.6539 - val_acc: 0.6444\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.6188 - acc: 0.6664 - val_loss: 0.6372 - val_acc: 0.6591\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.5994 - acc: 0.6810 - val_loss: 0.6194 - val_acc: 0.6672\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.5815 - acc: 0.6946 - val_loss: 0.6010 - val_acc: 0.6849\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.5622 - acc: 0.7090 - val_loss: 0.5840 - val_acc: 0.6839\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.5451 - acc: 0.7220 - val_loss: 0.5657 - val_acc: 0.7079\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.5262 - acc: 0.7353 - val_loss: 0.5507 - val_acc: 0.7170\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.5153 - acc: 0.7437 - val_loss: 0.5417 - val_acc: 0.7226\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.5003 - acc: 0.7526 - val_loss: 0.5256 - val_acc: 0.7301\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.4894 - acc: 0.7618 - val_loss: 0.5171 - val_acc: 0.7371\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.4791 - acc: 0.7688 - val_loss: 0.5159 - val_acc: 0.7382\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4730 - acc: 0.7722 - val_loss: 0.5110 - val_acc: 0.7433\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4652 - acc: 0.7774 - val_loss: 0.4985 - val_acc: 0.7497\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.4556 - acc: 0.7837 - val_loss: 0.4959 - val_acc: 0.7511\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4490 - acc: 0.7887 - val_loss: 0.4866 - val_acc: 0.7572\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.4444 - acc: 0.7909 - val_loss: 0.4852 - val_acc: 0.7576\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.4381 - acc: 0.7952 - val_loss: 0.4939 - val_acc: 0.7555\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4351 - acc: 0.7957 - val_loss: 0.5006 - val_acc: 0.7549\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.4285 - acc: 0.8015 - val_loss: 0.4777 - val_acc: 0.7648\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.4257 - acc: 0.8029 - val_loss: 0.4704 - val_acc: 0.7706\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4225 - acc: 0.8038 - val_loss: 0.4693 - val_acc: 0.7700\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4221 - acc: 0.8029 - val_loss: 0.4676 - val_acc: 0.7723\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4158 - acc: 0.8088 - val_loss: 0.4655 - val_acc: 0.7728\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4095 - acc: 0.8133 - val_loss: 0.4693 - val_acc: 0.7711\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4085 - acc: 0.8122 - val_loss: 0.4657 - val_acc: 0.7737\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4035 - acc: 0.8159 - val_loss: 0.4577 - val_acc: 0.7783\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4013 - acc: 0.8168 - val_loss: 0.4589 - val_acc: 0.7760\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3999 - acc: 0.8178 - val_loss: 0.4690 - val_acc: 0.7711\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3947 - acc: 0.8216 - val_loss: 0.4572 - val_acc: 0.7773\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3916 - acc: 0.8229 - val_loss: 0.4568 - val_acc: 0.7805\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3896 - acc: 0.8238 - val_loss: 0.4515 - val_acc: 0.7832\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3877 - acc: 0.8254 - val_loss: 0.4535 - val_acc: 0.7817\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3845 - acc: 0.8264 - val_loss: 0.4499 - val_acc: 0.7844\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3821 - acc: 0.8295 - val_loss: 0.4523 - val_acc: 0.7831\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3823 - acc: 0.8283 - val_loss: 0.4475 - val_acc: 0.7846\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3781 - acc: 0.8303 - val_loss: 0.4464 - val_acc: 0.7861\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3770 - acc: 0.8310 - val_loss: 0.4673 - val_acc: 0.7757\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3756 - acc: 0.8317 - val_loss: 0.4462 - val_acc: 0.7863\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3755 - acc: 0.8319 - val_loss: 0.4491 - val_acc: 0.7843\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3732 - acc: 0.8334 - val_loss: 0.4548 - val_acc: 0.7825\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3706 - acc: 0.8340 - val_loss: 0.4449 - val_acc: 0.7880\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3678 - acc: 0.8359 - val_loss: 0.4508 - val_acc: 0.7851\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3677 - acc: 0.8355 - val_loss: 0.4438 - val_acc: 0.7885\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3651 - acc: 0.8380 - val_loss: 0.4468 - val_acc: 0.7884\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3639 - acc: 0.8375 - val_loss: 0.4459 - val_acc: 0.7872\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3607 - acc: 0.8401 - val_loss: 0.4418 - val_acc: 0.7901\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3614 - acc: 0.8392 - val_loss: 0.4427 - val_acc: 0.7891\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3591 - acc: 0.8405 - val_loss: 0.4490 - val_acc: 0.7863\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3593 - acc: 0.8408 - val_loss: 0.4418 - val_acc: 0.7903\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3575 - acc: 0.8413 - val_loss: 0.4425 - val_acc: 0.7905\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3565 - acc: 0.8419 - val_loss: 0.4522 - val_acc: 0.7825\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3543 - acc: 0.8436 - val_loss: 0.4426 - val_acc: 0.7905\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3567 - acc: 0.8413 - val_loss: 0.4711 - val_acc: 0.7722\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 50us/step - loss: 0.3527 - acc: 0.8437 - val_loss: 0.4446 - val_acc: 0.7904\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3517 - acc: 0.8446 - val_loss: 0.4434 - val_acc: 0.7913\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3504 - acc: 0.8449 - val_loss: 0.4430 - val_acc: 0.7897\n",
      "auroc: 0.8446631533326473\n",
      "auprc: 0.7499685164914677\n",
      "auroc: 0.8422799944503029\n",
      "auprc: 0.7465540367172475\n",
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 5s 268us/step - loss: 0.7415 - acc: 0.6006 - val_loss: 0.6943 - val_acc: 0.6146\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6735 - acc: 0.6273 - val_loss: 0.6750 - val_acc: 0.6340\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6608 - acc: 0.6360 - val_loss: 0.6739 - val_acc: 0.6448\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6489 - acc: 0.6460 - val_loss: 0.6632 - val_acc: 0.6340\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6378 - acc: 0.6522 - val_loss: 0.6565 - val_acc: 0.6360\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.6300 - acc: 0.6568 - val_loss: 0.6468 - val_acc: 0.6487\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.6191 - acc: 0.6655 - val_loss: 0.6501 - val_acc: 0.6445\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.6110 - acc: 0.6710 - val_loss: 0.6394 - val_acc: 0.6577\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5998 - acc: 0.6814 - val_loss: 0.6312 - val_acc: 0.6535\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5884 - acc: 0.6906 - val_loss: 0.6209 - val_acc: 0.6849\n",
      "Epoch 11/200\n",
      "14000/17142 [=======================>......] - ETA: 0s - loss: 0.5787 - acc: 0.6987"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3852 - acc: 0.8282 - val_loss: 0.4462 - val_acc: 0.7861\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3824 - acc: 0.8287 - val_loss: 0.4426 - val_acc: 0.7886\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3803 - acc: 0.8313 - val_loss: 0.4454 - val_acc: 0.7865\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3815 - acc: 0.8287 - val_loss: 0.4705 - val_acc: 0.7701\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3788 - acc: 0.8307 - val_loss: 0.4501 - val_acc: 0.7846\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3796 - acc: 0.8308 - val_loss: 0.4739 - val_acc: 0.7710\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3743 - acc: 0.8328 - val_loss: 0.4458 - val_acc: 0.7876\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3710 - acc: 0.8356 - val_loss: 0.4381 - val_acc: 0.7920\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3680 - acc: 0.8374 - val_loss: 0.4497 - val_acc: 0.7866\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3689 - acc: 0.8363 - val_loss: 0.4336 - val_acc: 0.7939\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3657 - acc: 0.8382 - val_loss: 0.4403 - val_acc: 0.7914\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3646 - acc: 0.8387 - val_loss: 0.4334 - val_acc: 0.7939\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3603 - acc: 0.8414 - val_loss: 0.4337 - val_acc: 0.7957\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3604 - acc: 0.8407 - val_loss: 0.4387 - val_acc: 0.7915\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3594 - acc: 0.8403 - val_loss: 0.4326 - val_acc: 0.7956\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3596 - acc: 0.8409 - val_loss: 0.4460 - val_acc: 0.7889\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3559 - acc: 0.8436 - val_loss: 0.4321 - val_acc: 0.7951\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3566 - acc: 0.8430 - val_loss: 0.4345 - val_acc: 0.7963\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3532 - acc: 0.8447 - val_loss: 0.4519 - val_acc: 0.7854\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3533 - acc: 0.8442 - val_loss: 0.4344 - val_acc: 0.7948\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3532 - acc: 0.8432 - val_loss: 0.4377 - val_acc: 0.7920\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3508 - acc: 0.8445 - val_loss: 0.4355 - val_acc: 0.7942\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3488 - acc: 0.8458 - val_loss: 0.4414 - val_acc: 0.7921\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3506 - acc: 0.8448 - val_loss: 0.4354 - val_acc: 0.7957\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3475 - acc: 0.8467 - val_loss: 0.4351 - val_acc: 0.7961\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3454 - acc: 0.8485 - val_loss: 0.4357 - val_acc: 0.7954\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3444 - acc: 0.8478 - val_loss: 0.4317 - val_acc: 0.7974\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3434 - acc: 0.8495 - val_loss: 0.4404 - val_acc: 0.7944\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3420 - acc: 0.8506 - val_loss: 0.4502 - val_acc: 0.7880\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3464 - acc: 0.8467 - val_loss: 0.4326 - val_acc: 0.7971\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3396 - acc: 0.8516 - val_loss: 0.4391 - val_acc: 0.7930\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3395 - acc: 0.8509 - val_loss: 0.4336 - val_acc: 0.7956\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3392 - acc: 0.8522 - val_loss: 0.4327 - val_acc: 0.7981\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3403 - acc: 0.8497 - val_loss: 0.4388 - val_acc: 0.7943\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3383 - acc: 0.8524 - val_loss: 0.4377 - val_acc: 0.7947\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3361 - acc: 0.8528 - val_loss: 0.4412 - val_acc: 0.7920\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3362 - acc: 0.8526 - val_loss: 0.4353 - val_acc: 0.7941\n",
      "auroc: 0.8535593841145405\n",
      "auprc: 0.7634740724991097\n",
      "auroc: 0.8528423300672475\n",
      "auprc: 0.7625908911450162\n",
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 5s 299us/step - loss: 0.7847 - acc: 0.5859 - val_loss: 0.6880 - val_acc: 0.6277\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 1s 59us/step - loss: 0.6781 - acc: 0.6272 - val_loss: 0.6830 - val_acc: 0.6198\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6635 - acc: 0.6354 - val_loss: 0.6797 - val_acc: 0.6193\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6550 - acc: 0.6418 - val_loss: 0.6778 - val_acc: 0.6184\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6479 - acc: 0.6464 - val_loss: 0.6669 - val_acc: 0.6313\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6382 - acc: 0.6548 - val_loss: 0.6641 - val_acc: 0.6300\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6314 - acc: 0.6590 - val_loss: 0.6604 - val_acc: 0.6365\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6257 - acc: 0.6630 - val_loss: 0.6488 - val_acc: 0.6492\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.6166 - acc: 0.6690 - val_loss: 0.6422 - val_acc: 0.6528\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.6062 - acc: 0.6749 - val_loss: 0.6358 - val_acc: 0.6524\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5964 - acc: 0.6827 - val_loss: 0.6235 - val_acc: 0.6648\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5861 - acc: 0.6898 - val_loss: 0.6153 - val_acc: 0.6705\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5770 - acc: 0.6985 - val_loss: 0.6028 - val_acc: 0.6757\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5652 - acc: 0.7068 - val_loss: 0.5966 - val_acc: 0.6811\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5559 - acc: 0.7153 - val_loss: 0.5897 - val_acc: 0.6945\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5465 - acc: 0.7224 - val_loss: 0.5761 - val_acc: 0.6925\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.5373 - acc: 0.7280 - val_loss: 0.5732 - val_acc: 0.7022\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.5348 - acc: 0.7299 - val_loss: 0.5635 - val_acc: 0.7037\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5250 - acc: 0.7376 - val_loss: 0.5524 - val_acc: 0.7170\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5144 - acc: 0.7458 - val_loss: 0.5650 - val_acc: 0.7097\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5096 - acc: 0.7478 - val_loss: 0.5432 - val_acc: 0.7179\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5052 - acc: 0.7509 - val_loss: 0.5406 - val_acc: 0.7228\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5042 - acc: 0.7499 - val_loss: 0.5414 - val_acc: 0.7299\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.5002 - acc: 0.7532 - val_loss: 0.5345 - val_acc: 0.7184\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4887 - acc: 0.7625 - val_loss: 0.5246 - val_acc: 0.7335\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4824 - acc: 0.7667 - val_loss: 0.5197 - val_acc: 0.7381\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4790 - acc: 0.7690 - val_loss: 0.5142 - val_acc: 0.7395\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4744 - acc: 0.7720 - val_loss: 0.5153 - val_acc: 0.7385\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4701 - acc: 0.7756 - val_loss: 0.5082 - val_acc: 0.7422\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4664 - acc: 0.7779 - val_loss: 0.5078 - val_acc: 0.7462\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4633 - acc: 0.7783 - val_loss: 0.5027 - val_acc: 0.7490\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4620 - acc: 0.7790 - val_loss: 0.5000 - val_acc: 0.7476\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4563 - acc: 0.7839 - val_loss: 0.5067 - val_acc: 0.7495\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4530 - acc: 0.7863 - val_loss: 0.4984 - val_acc: 0.7466\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4513 - acc: 0.7881 - val_loss: 0.4933 - val_acc: 0.7530\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4461 - acc: 0.7917 - val_loss: 0.4958 - val_acc: 0.7551\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4459 - acc: 0.7908 - val_loss: 0.4885 - val_acc: 0.7575\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4415 - acc: 0.7936 - val_loss: 0.4854 - val_acc: 0.7603\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4392 - acc: 0.7952 - val_loss: 0.4899 - val_acc: 0.7539\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4394 - acc: 0.7952 - val_loss: 0.4824 - val_acc: 0.7614\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4338 - acc: 0.7993 - val_loss: 0.4857 - val_acc: 0.7572\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4315 - acc: 0.8011 - val_loss: 0.4837 - val_acc: 0.7610\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4305 - acc: 0.8009 - val_loss: 0.4783 - val_acc: 0.7638\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4275 - acc: 0.8028 - val_loss: 0.4781 - val_acc: 0.7654\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4244 - acc: 0.8059 - val_loss: 0.4889 - val_acc: 0.7607\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4298 - acc: 0.7992 - val_loss: 0.4838 - val_acc: 0.7629\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4227 - acc: 0.8061 - val_loss: 0.4718 - val_acc: 0.7696\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4191 - acc: 0.8087 - val_loss: 0.4708 - val_acc: 0.7715\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4172 - acc: 0.8111 - val_loss: 0.4835 - val_acc: 0.7629\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4158 - acc: 0.8090 - val_loss: 0.4687 - val_acc: 0.7725\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4130 - acc: 0.8128 - val_loss: 0.4674 - val_acc: 0.7728\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4105 - acc: 0.8138 - val_loss: 0.4675 - val_acc: 0.7722\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4112 - acc: 0.8131 - val_loss: 0.4935 - val_acc: 0.7564\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4084 - acc: 0.8154 - val_loss: 0.4665 - val_acc: 0.7741\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4098 - acc: 0.8138 - val_loss: 0.4649 - val_acc: 0.7735\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4054 - acc: 0.8168 - val_loss: 0.4646 - val_acc: 0.7745\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4037 - acc: 0.8187 - val_loss: 0.4654 - val_acc: 0.7748\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4009 - acc: 0.8175 - val_loss: 0.4656 - val_acc: 0.7734\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4028 - acc: 0.8172 - val_loss: 0.4677 - val_acc: 0.7709\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.4017 - acc: 0.8173 - val_loss: 0.4616 - val_acc: 0.7767\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3971 - acc: 0.8217 - val_loss: 0.4619 - val_acc: 0.7758\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3984 - acc: 0.8209 - val_loss: 0.4666 - val_acc: 0.7731\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4002 - acc: 0.8189 - val_loss: 0.4708 - val_acc: 0.7740\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3962 - acc: 0.8208 - val_loss: 0.4591 - val_acc: 0.7785\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3931 - acc: 0.8236 - val_loss: 0.4659 - val_acc: 0.7732\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4003 - acc: 0.8184 - val_loss: 0.4726 - val_acc: 0.7684\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4003 - acc: 0.8171 - val_loss: 0.4654 - val_acc: 0.7755\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3919 - acc: 0.8225 - val_loss: 0.4593 - val_acc: 0.7774\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3922 - acc: 0.8226 - val_loss: 0.4574 - val_acc: 0.7802\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3954 - acc: 0.8225 - val_loss: 0.4594 - val_acc: 0.7783\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3899 - acc: 0.8240 - val_loss: 0.4572 - val_acc: 0.7799\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3859 - acc: 0.8279 - val_loss: 0.4594 - val_acc: 0.7783\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3845 - acc: 0.8280 - val_loss: 0.4603 - val_acc: 0.7785\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3837 - acc: 0.8284 - val_loss: 0.4630 - val_acc: 0.7774\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3818 - acc: 0.8301 - val_loss: 0.4573 - val_acc: 0.7803\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3828 - acc: 0.8292 - val_loss: 0.4605 - val_acc: 0.7774\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3829 - acc: 0.8281 - val_loss: 0.4551 - val_acc: 0.7815\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3808 - acc: 0.8297 - val_loss: 0.4626 - val_acc: 0.7758\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3790 - acc: 0.8315 - val_loss: 0.4557 - val_acc: 0.7803\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3773 - acc: 0.8326 - val_loss: 0.4575 - val_acc: 0.7778\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3779 - acc: 0.8322 - val_loss: 0.4555 - val_acc: 0.7822\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3778 - acc: 0.8313 - val_loss: 0.4625 - val_acc: 0.7789\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3777 - acc: 0.8310 - val_loss: 0.4699 - val_acc: 0.7773\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3781 - acc: 0.8309 - val_loss: 0.4515 - val_acc: 0.7837\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3773 - acc: 0.8296 - val_loss: 0.4507 - val_acc: 0.7850\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3734 - acc: 0.8336 - val_loss: 0.4506 - val_acc: 0.7840\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3741 - acc: 0.8344 - val_loss: 0.4700 - val_acc: 0.7759\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3740 - acc: 0.8336 - val_loss: 0.4515 - val_acc: 0.7845\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3720 - acc: 0.8345 - val_loss: 0.4502 - val_acc: 0.7852\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3719 - acc: 0.8347 - val_loss: 0.4660 - val_acc: 0.7772\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3738 - acc: 0.8327 - val_loss: 0.4610 - val_acc: 0.7809\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3696 - acc: 0.8357 - val_loss: 0.4503 - val_acc: 0.7852\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3735 - acc: 0.8332 - val_loss: 0.4547 - val_acc: 0.7822\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3727 - acc: 0.8329 - val_loss: 0.4676 - val_acc: 0.7741\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3691 - acc: 0.8364 - val_loss: 0.4689 - val_acc: 0.7747\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3677 - acc: 0.8378 - val_loss: 0.4633 - val_acc: 0.7776\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3668 - acc: 0.8370 - val_loss: 0.4513 - val_acc: 0.7844\n",
      "Epoch 98/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3644 - acc: 0.8393 - val_loss: 0.4524 - val_acc: 0.7844\n",
      "Epoch 99/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3643 - acc: 0.8396 - val_loss: 0.4491 - val_acc: 0.7860\n",
      "Epoch 100/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3634 - acc: 0.8402 - val_loss: 0.4546 - val_acc: 0.7834\n",
      "Epoch 101/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3669 - acc: 0.8365 - val_loss: 0.4519 - val_acc: 0.7834\n",
      "Epoch 102/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3632 - acc: 0.8404 - val_loss: 0.4589 - val_acc: 0.7797\n",
      "Epoch 103/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3634 - acc: 0.8400 - val_loss: 0.4545 - val_acc: 0.7818\n",
      "Epoch 104/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3635 - acc: 0.8394 - val_loss: 0.4507 - val_acc: 0.7842\n",
      "Epoch 105/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3631 - acc: 0.8390 - val_loss: 0.4760 - val_acc: 0.7736\n",
      "Epoch 106/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3670 - acc: 0.8376 - val_loss: 0.4480 - val_acc: 0.7865\n",
      "Epoch 107/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3584 - acc: 0.8434 - val_loss: 0.4527 - val_acc: 0.7850\n",
      "Epoch 108/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3592 - acc: 0.8424 - val_loss: 0.4531 - val_acc: 0.7853\n",
      "Epoch 109/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3616 - acc: 0.8393 - val_loss: 0.4719 - val_acc: 0.7751\n",
      "Epoch 110/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3613 - acc: 0.8391 - val_loss: 0.4621 - val_acc: 0.7809\n",
      "Epoch 111/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3629 - acc: 0.8389 - val_loss: 0.4598 - val_acc: 0.7808\n",
      "Epoch 112/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3596 - acc: 0.8418 - val_loss: 0.4491 - val_acc: 0.7870\n",
      "Epoch 113/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3569 - acc: 0.8424 - val_loss: 0.4713 - val_acc: 0.7763\n",
      "Epoch 114/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3603 - acc: 0.8396 - val_loss: 0.4498 - val_acc: 0.7867\n",
      "Epoch 115/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3560 - acc: 0.8441 - val_loss: 0.4478 - val_acc: 0.7887\n",
      "Epoch 116/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3546 - acc: 0.8448 - val_loss: 0.4529 - val_acc: 0.7870\n",
      "Epoch 117/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3583 - acc: 0.8429 - val_loss: 0.4670 - val_acc: 0.7766\n",
      "Epoch 118/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3567 - acc: 0.8429 - val_loss: 0.4522 - val_acc: 0.7852\n",
      "Epoch 119/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3548 - acc: 0.8435 - val_loss: 0.4505 - val_acc: 0.7879\n",
      "Epoch 120/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3538 - acc: 0.8447 - val_loss: 0.4490 - val_acc: 0.7874\n",
      "Epoch 121/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3535 - acc: 0.8455 - val_loss: 0.4561 - val_acc: 0.7849\n",
      "Epoch 122/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3563 - acc: 0.8434 - val_loss: 0.4561 - val_acc: 0.7850\n",
      "Epoch 123/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3549 - acc: 0.8437 - val_loss: 0.4501 - val_acc: 0.7866\n",
      "Epoch 124/200\n",
      "17142/17142 [==============================] - 1s 54us/step - loss: 0.3505 - acc: 0.8450 - val_loss: 0.4494 - val_acc: 0.7876\n",
      "Epoch 125/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3509 - acc: 0.8454 - val_loss: 0.4602 - val_acc: 0.7807\n",
      "auroc: 0.8421822023085014\n",
      "auprc: 0.7455553360841463\n",
      "auroc: 0.8414537375061567\n",
      "auprc: 0.7442866210591251\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 6s 170us/step - loss: 0.7318 - acc: 0.6073 - val_loss: 0.6835 - val_acc: 0.6270\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.6679 - acc: 0.6325 - val_loss: 0.6713 - val_acc: 0.6254\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.6532 - acc: 0.6421 - val_loss: 0.6589 - val_acc: 0.6344\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6415 - acc: 0.6509 - val_loss: 0.6497 - val_acc: 0.6374\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.6204 - acc: 0.6654 - val_loss: 0.6243 - val_acc: 0.6660\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.6019 - acc: 0.6786 - val_loss: 0.6091 - val_acc: 0.6734\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5866 - acc: 0.6907 - val_loss: 0.5854 - val_acc: 0.6912\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.5644 - acc: 0.7077 - val_loss: 0.5739 - val_acc: 0.6980\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.5495 - acc: 0.7190 - val_loss: 0.5621 - val_acc: 0.7001\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5368 - acc: 0.7276 - val_loss: 0.5455 - val_acc: 0.7212\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5238 - acc: 0.7360 - val_loss: 0.5346 - val_acc: 0.7328\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.5102 - acc: 0.7472 - val_loss: 0.5274 - val_acc: 0.7361\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.5004 - acc: 0.7525 - val_loss: 0.5129 - val_acc: 0.7428\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4903 - acc: 0.7587 - val_loss: 0.5020 - val_acc: 0.7505\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4809 - acc: 0.7656 - val_loss: 0.4914 - val_acc: 0.7563\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4760 - acc: 0.7683 - val_loss: 0.4871 - val_acc: 0.7601\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4674 - acc: 0.7749 - val_loss: 0.4814 - val_acc: 0.7629\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4611 - acc: 0.7788 - val_loss: 0.4776 - val_acc: 0.7661\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4545 - acc: 0.7834 - val_loss: 0.4721 - val_acc: 0.7687\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4489 - acc: 0.7882 - val_loss: 0.4616 - val_acc: 0.7767\n",
      "Epoch 21/200\n",
      "12500/34284 [=========>....................] - ETA: 0s - loss: 0.4443 - acc: 0.7901"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4268 - acc: 0.8007 - val_loss: 0.4806 - val_acc: 0.7632\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4265 - acc: 0.8005 - val_loss: 0.4801 - val_acc: 0.7641\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4238 - acc: 0.8031 - val_loss: 0.4804 - val_acc: 0.7618\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4222 - acc: 0.8034 - val_loss: 0.4819 - val_acc: 0.7638\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4197 - acc: 0.8051 - val_loss: 0.4769 - val_acc: 0.7632\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4147 - acc: 0.8095 - val_loss: 0.4759 - val_acc: 0.7675\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4139 - acc: 0.8082 - val_loss: 0.4756 - val_acc: 0.7667\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4114 - acc: 0.8105 - val_loss: 0.4696 - val_acc: 0.7713\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4116 - acc: 0.8095 - val_loss: 0.4766 - val_acc: 0.7660\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4128 - acc: 0.8092 - val_loss: 0.4689 - val_acc: 0.7716\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4096 - acc: 0.8116 - val_loss: 0.5006 - val_acc: 0.7559\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4103 - acc: 0.8101 - val_loss: 0.4689 - val_acc: 0.7698\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.4059 - acc: 0.8147 - val_loss: 0.4662 - val_acc: 0.7737\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4023 - acc: 0.8162 - val_loss: 0.4738 - val_acc: 0.7696\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4030 - acc: 0.8155 - val_loss: 0.4711 - val_acc: 0.7693\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.4026 - acc: 0.8173 - val_loss: 0.4663 - val_acc: 0.7725\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3995 - acc: 0.8164 - val_loss: 0.4694 - val_acc: 0.7713\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3991 - acc: 0.8182 - val_loss: 0.4610 - val_acc: 0.7759\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3969 - acc: 0.8190 - val_loss: 0.4586 - val_acc: 0.7778\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3929 - acc: 0.8210 - val_loss: 0.4620 - val_acc: 0.7758\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3954 - acc: 0.8201 - val_loss: 0.4769 - val_acc: 0.7673\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3981 - acc: 0.8171 - val_loss: 0.4622 - val_acc: 0.7744\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 1s 57us/step - loss: 0.3890 - acc: 0.8229 - val_loss: 0.4620 - val_acc: 0.7764\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3894 - acc: 0.8240 - val_loss: 0.4645 - val_acc: 0.7755\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3901 - acc: 0.8234 - val_loss: 0.4609 - val_acc: 0.7755\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3891 - acc: 0.8245 - val_loss: 0.4556 - val_acc: 0.7788\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3842 - acc: 0.8266 - val_loss: 0.4558 - val_acc: 0.7792\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3857 - acc: 0.8254 - val_loss: 0.4535 - val_acc: 0.7802\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3820 - acc: 0.8279 - val_loss: 0.4559 - val_acc: 0.7790\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3818 - acc: 0.8290 - val_loss: 0.4530 - val_acc: 0.7809\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3810 - acc: 0.8288 - val_loss: 0.4746 - val_acc: 0.7694\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3831 - acc: 0.8270 - val_loss: 0.4514 - val_acc: 0.7817\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3824 - acc: 0.8269 - val_loss: 0.4506 - val_acc: 0.7821\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3800 - acc: 0.8282 - val_loss: 0.4562 - val_acc: 0.7790\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3753 - acc: 0.8324 - val_loss: 0.4496 - val_acc: 0.7837\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3778 - acc: 0.8307 - val_loss: 0.4640 - val_acc: 0.7732\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3800 - acc: 0.8282 - val_loss: 0.4806 - val_acc: 0.7686\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3788 - acc: 0.8276 - val_loss: 0.4611 - val_acc: 0.7760\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3747 - acc: 0.8323 - val_loss: 0.4526 - val_acc: 0.7813\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3711 - acc: 0.8336 - val_loss: 0.4479 - val_acc: 0.7845\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3766 - acc: 0.8291 - val_loss: 0.4640 - val_acc: 0.7734\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3734 - acc: 0.8325 - val_loss: 0.4489 - val_acc: 0.7848\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3695 - acc: 0.8351 - val_loss: 0.4499 - val_acc: 0.7843\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3676 - acc: 0.8365 - val_loss: 0.4482 - val_acc: 0.7850\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3692 - acc: 0.8329 - val_loss: 0.4545 - val_acc: 0.7800\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3672 - acc: 0.8360 - val_loss: 0.4508 - val_acc: 0.7830\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3654 - acc: 0.8369 - val_loss: 0.4627 - val_acc: 0.7785\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3675 - acc: 0.8366 - val_loss: 0.4454 - val_acc: 0.7862\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3661 - acc: 0.8371 - val_loss: 0.4504 - val_acc: 0.7825\n",
      "Epoch 98/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3664 - acc: 0.8371 - val_loss: 0.4497 - val_acc: 0.7844\n",
      "Epoch 99/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3630 - acc: 0.8383 - val_loss: 0.4513 - val_acc: 0.7854\n",
      "Epoch 100/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3633 - acc: 0.8389 - val_loss: 0.4552 - val_acc: 0.7814\n",
      "Epoch 101/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3620 - acc: 0.8386 - val_loss: 0.4583 - val_acc: 0.7791\n",
      "Epoch 102/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3634 - acc: 0.8389 - val_loss: 0.4475 - val_acc: 0.7859\n",
      "Epoch 103/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3611 - acc: 0.8395 - val_loss: 0.4475 - val_acc: 0.7869\n",
      "Epoch 104/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3595 - acc: 0.8411 - val_loss: 0.4461 - val_acc: 0.7873\n",
      "Epoch 105/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3589 - acc: 0.8414 - val_loss: 0.4467 - val_acc: 0.7878\n",
      "Epoch 106/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3580 - acc: 0.8413 - val_loss: 0.4437 - val_acc: 0.7892\n",
      "Epoch 107/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3561 - acc: 0.8432 - val_loss: 0.4848 - val_acc: 0.7683\n",
      "Epoch 108/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3616 - acc: 0.8377 - val_loss: 0.4490 - val_acc: 0.7862\n",
      "Epoch 109/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3596 - acc: 0.8400 - val_loss: 0.4464 - val_acc: 0.7874\n",
      "Epoch 110/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3555 - acc: 0.8430 - val_loss: 0.4443 - val_acc: 0.7891\n",
      "Epoch 111/200\n",
      "17142/17142 [==============================] - 1s 55us/step - loss: 0.3576 - acc: 0.8417 - val_loss: 0.4553 - val_acc: 0.7813\n",
      "Epoch 112/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3580 - acc: 0.8409 - val_loss: 0.4487 - val_acc: 0.7865\n",
      "Epoch 113/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3537 - acc: 0.8440 - val_loss: 0.4455 - val_acc: 0.7881\n",
      "Epoch 114/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3551 - acc: 0.8431 - val_loss: 0.4560 - val_acc: 0.7820\n",
      "Epoch 115/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3543 - acc: 0.8430 - val_loss: 0.4467 - val_acc: 0.7869\n",
      "Epoch 116/200\n",
      "17142/17142 [==============================] - 1s 56us/step - loss: 0.3523 - acc: 0.8455 - val_loss: 0.4478 - val_acc: 0.7863\n",
      "auroc: 0.8431500339919725\n",
      "auprc: 0.7444148584127612\n",
      "auroc: 0.842086821273672\n",
      "auprc: 0.742563848919927\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 6s 182us/step - loss: 0.7447 - acc: 0.6078 - val_loss: 0.6846 - val_acc: 0.6334\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.6682 - acc: 0.6341 - val_loss: 0.6712 - val_acc: 0.6386\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6552 - acc: 0.6419 - val_loss: 0.6701 - val_acc: 0.6397\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6461 - acc: 0.6487 - val_loss: 0.6626 - val_acc: 0.6338\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6339 - acc: 0.6577 - val_loss: 0.6452 - val_acc: 0.6496\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6173 - acc: 0.6684 - val_loss: 0.6306 - val_acc: 0.6672\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5973 - acc: 0.6839 - val_loss: 0.6072 - val_acc: 0.6898\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5787 - acc: 0.6974 - val_loss: 0.5880 - val_acc: 0.7005\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5603 - acc: 0.7120 - val_loss: 0.5809 - val_acc: 0.7009\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.5467 - acc: 0.7213 - val_loss: 0.5648 - val_acc: 0.7185\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5338 - acc: 0.7310 - val_loss: 0.5413 - val_acc: 0.7210\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5217 - acc: 0.7387 - val_loss: 0.5317 - val_acc: 0.7315\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5147 - acc: 0.7433 - val_loss: 0.5208 - val_acc: 0.7377\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5010 - acc: 0.7528 - val_loss: 0.5148 - val_acc: 0.7417\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4941 - acc: 0.7570 - val_loss: 0.5030 - val_acc: 0.7498\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4854 - acc: 0.7629 - val_loss: 0.5035 - val_acc: 0.7490\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4768 - acc: 0.7691 - val_loss: 0.4920 - val_acc: 0.7575\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4740 - acc: 0.7697 - val_loss: 0.4826 - val_acc: 0.7625\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4659 - acc: 0.7744 - val_loss: 0.4810 - val_acc: 0.7626\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4617 - acc: 0.7775 - val_loss: 0.4746 - val_acc: 0.7687\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4545 - acc: 0.7847 - val_loss: 0.4669 - val_acc: 0.7739\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4502 - acc: 0.7855 - val_loss: 0.4807 - val_acc: 0.7647\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4472 - acc: 0.7875 - val_loss: 0.4616 - val_acc: 0.7767\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4425 - acc: 0.7906 - val_loss: 0.4573 - val_acc: 0.7806\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4367 - acc: 0.7953 - val_loss: 0.4600 - val_acc: 0.7780\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4350 - acc: 0.7961 - val_loss: 0.4561 - val_acc: 0.7818\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4308 - acc: 0.7986 - val_loss: 0.4511 - val_acc: 0.7825\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4273 - acc: 0.8012 - val_loss: 0.4556 - val_acc: 0.7809\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4255 - acc: 0.8011 - val_loss: 0.4522 - val_acc: 0.7823\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4216 - acc: 0.8043 - val_loss: 0.4420 - val_acc: 0.7899\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4183 - acc: 0.8062 - val_loss: 0.4514 - val_acc: 0.7834\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.4163 - acc: 0.8074 - val_loss: 0.4404 - val_acc: 0.7914\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4170 - acc: 0.8071 - val_loss: 0.4434 - val_acc: 0.7898\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4171 - acc: 0.8068 - val_loss: 0.4413 - val_acc: 0.7898\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4113 - acc: 0.8101 - val_loss: 0.4607 - val_acc: 0.7783\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4083 - acc: 0.8126 - val_loss: 0.4386 - val_acc: 0.7922\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4075 - acc: 0.8123 - val_loss: 0.4303 - val_acc: 0.7968\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4098 - acc: 0.8117 - val_loss: 0.4306 - val_acc: 0.7958\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4007 - acc: 0.8173 - val_loss: 0.4281 - val_acc: 0.7988\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4015 - acc: 0.8169 - val_loss: 0.4297 - val_acc: 0.7972\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4050 - acc: 0.8140 - val_loss: 0.4287 - val_acc: 0.7964\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4001 - acc: 0.8166 - val_loss: 0.4341 - val_acc: 0.7946\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3960 - acc: 0.8193 - val_loss: 0.4235 - val_acc: 0.8004\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3935 - acc: 0.8218 - val_loss: 0.4255 - val_acc: 0.8007\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3913 - acc: 0.8238 - val_loss: 0.4239 - val_acc: 0.8003\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3922 - acc: 0.8222 - val_loss: 0.4245 - val_acc: 0.8004\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3902 - acc: 0.8221 - val_loss: 0.4289 - val_acc: 0.7985\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3916 - acc: 0.8213 - val_loss: 0.4222 - val_acc: 0.8007\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3868 - acc: 0.8249 - val_loss: 0.4207 - val_acc: 0.8021\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3874 - acc: 0.8244 - val_loss: 0.4205 - val_acc: 0.8021\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3853 - acc: 0.8254 - val_loss: 0.4286 - val_acc: 0.7986\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3857 - acc: 0.8254 - val_loss: 0.4194 - val_acc: 0.8032\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3832 - acc: 0.8261 - val_loss: 0.4208 - val_acc: 0.8028\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3832 - acc: 0.8261 - val_loss: 0.4147 - val_acc: 0.8049\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3818 - acc: 0.8265 - val_loss: 0.4164 - val_acc: 0.8042\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3814 - acc: 0.8262 - val_loss: 0.4193 - val_acc: 0.8030\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3837 - acc: 0.8262 - val_loss: 0.4214 - val_acc: 0.8010\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3773 - acc: 0.8295 - val_loss: 0.4146 - val_acc: 0.8058\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3796 - acc: 0.8277 - val_loss: 0.4120 - val_acc: 0.8085\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3768 - acc: 0.8296 - val_loss: 0.4147 - val_acc: 0.8061\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3761 - acc: 0.8297 - val_loss: 0.4133 - val_acc: 0.8069\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3749 - acc: 0.8312 - val_loss: 0.4225 - val_acc: 0.8001\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3742 - acc: 0.8303 - val_loss: 0.4127 - val_acc: 0.8079\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3755 - acc: 0.8293 - val_loss: 0.4117 - val_acc: 0.8090\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3728 - acc: 0.8318 - val_loss: 0.4153 - val_acc: 0.8056\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3738 - acc: 0.8313 - val_loss: 0.4166 - val_acc: 0.8056\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3707 - acc: 0.8327 - val_loss: 0.4081 - val_acc: 0.8104\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3709 - acc: 0.8324 - val_loss: 0.4116 - val_acc: 0.8089\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3716 - acc: 0.8320 - val_loss: 0.4078 - val_acc: 0.8101\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3696 - acc: 0.8332 - val_loss: 0.4093 - val_acc: 0.8103\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3728 - acc: 0.8306 - val_loss: 0.4197 - val_acc: 0.8037\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 2s 46us/step - loss: 0.3692 - acc: 0.8338 - val_loss: 0.4124 - val_acc: 0.8087\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3665 - acc: 0.8356 - val_loss: 0.4122 - val_acc: 0.8090\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3670 - acc: 0.8348 - val_loss: 0.4137 - val_acc: 0.8071\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3649 - acc: 0.8366 - val_loss: 0.4091 - val_acc: 0.8103\n",
      "Epoch 76/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3649 - acc: 0.8363 - val_loss: 0.4123 - val_acc: 0.8088\n",
      "Epoch 77/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3643 - acc: 0.8356 - val_loss: 0.4058 - val_acc: 0.8124\n",
      "Epoch 78/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3638 - acc: 0.8366 - val_loss: 0.4070 - val_acc: 0.8107\n",
      "Epoch 79/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3622 - acc: 0.8378 - val_loss: 0.4040 - val_acc: 0.8123\n",
      "Epoch 80/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3618 - acc: 0.8379 - val_loss: 0.4228 - val_acc: 0.8043\n",
      "Epoch 81/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3640 - acc: 0.8363 - val_loss: 0.4132 - val_acc: 0.8080\n",
      "Epoch 82/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3624 - acc: 0.8375 - val_loss: 0.4075 - val_acc: 0.8111\n",
      "Epoch 83/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3616 - acc: 0.8380 - val_loss: 0.4049 - val_acc: 0.8129\n",
      "Epoch 84/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3603 - acc: 0.8387 - val_loss: 0.4066 - val_acc: 0.8104\n",
      "Epoch 85/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3607 - acc: 0.8382 - val_loss: 0.4039 - val_acc: 0.8134\n",
      "Epoch 86/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3594 - acc: 0.8397 - val_loss: 0.4032 - val_acc: 0.8149\n",
      "Epoch 87/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3585 - acc: 0.8395 - val_loss: 0.4058 - val_acc: 0.8136\n",
      "Epoch 88/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3584 - acc: 0.8402 - val_loss: 0.4047 - val_acc: 0.8120\n",
      "Epoch 89/200\n",
      "  500/34284 [..............................] - ETA: 1s - loss: 0.3546 - acc: 0.8473"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3823 - acc: 0.8279 - val_loss: 0.4203 - val_acc: 0.8038\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3825 - acc: 0.8275 - val_loss: 0.4200 - val_acc: 0.8040\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3815 - acc: 0.8280 - val_loss: 0.4168 - val_acc: 0.8053\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3805 - acc: 0.8278 - val_loss: 0.4170 - val_acc: 0.8050\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3801 - acc: 0.8285 - val_loss: 0.4218 - val_acc: 0.8029\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3792 - acc: 0.8284 - val_loss: 0.4265 - val_acc: 0.7995\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3800 - acc: 0.8286 - val_loss: 0.4199 - val_acc: 0.8029\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3782 - acc: 0.8290 - val_loss: 0.4209 - val_acc: 0.8031\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3759 - acc: 0.8302 - val_loss: 0.4141 - val_acc: 0.8078\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3785 - acc: 0.8281 - val_loss: 0.4163 - val_acc: 0.8059\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3759 - acc: 0.8303 - val_loss: 0.4160 - val_acc: 0.8061\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3755 - acc: 0.8306 - val_loss: 0.4319 - val_acc: 0.7975\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3739 - acc: 0.8305 - val_loss: 0.4153 - val_acc: 0.8071\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3733 - acc: 0.8320 - val_loss: 0.4151 - val_acc: 0.8066\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3722 - acc: 0.8317 - val_loss: 0.4194 - val_acc: 0.8037\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3707 - acc: 0.8326 - val_loss: 0.4150 - val_acc: 0.8066\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3762 - acc: 0.8298 - val_loss: 0.4197 - val_acc: 0.8036\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3705 - acc: 0.8338 - val_loss: 0.4275 - val_acc: 0.7991\n",
      "Epoch 70/200\n",
      "34284/34284 [==============================] - 2s 53us/step - loss: 0.3684 - acc: 0.8342 - val_loss: 0.4132 - val_acc: 0.8087\n",
      "Epoch 71/200\n",
      "34284/34284 [==============================] - 2s 53us/step - loss: 0.3728 - acc: 0.8313 - val_loss: 0.4229 - val_acc: 0.8018\n",
      "Epoch 72/200\n",
      "34284/34284 [==============================] - 2s 53us/step - loss: 0.3702 - acc: 0.8338 - val_loss: 0.4123 - val_acc: 0.8084\n",
      "Epoch 73/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3674 - acc: 0.8345 - val_loss: 0.4159 - val_acc: 0.8066\n",
      "Epoch 74/200\n",
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3681 - acc: 0.8341 - val_loss: 0.4120 - val_acc: 0.8084\n",
      "Epoch 75/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3672 - acc: 0.8345 - val_loss: 0.4148 - val_acc: 0.8066\n",
      "Epoch 76/200\n",
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3655 - acc: 0.8359 - val_loss: 0.4153 - val_acc: 0.8053\n",
      "Epoch 77/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3663 - acc: 0.8359 - val_loss: 0.4128 - val_acc: 0.8082\n",
      "Epoch 78/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3663 - acc: 0.8356 - val_loss: 0.4155 - val_acc: 0.8064\n",
      "Epoch 79/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3652 - acc: 0.8359 - val_loss: 0.4111 - val_acc: 0.8098\n",
      "Epoch 80/200\n",
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3645 - acc: 0.8368 - val_loss: 0.4160 - val_acc: 0.8071\n",
      "Epoch 81/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3625 - acc: 0.8378 - val_loss: 0.4119 - val_acc: 0.8085\n",
      "Epoch 82/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3620 - acc: 0.8389 - val_loss: 0.4265 - val_acc: 0.8007\n",
      "Epoch 83/200\n",
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3662 - acc: 0.8346 - val_loss: 0.4359 - val_acc: 0.7958\n",
      "Epoch 84/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3660 - acc: 0.8354 - val_loss: 0.4118 - val_acc: 0.8081\n",
      "Epoch 85/200\n",
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3636 - acc: 0.8362 - val_loss: 0.4186 - val_acc: 0.8046\n",
      "Epoch 86/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3609 - acc: 0.8392 - val_loss: 0.4232 - val_acc: 0.8024\n",
      "Epoch 87/200\n",
      "34284/34284 [==============================] - 2s 52us/step - loss: 0.3620 - acc: 0.8386 - val_loss: 0.4229 - val_acc: 0.8037\n",
      "Epoch 88/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3614 - acc: 0.8388 - val_loss: 0.4146 - val_acc: 0.8078\n",
      "Epoch 89/200\n",
      "34284/34284 [==============================] - 2s 51us/step - loss: 0.3616 - acc: 0.8375 - val_loss: 0.4188 - val_acc: 0.8063\n",
      "auroc: 0.8689724848273729\n",
      "auprc: 0.7875459408562144\n",
      "auroc: 0.8675836464534935\n",
      "auprc: 0.7853724794444563\n",
      "Train on 34284 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "34284/34284 [==============================] - 7s 200us/step - loss: 0.6916 - acc: 0.6192 - val_loss: 0.6860 - val_acc: 0.6317\n",
      "Epoch 2/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.6563 - acc: 0.6414 - val_loss: 0.6703 - val_acc: 0.6406\n",
      "Epoch 3/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6375 - acc: 0.6547 - val_loss: 0.6586 - val_acc: 0.6353\n",
      "Epoch 4/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6217 - acc: 0.6647 - val_loss: 0.6450 - val_acc: 0.6439\n",
      "Epoch 5/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.6022 - acc: 0.6793 - val_loss: 0.6230 - val_acc: 0.6710\n",
      "Epoch 6/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5837 - acc: 0.6930 - val_loss: 0.6063 - val_acc: 0.6752\n",
      "Epoch 7/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5671 - acc: 0.7062 - val_loss: 0.5960 - val_acc: 0.6895\n",
      "Epoch 8/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5477 - acc: 0.7203 - val_loss: 0.5783 - val_acc: 0.6990\n",
      "Epoch 9/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.5317 - acc: 0.7307 - val_loss: 0.5734 - val_acc: 0.7072\n",
      "Epoch 10/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5214 - acc: 0.7381 - val_loss: 0.5518 - val_acc: 0.7152\n",
      "Epoch 11/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.5072 - acc: 0.7466 - val_loss: 0.5404 - val_acc: 0.7183\n",
      "Epoch 12/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4961 - acc: 0.7543 - val_loss: 0.5324 - val_acc: 0.7249\n",
      "Epoch 13/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4888 - acc: 0.7574 - val_loss: 0.5226 - val_acc: 0.7325\n",
      "Epoch 14/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4784 - acc: 0.7665 - val_loss: 0.5272 - val_acc: 0.7312\n",
      "Epoch 15/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4724 - acc: 0.7720 - val_loss: 0.5125 - val_acc: 0.7357\n",
      "Epoch 16/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4686 - acc: 0.7727 - val_loss: 0.5060 - val_acc: 0.7438\n",
      "Epoch 17/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4583 - acc: 0.7791 - val_loss: 0.5012 - val_acc: 0.7506\n",
      "Epoch 18/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4526 - acc: 0.7847 - val_loss: 0.5030 - val_acc: 0.7469\n",
      "Epoch 19/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4481 - acc: 0.7875 - val_loss: 0.4939 - val_acc: 0.7542\n",
      "Epoch 20/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4413 - acc: 0.7924 - val_loss: 0.4932 - val_acc: 0.7557\n",
      "Epoch 21/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4378 - acc: 0.7944 - val_loss: 0.4802 - val_acc: 0.7618\n",
      "Epoch 22/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4318 - acc: 0.7986 - val_loss: 0.4874 - val_acc: 0.7566\n",
      "Epoch 23/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4284 - acc: 0.8008 - val_loss: 0.4804 - val_acc: 0.7647\n",
      "Epoch 24/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.4228 - acc: 0.8047 - val_loss: 0.4786 - val_acc: 0.7619\n",
      "Epoch 25/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4184 - acc: 0.8087 - val_loss: 0.4736 - val_acc: 0.7684\n",
      "Epoch 26/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4142 - acc: 0.8106 - val_loss: 0.4696 - val_acc: 0.7710\n",
      "Epoch 27/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4124 - acc: 0.8111 - val_loss: 0.4705 - val_acc: 0.7697\n",
      "Epoch 28/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4152 - acc: 0.8086 - val_loss: 0.4622 - val_acc: 0.7755\n",
      "Epoch 29/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.4069 - acc: 0.8152 - val_loss: 0.4647 - val_acc: 0.7747\n",
      "Epoch 30/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4039 - acc: 0.8169 - val_loss: 0.4630 - val_acc: 0.7760\n",
      "Epoch 31/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.4025 - acc: 0.8167 - val_loss: 0.4588 - val_acc: 0.7800\n",
      "Epoch 32/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3985 - acc: 0.8193 - val_loss: 0.4669 - val_acc: 0.7717\n",
      "Epoch 33/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3967 - acc: 0.8194 - val_loss: 0.4567 - val_acc: 0.7813\n",
      "Epoch 34/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3932 - acc: 0.8221 - val_loss: 0.4666 - val_acc: 0.7713\n",
      "Epoch 35/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3918 - acc: 0.8222 - val_loss: 0.4564 - val_acc: 0.7792\n",
      "Epoch 36/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3913 - acc: 0.8221 - val_loss: 0.4575 - val_acc: 0.7790\n",
      "Epoch 37/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3907 - acc: 0.8226 - val_loss: 0.4612 - val_acc: 0.7771\n",
      "Epoch 38/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3905 - acc: 0.8225 - val_loss: 0.4578 - val_acc: 0.7794\n",
      "Epoch 39/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3858 - acc: 0.8258 - val_loss: 0.4516 - val_acc: 0.7836\n",
      "Epoch 40/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3828 - acc: 0.8280 - val_loss: 0.4518 - val_acc: 0.7839\n",
      "Epoch 41/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3807 - acc: 0.8283 - val_loss: 0.4733 - val_acc: 0.7699\n",
      "Epoch 42/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3812 - acc: 0.8281 - val_loss: 0.4505 - val_acc: 0.7852\n",
      "Epoch 43/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3785 - acc: 0.8290 - val_loss: 0.4596 - val_acc: 0.7767\n",
      "Epoch 44/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3777 - acc: 0.8310 - val_loss: 0.4542 - val_acc: 0.7827\n",
      "Epoch 45/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3773 - acc: 0.8293 - val_loss: 0.4537 - val_acc: 0.7822\n",
      "Epoch 46/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3762 - acc: 0.8312 - val_loss: 0.4673 - val_acc: 0.7751\n",
      "Epoch 47/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3747 - acc: 0.8313 - val_loss: 0.4525 - val_acc: 0.7828\n",
      "Epoch 48/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3742 - acc: 0.8309 - val_loss: 0.4527 - val_acc: 0.7832\n",
      "Epoch 49/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3723 - acc: 0.8327 - val_loss: 0.4545 - val_acc: 0.7833\n",
      "Epoch 50/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3677 - acc: 0.8351 - val_loss: 0.4493 - val_acc: 0.7848\n",
      "Epoch 51/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3670 - acc: 0.8361 - val_loss: 0.4597 - val_acc: 0.7814\n",
      "Epoch 52/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3683 - acc: 0.8344 - val_loss: 0.4612 - val_acc: 0.7803\n",
      "Epoch 53/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3661 - acc: 0.8356 - val_loss: 0.4525 - val_acc: 0.7849\n",
      "Epoch 54/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3672 - acc: 0.8353 - val_loss: 0.4613 - val_acc: 0.7790\n",
      "Epoch 55/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3632 - acc: 0.8372 - val_loss: 0.4507 - val_acc: 0.7851\n",
      "Epoch 56/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3609 - acc: 0.8388 - val_loss: 0.4482 - val_acc: 0.7857\n",
      "Epoch 57/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3616 - acc: 0.8382 - val_loss: 0.4526 - val_acc: 0.7853\n",
      "Epoch 58/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3670 - acc: 0.8339 - val_loss: 0.4855 - val_acc: 0.7682\n",
      "Epoch 59/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3623 - acc: 0.8374 - val_loss: 0.4479 - val_acc: 0.7878\n",
      "Epoch 60/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3586 - acc: 0.8397 - val_loss: 0.4525 - val_acc: 0.7849\n",
      "Epoch 61/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3595 - acc: 0.8379 - val_loss: 0.4520 - val_acc: 0.7857\n",
      "Epoch 62/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3571 - acc: 0.8401 - val_loss: 0.4511 - val_acc: 0.7879\n",
      "Epoch 63/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3547 - acc: 0.8421 - val_loss: 0.4506 - val_acc: 0.7868\n",
      "Epoch 64/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3559 - acc: 0.8410 - val_loss: 0.4540 - val_acc: 0.7829\n",
      "Epoch 65/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3548 - acc: 0.8415 - val_loss: 0.4608 - val_acc: 0.7831\n",
      "Epoch 66/200\n",
      "34284/34284 [==============================] - 2s 47us/step - loss: 0.3561 - acc: 0.8410 - val_loss: 0.4656 - val_acc: 0.7771\n",
      "Epoch 67/200\n",
      "34284/34284 [==============================] - 2s 48us/step - loss: 0.3554 - acc: 0.8405 - val_loss: 0.4495 - val_acc: 0.7879\n",
      "Epoch 68/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3563 - acc: 0.8400 - val_loss: 0.4516 - val_acc: 0.7868\n",
      "Epoch 69/200\n",
      "34284/34284 [==============================] - 2s 49us/step - loss: 0.3528 - acc: 0.8420 - val_loss: 0.4557 - val_acc: 0.7837\n",
      "auroc: 0.8398918020795193\n",
      "auprc: 0.740759722611463\n",
      "auroc: 0.8397321332272858\n",
      "auprc: 0.7402510034770851\n"
     ]
    }
   ],
   "source": [
    "for curr_seed in all_seeds: \n",
    "    model, early_stopping_callback, auroc_callback = train_model(model_wrapper = REG_WRAPPER, \n",
    "                                                                 aug = None, \n",
    "                                                                 curr_seed = curr_seed, \n",
    "                                                                 batch_size = 500,\n",
    "                                                                 x = x_train_1000, \n",
    "                                                                 y = y_train_1000, \n",
    "                                                                 val_data = (x_val_1000, y_val_1000))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"Standard_1000_auROC\", curr_seed = curr_seed,\n",
    "             callback = auroc_callback, model = model, val_data = (x_val_1000, y_val_1000))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"Standard_1000_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_val_1000, y_val_1000))\n",
    "    \n",
    "    model, early_stopping_callback, auroc_callback = train_model(model_wrapper = REG_WRAPPER, \n",
    "                                                                 aug = \"rev_after_each\", \n",
    "                                                                 curr_seed = curr_seed, \n",
    "                                                                 batch_size = 500,\n",
    "                                                                 x = x_train_1000, \n",
    "                                                                 y = y_train_1000, \n",
    "                                                                 val_data = (x_val_1000, y_val_1000))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"Aug_1000_auROC\", curr_seed = curr_seed,\n",
    "             callback = auroc_callback, model = model, val_data = (x_val_1000, y_val_1000))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"Aug_1000_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_val_1000, y_val_1000))\n",
    "    \n",
    "    model, early_stopping_callback, auroc_callback = train_model(model_wrapper = REG_WRAPPER, \n",
    "                                                                 aug = \"rev_after_all\", \n",
    "                                                                 curr_seed = curr_seed, \n",
    "                                                                 batch_size = 500,\n",
    "                                                                 x = x_train_1000, \n",
    "                                                                 y = y_train_1000, \n",
    "                                                                 val_data = (x_val_1000, y_val_1000))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"Aug_Alt_1000_auROC\", curr_seed = curr_seed,\n",
    "             callback = auroc_callback, model = model, val_data = (x_val_1000, y_val_1000))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"Aug_Alt_1000_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_val_1000, y_val_1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 7s 407us/step - loss: 0.7456 - acc: 0.6061 - val_loss: 0.6687 - val_acc: 0.6595\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.6530 - acc: 0.6487 - val_loss: 0.6540 - val_acc: 0.6437\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.6429 - acc: 0.6543 - val_loss: 0.6501 - val_acc: 0.6588\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.6327 - acc: 0.6611 - val_loss: 0.6453 - val_acc: 0.6488\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.6233 - acc: 0.6671 - val_loss: 0.6396 - val_acc: 0.6468\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.6143 - acc: 0.6708 - val_loss: 0.6285 - val_acc: 0.6645\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.6029 - acc: 0.6795 - val_loss: 0.6186 - val_acc: 0.6727\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.5955 - acc: 0.6854 - val_loss: 0.6113 - val_acc: 0.6758\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.5870 - acc: 0.6911 - val_loss: 0.5998 - val_acc: 0.6789\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.5752 - acc: 0.7000 - val_loss: 0.5908 - val_acc: 0.6851\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.5691 - acc: 0.7054 - val_loss: 0.5869 - val_acc: 0.6934\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.5594 - acc: 0.7143 - val_loss: 0.5865 - val_acc: 0.6753\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.5505 - acc: 0.7197 - val_loss: 0.5683 - val_acc: 0.7020\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.5398 - acc: 0.7281 - val_loss: 0.5567 - val_acc: 0.7060\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.5307 - acc: 0.7353 - val_loss: 0.5483 - val_acc: 0.7179\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.5227 - acc: 0.7411 - val_loss: 0.5556 - val_acc: 0.7091\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.5168 - acc: 0.7446 - val_loss: 0.5295 - val_acc: 0.7351\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.5108 - acc: 0.7479 - val_loss: 0.5232 - val_acc: 0.7350\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4989 - acc: 0.7577 - val_loss: 0.5218 - val_acc: 0.7355\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4955 - acc: 0.7610 - val_loss: 0.5319 - val_acc: 0.7303\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4927 - acc: 0.7613 - val_loss: 0.5133 - val_acc: 0.7410\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4813 - acc: 0.7695 - val_loss: 0.5026 - val_acc: 0.7497\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.4788 - acc: 0.7719 - val_loss: 0.4953 - val_acc: 0.7574\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4703 - acc: 0.7774 - val_loss: 0.4901 - val_acc: 0.7578\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.4665 - acc: 0.7804 - val_loss: 0.4851 - val_acc: 0.7632\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4637 - acc: 0.7815 - val_loss: 0.4862 - val_acc: 0.7610\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4581 - acc: 0.7858 - val_loss: 0.4820 - val_acc: 0.7658\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.4556 - acc: 0.7872 - val_loss: 0.4807 - val_acc: 0.7644\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4518 - acc: 0.7901 - val_loss: 0.4742 - val_acc: 0.7697\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4506 - acc: 0.7891 - val_loss: 0.4714 - val_acc: 0.7707\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4474 - acc: 0.7917 - val_loss: 0.4694 - val_acc: 0.7725\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4414 - acc: 0.7952 - val_loss: 0.4633 - val_acc: 0.7791\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4349 - acc: 0.8023 - val_loss: 0.4672 - val_acc: 0.7723\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4373 - acc: 0.7995 - val_loss: 0.4609 - val_acc: 0.7768\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.4352 - acc: 0.8002 - val_loss: 0.4631 - val_acc: 0.7761\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4286 - acc: 0.8057 - val_loss: 0.4553 - val_acc: 0.7837\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.4257 - acc: 0.8085 - val_loss: 0.4645 - val_acc: 0.7762\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.4230 - acc: 0.8092 - val_loss: 0.4498 - val_acc: 0.7868\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4222 - acc: 0.8094 - val_loss: 0.4518 - val_acc: 0.7845\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.4225 - acc: 0.8085 - val_loss: 0.4610 - val_acc: 0.7783\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4200 - acc: 0.8090 - val_loss: 0.4447 - val_acc: 0.7901\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.4149 - acc: 0.8131 - val_loss: 0.4426 - val_acc: 0.7903\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4115 - acc: 0.8158 - val_loss: 0.4430 - val_acc: 0.7909\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.4128 - acc: 0.8135 - val_loss: 0.4515 - val_acc: 0.7825\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4080 - acc: 0.8174 - val_loss: 0.4382 - val_acc: 0.7942\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4114 - acc: 0.8141 - val_loss: 0.4502 - val_acc: 0.7845\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.4078 - acc: 0.8183 - val_loss: 0.4487 - val_acc: 0.7838\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.4021 - acc: 0.8212 - val_loss: 0.4345 - val_acc: 0.7954\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.4013 - acc: 0.8217 - val_loss: 0.4343 - val_acc: 0.7949\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3997 - acc: 0.8217 - val_loss: 0.4334 - val_acc: 0.7958\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3979 - acc: 0.8221 - val_loss: 0.4358 - val_acc: 0.7935\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3987 - acc: 0.8204 - val_loss: 0.4354 - val_acc: 0.7942\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3947 - acc: 0.8235 - val_loss: 0.4371 - val_acc: 0.7918\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3921 - acc: 0.8258 - val_loss: 0.4364 - val_acc: 0.7936\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3905 - acc: 0.8263 - val_loss: 0.4383 - val_acc: 0.7928\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3905 - acc: 0.8264 - val_loss: 0.4256 - val_acc: 0.8002\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3893 - acc: 0.8282 - val_loss: 0.4282 - val_acc: 0.7981\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.3895 - acc: 0.8284 - val_loss: 0.4524 - val_acc: 0.7854\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3862 - acc: 0.8296 - val_loss: 0.4300 - val_acc: 0.7968\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3835 - acc: 0.8316 - val_loss: 0.4349 - val_acc: 0.7946\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3817 - acc: 0.8331 - val_loss: 0.4219 - val_acc: 0.8008\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3840 - acc: 0.8306 - val_loss: 0.4248 - val_acc: 0.7997\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3835 - acc: 0.8292 - val_loss: 0.4237 - val_acc: 0.8004\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3804 - acc: 0.8320 - val_loss: 0.4241 - val_acc: 0.7996\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3775 - acc: 0.8343 - val_loss: 0.4172 - val_acc: 0.8039\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3751 - acc: 0.8354 - val_loss: 0.4167 - val_acc: 0.8038\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3745 - acc: 0.8354 - val_loss: 0.4283 - val_acc: 0.7954\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3747 - acc: 0.8355 - val_loss: 0.4243 - val_acc: 0.7993\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3768 - acc: 0.8328 - val_loss: 0.4147 - val_acc: 0.8054\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3698 - acc: 0.8387 - val_loss: 0.4181 - val_acc: 0.8038\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.3729 - acc: 0.8369 - val_loss: 0.4174 - val_acc: 0.8036\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3675 - acc: 0.8397 - val_loss: 0.4132 - val_acc: 0.8069\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3691 - acc: 0.8394 - val_loss: 0.4124 - val_acc: 0.8068\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3661 - acc: 0.8416 - val_loss: 0.4263 - val_acc: 0.8000\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3682 - acc: 0.8383 - val_loss: 0.4177 - val_acc: 0.8039\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.3668 - acc: 0.8384 - val_loss: 0.4145 - val_acc: 0.8056\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3672 - acc: 0.8386 - val_loss: 0.4588 - val_acc: 0.7829\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3655 - acc: 0.8401 - val_loss: 0.4336 - val_acc: 0.7914\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3644 - acc: 0.8412 - val_loss: 0.4209 - val_acc: 0.7998\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3609 - acc: 0.8434 - val_loss: 0.4191 - val_acc: 0.8027\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3611 - acc: 0.8425 - val_loss: 0.4078 - val_acc: 0.8106\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3584 - acc: 0.8452 - val_loss: 0.4112 - val_acc: 0.8063\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3573 - acc: 0.8456 - val_loss: 0.4068 - val_acc: 0.8095\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3581 - acc: 0.8442 - val_loss: 0.4230 - val_acc: 0.7991\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3577 - acc: 0.8447 - val_loss: 0.4280 - val_acc: 0.7969\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3573 - acc: 0.8447 - val_loss: 0.4080 - val_acc: 0.8079\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3558 - acc: 0.8462 - val_loss: 0.4070 - val_acc: 0.8096\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3572 - acc: 0.8435 - val_loss: 0.4216 - val_acc: 0.8019\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3532 - acc: 0.8473 - val_loss: 0.4165 - val_acc: 0.8042\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3599 - acc: 0.8407 - val_loss: 0.4082 - val_acc: 0.8076\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3514 - acc: 0.8481 - val_loss: 0.4062 - val_acc: 0.8101\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3495 - acc: 0.8491 - val_loss: 0.4149 - val_acc: 0.8056\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3543 - acc: 0.8469 - val_loss: 0.4117 - val_acc: 0.8063\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.3505 - acc: 0.8488 - val_loss: 0.4092 - val_acc: 0.8079\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3560 - acc: 0.8423 - val_loss: 0.4080 - val_acc: 0.8085\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3483 - acc: 0.8499 - val_loss: 0.4046 - val_acc: 0.8112\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3515 - acc: 0.8476 - val_loss: 0.4051 - val_acc: 0.8110\n",
      "Epoch 98/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3479 - acc: 0.8495 - val_loss: 0.3992 - val_acc: 0.8141\n",
      "Epoch 99/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3445 - acc: 0.8509 - val_loss: 0.4105 - val_acc: 0.8070\n",
      "Epoch 100/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3444 - acc: 0.8516 - val_loss: 0.4051 - val_acc: 0.8118\n",
      "Epoch 101/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3455 - acc: 0.8503 - val_loss: 0.4152 - val_acc: 0.8044\n",
      "Epoch 102/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3424 - acc: 0.8526 - val_loss: 0.3980 - val_acc: 0.8144\n",
      "Epoch 103/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3399 - acc: 0.8554 - val_loss: 0.4015 - val_acc: 0.8108\n",
      "Epoch 104/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3444 - acc: 0.8503 - val_loss: 0.4002 - val_acc: 0.8129\n",
      "Epoch 105/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3408 - acc: 0.8536 - val_loss: 0.3993 - val_acc: 0.8135\n",
      "Epoch 106/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3386 - acc: 0.8553 - val_loss: 0.3981 - val_acc: 0.8142\n",
      "Epoch 107/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3406 - acc: 0.8530 - val_loss: 0.3961 - val_acc: 0.8150\n",
      "Epoch 108/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3378 - acc: 0.8544 - val_loss: 0.3965 - val_acc: 0.8146\n",
      "Epoch 109/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3401 - acc: 0.8531 - val_loss: 0.4027 - val_acc: 0.8113\n",
      "Epoch 110/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3383 - acc: 0.8540 - val_loss: 0.4001 - val_acc: 0.8120\n",
      "Epoch 111/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3426 - acc: 0.8498 - val_loss: 0.4139 - val_acc: 0.8031\n",
      "Epoch 112/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3366 - acc: 0.8550 - val_loss: 0.3990 - val_acc: 0.8129\n",
      "Epoch 113/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.3367 - acc: 0.8563 - val_loss: 0.3955 - val_acc: 0.8157\n",
      "Epoch 114/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3344 - acc: 0.8567 - val_loss: 0.4100 - val_acc: 0.8080\n",
      "Epoch 115/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3356 - acc: 0.8560 - val_loss: 0.3977 - val_acc: 0.8145\n",
      "Epoch 116/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3334 - acc: 0.8570 - val_loss: 0.4008 - val_acc: 0.8102\n",
      "Epoch 117/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3333 - acc: 0.8573 - val_loss: 0.4022 - val_acc: 0.8124\n",
      "Epoch 118/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3333 - acc: 0.8563 - val_loss: 0.3957 - val_acc: 0.8161\n",
      "Epoch 119/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3348 - acc: 0.8568 - val_loss: 0.3929 - val_acc: 0.8159\n",
      "Epoch 120/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3330 - acc: 0.8574 - val_loss: 0.3999 - val_acc: 0.8127\n",
      "Epoch 121/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3316 - acc: 0.8566 - val_loss: 0.3919 - val_acc: 0.8169\n",
      "Epoch 122/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3300 - acc: 0.8596 - val_loss: 0.3922 - val_acc: 0.8165\n",
      "Epoch 123/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3317 - acc: 0.8563 - val_loss: 0.4020 - val_acc: 0.8133\n",
      "Epoch 124/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3303 - acc: 0.8593 - val_loss: 0.4008 - val_acc: 0.8135\n",
      "Epoch 125/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3277 - acc: 0.8593 - val_loss: 0.3910 - val_acc: 0.8176\n",
      "Epoch 126/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3299 - acc: 0.8580 - val_loss: 0.3944 - val_acc: 0.8159\n",
      "Epoch 127/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3266 - acc: 0.8613 - val_loss: 0.4165 - val_acc: 0.8029\n",
      "Epoch 128/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3312 - acc: 0.8570 - val_loss: 0.4019 - val_acc: 0.8141\n",
      "Epoch 129/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3304 - acc: 0.8568 - val_loss: 0.4016 - val_acc: 0.8111\n",
      "Epoch 130/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3297 - acc: 0.8578 - val_loss: 0.4097 - val_acc: 0.8079\n",
      "Epoch 131/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3247 - acc: 0.8622 - val_loss: 0.3918 - val_acc: 0.8174\n",
      "Epoch 132/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3245 - acc: 0.8623 - val_loss: 0.3902 - val_acc: 0.8175\n",
      "Epoch 133/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3238 - acc: 0.8623 - val_loss: 0.3991 - val_acc: 0.8134\n",
      "Epoch 134/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3224 - acc: 0.8631 - val_loss: 0.4110 - val_acc: 0.8073\n",
      "Epoch 135/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3225 - acc: 0.8644 - val_loss: 0.4007 - val_acc: 0.8134\n",
      "Epoch 136/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3215 - acc: 0.8644 - val_loss: 0.3975 - val_acc: 0.8140\n",
      "Epoch 137/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3256 - acc: 0.8601 - val_loss: 0.3897 - val_acc: 0.8188\n",
      "Epoch 138/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3227 - acc: 0.8622 - val_loss: 0.3896 - val_acc: 0.8185\n",
      "Epoch 139/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3232 - acc: 0.8613 - val_loss: 0.3902 - val_acc: 0.8191\n",
      "Epoch 140/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3197 - acc: 0.8650 - val_loss: 0.3936 - val_acc: 0.8181\n",
      "Epoch 141/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3195 - acc: 0.8640 - val_loss: 0.3926 - val_acc: 0.8172\n",
      "Epoch 142/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3202 - acc: 0.8638 - val_loss: 0.3965 - val_acc: 0.8156\n",
      "Epoch 143/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3184 - acc: 0.8665 - val_loss: 0.3994 - val_acc: 0.8153\n",
      "Epoch 144/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3234 - acc: 0.8613 - val_loss: 0.3880 - val_acc: 0.8193\n",
      "Epoch 145/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3226 - acc: 0.8608 - val_loss: 0.3892 - val_acc: 0.8171\n",
      "Epoch 146/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3190 - acc: 0.8656 - val_loss: 0.3895 - val_acc: 0.8171\n",
      "Epoch 147/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.3184 - acc: 0.8643 - val_loss: 0.3913 - val_acc: 0.8167\n",
      "Epoch 148/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3184 - acc: 0.8645 - val_loss: 0.3887 - val_acc: 0.8192\n",
      "Epoch 149/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3185 - acc: 0.8637 - val_loss: 0.3882 - val_acc: 0.8178\n",
      "Epoch 150/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3148 - acc: 0.8671 - val_loss: 0.3911 - val_acc: 0.8173\n",
      "Epoch 151/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3163 - acc: 0.8654 - val_loss: 0.3883 - val_acc: 0.8199\n",
      "Epoch 152/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3179 - acc: 0.8639 - val_loss: 0.3948 - val_acc: 0.8167\n",
      "Epoch 153/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3197 - acc: 0.8614 - val_loss: 0.3863 - val_acc: 0.8209\n",
      "Epoch 154/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3164 - acc: 0.8653 - val_loss: 0.3884 - val_acc: 0.8192\n",
      "Epoch 155/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3141 - acc: 0.8674 - val_loss: 0.3869 - val_acc: 0.8194\n",
      "Epoch 156/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3141 - acc: 0.8677 - val_loss: 0.3909 - val_acc: 0.8164\n",
      "Epoch 157/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3116 - acc: 0.8686 - val_loss: 0.3854 - val_acc: 0.8208\n",
      "Epoch 158/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.3117 - acc: 0.8687 - val_loss: 0.3926 - val_acc: 0.8180\n",
      "Epoch 159/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3171 - acc: 0.8641 - val_loss: 0.3902 - val_acc: 0.8173\n",
      "Epoch 160/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3118 - acc: 0.8685 - val_loss: 0.3897 - val_acc: 0.8186\n",
      "Epoch 161/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3141 - acc: 0.8671 - val_loss: 0.4099 - val_acc: 0.8105\n",
      "Epoch 162/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3103 - acc: 0.8691 - val_loss: 0.3861 - val_acc: 0.8194\n",
      "Epoch 163/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3106 - acc: 0.8698 - val_loss: 0.3923 - val_acc: 0.8175\n",
      "Epoch 164/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3098 - acc: 0.8692 - val_loss: 0.3856 - val_acc: 0.8194\n",
      "Epoch 165/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3089 - acc: 0.8709 - val_loss: 0.3859 - val_acc: 0.8199\n",
      "Epoch 166/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3129 - acc: 0.8675 - val_loss: 0.3964 - val_acc: 0.8157\n",
      "Epoch 167/200\n",
      "10500/17142 [=================>............] - ETA: 0s - loss: 0.3093 - acc: 0.8688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3297 - acc: 0.8589 - val_loss: 0.4061 - val_acc: 0.8117\n",
      "Epoch 116/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3318 - acc: 0.8559 - val_loss: 0.3942 - val_acc: 0.8168\n",
      "Epoch 117/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3295 - acc: 0.8583 - val_loss: 0.3868 - val_acc: 0.8213\n",
      "Epoch 118/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3269 - acc: 0.8590 - val_loss: 0.3904 - val_acc: 0.8192\n",
      "Epoch 119/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3299 - acc: 0.8580 - val_loss: 0.3923 - val_acc: 0.8195\n",
      "Epoch 120/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3255 - acc: 0.8612 - val_loss: 0.3859 - val_acc: 0.8216\n",
      "Epoch 121/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3245 - acc: 0.8616 - val_loss: 0.3873 - val_acc: 0.8210\n",
      "Epoch 122/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3262 - acc: 0.8596 - val_loss: 0.3861 - val_acc: 0.8215\n",
      "Epoch 123/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3242 - acc: 0.8621 - val_loss: 0.3860 - val_acc: 0.8220\n",
      "Epoch 124/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3237 - acc: 0.8618 - val_loss: 0.3948 - val_acc: 0.8165\n",
      "Epoch 125/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3240 - acc: 0.8619 - val_loss: 0.3877 - val_acc: 0.8203\n",
      "Epoch 126/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3229 - acc: 0.8615 - val_loss: 0.4072 - val_acc: 0.8084\n",
      "Epoch 127/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3232 - acc: 0.8614 - val_loss: 0.3874 - val_acc: 0.8212\n",
      "Epoch 128/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3238 - acc: 0.8612 - val_loss: 0.3843 - val_acc: 0.8217\n",
      "Epoch 129/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3206 - acc: 0.8635 - val_loss: 0.3885 - val_acc: 0.8208\n",
      "Epoch 130/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3267 - acc: 0.8585 - val_loss: 0.4246 - val_acc: 0.7970\n",
      "Epoch 131/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3258 - acc: 0.8585 - val_loss: 0.3902 - val_acc: 0.8185\n",
      "Epoch 132/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3201 - acc: 0.8639 - val_loss: 0.4027 - val_acc: 0.8120\n",
      "Epoch 133/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3208 - acc: 0.8635 - val_loss: 0.3867 - val_acc: 0.8208\n",
      "Epoch 134/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3193 - acc: 0.8643 - val_loss: 0.4030 - val_acc: 0.8107\n",
      "Epoch 135/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3202 - acc: 0.8622 - val_loss: 0.3887 - val_acc: 0.8202\n",
      "Epoch 136/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3240 - acc: 0.8598 - val_loss: 0.3831 - val_acc: 0.8234\n",
      "Epoch 137/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3194 - acc: 0.8615 - val_loss: 0.3813 - val_acc: 0.8238\n",
      "Epoch 138/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3185 - acc: 0.8642 - val_loss: 0.3843 - val_acc: 0.8230\n",
      "Epoch 139/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3186 - acc: 0.8639 - val_loss: 0.3813 - val_acc: 0.8234\n",
      "Epoch 140/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3155 - acc: 0.8647 - val_loss: 0.3834 - val_acc: 0.8227\n",
      "Epoch 141/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3146 - acc: 0.8665 - val_loss: 0.3870 - val_acc: 0.8201\n",
      "Epoch 142/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3183 - acc: 0.8622 - val_loss: 0.3843 - val_acc: 0.8220\n",
      "Epoch 143/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3139 - acc: 0.8664 - val_loss: 0.3850 - val_acc: 0.8206\n",
      "Epoch 144/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3201 - acc: 0.8621 - val_loss: 0.3909 - val_acc: 0.8200\n",
      "Epoch 145/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3154 - acc: 0.8652 - val_loss: 0.3800 - val_acc: 0.8246\n",
      "Epoch 146/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3146 - acc: 0.8647 - val_loss: 0.3948 - val_acc: 0.8171\n",
      "Epoch 147/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3164 - acc: 0.8639 - val_loss: 0.3832 - val_acc: 0.8238\n",
      "Epoch 148/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3222 - acc: 0.8599 - val_loss: 0.4109 - val_acc: 0.8078\n",
      "Epoch 149/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3121 - acc: 0.8663 - val_loss: 0.3919 - val_acc: 0.8196\n",
      "Epoch 150/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3125 - acc: 0.8663 - val_loss: 0.4171 - val_acc: 0.8044\n",
      "Epoch 151/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3222 - acc: 0.8592 - val_loss: 0.3902 - val_acc: 0.8207\n",
      "Epoch 152/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3130 - acc: 0.8658 - val_loss: 0.3917 - val_acc: 0.8196\n",
      "Epoch 153/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3111 - acc: 0.8673 - val_loss: 0.3819 - val_acc: 0.8251\n",
      "Epoch 154/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3072 - acc: 0.8708 - val_loss: 0.3794 - val_acc: 0.8248\n",
      "Epoch 155/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3079 - acc: 0.8707 - val_loss: 0.3800 - val_acc: 0.8243\n",
      "Epoch 156/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3093 - acc: 0.8686 - val_loss: 0.3817 - val_acc: 0.8239\n",
      "Epoch 157/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3086 - acc: 0.8679 - val_loss: 0.3818 - val_acc: 0.8244\n",
      "Epoch 158/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3183 - acc: 0.8613 - val_loss: 0.3797 - val_acc: 0.8252\n",
      "Epoch 159/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3114 - acc: 0.8668 - val_loss: 0.3830 - val_acc: 0.8238\n",
      "Epoch 160/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3092 - acc: 0.8673 - val_loss: 0.3814 - val_acc: 0.8233\n",
      "Epoch 161/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3072 - acc: 0.8694 - val_loss: 0.3775 - val_acc: 0.8262\n",
      "Epoch 162/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3095 - acc: 0.8683 - val_loss: 0.3789 - val_acc: 0.8253\n",
      "Epoch 163/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3059 - acc: 0.8709 - val_loss: 0.3828 - val_acc: 0.8217\n",
      "Epoch 164/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3062 - acc: 0.8696 - val_loss: 0.3792 - val_acc: 0.8248\n",
      "Epoch 165/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3050 - acc: 0.8698 - val_loss: 0.3827 - val_acc: 0.8236\n",
      "Epoch 166/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3051 - acc: 0.8707 - val_loss: 0.3842 - val_acc: 0.8221\n",
      "Epoch 167/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3061 - acc: 0.8687 - val_loss: 0.3856 - val_acc: 0.8217\n",
      "Epoch 168/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3103 - acc: 0.8665 - val_loss: 0.3811 - val_acc: 0.8225\n",
      "Epoch 169/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3037 - acc: 0.8717 - val_loss: 0.3782 - val_acc: 0.8268\n",
      "Epoch 170/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3059 - acc: 0.8707 - val_loss: 0.3806 - val_acc: 0.8254\n",
      "Epoch 171/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3024 - acc: 0.8735 - val_loss: 0.3791 - val_acc: 0.8250\n",
      "auroc: 0.8884025144571998\n",
      "auprc: 0.8178801976002837\n",
      "auroc: 0.8876066631901728\n",
      "auprc: 0.8166257987793261\n",
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 8s 447us/step - loss: 0.7054 - acc: 0.6172 - val_loss: 0.6672 - val_acc: 0.6547\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.6602 - acc: 0.6416 - val_loss: 0.6652 - val_acc: 0.6329\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.6507 - acc: 0.6492 - val_loss: 0.6549 - val_acc: 0.6441\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.6423 - acc: 0.6552 - val_loss: 0.6678 - val_acc: 0.6348\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.6374 - acc: 0.6576 - val_loss: 0.6478 - val_acc: 0.6588\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.6291 - acc: 0.6634 - val_loss: 0.6408 - val_acc: 0.6514\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.6230 - acc: 0.6661 - val_loss: 0.6338 - val_acc: 0.6626\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.6145 - acc: 0.6709 - val_loss: 0.6291 - val_acc: 0.6768\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.6041 - acc: 0.6791 - val_loss: 0.6186 - val_acc: 0.6825\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.5887 - acc: 0.6907 - val_loss: 0.5982 - val_acc: 0.6846\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.5762 - acc: 0.6997 - val_loss: 0.5932 - val_acc: 0.6948\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.5619 - acc: 0.7101 - val_loss: 0.5883 - val_acc: 0.6996\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.5512 - acc: 0.7187 - val_loss: 0.6062 - val_acc: 0.6796\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.5474 - acc: 0.7188 - val_loss: 0.5739 - val_acc: 0.7077\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.5358 - acc: 0.7260 - val_loss: 0.5655 - val_acc: 0.7103\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.5268 - acc: 0.7336 - val_loss: 0.5407 - val_acc: 0.7258\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.5215 - acc: 0.7355 - val_loss: 0.5305 - val_acc: 0.7295\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.5114 - acc: 0.7435 - val_loss: 0.5281 - val_acc: 0.7307\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.5071 - acc: 0.7463 - val_loss: 0.5235 - val_acc: 0.7356\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4996 - acc: 0.7518 - val_loss: 0.5198 - val_acc: 0.7381\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4983 - acc: 0.7523 - val_loss: 0.5083 - val_acc: 0.7438\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4903 - acc: 0.7586 - val_loss: 0.5046 - val_acc: 0.7450\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.4854 - acc: 0.7624 - val_loss: 0.5034 - val_acc: 0.7483\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.4823 - acc: 0.7636 - val_loss: 0.4956 - val_acc: 0.7508\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4774 - acc: 0.7672 - val_loss: 0.4920 - val_acc: 0.7543\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4733 - acc: 0.7707 - val_loss: 0.4898 - val_acc: 0.7554\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4728 - acc: 0.7710 - val_loss: 0.4867 - val_acc: 0.7580\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4650 - acc: 0.7766 - val_loss: 0.4954 - val_acc: 0.7507\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4630 - acc: 0.7785 - val_loss: 0.4979 - val_acc: 0.7479\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4612 - acc: 0.7807 - val_loss: 0.4829 - val_acc: 0.7597\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4551 - acc: 0.7848 - val_loss: 0.4780 - val_acc: 0.7649\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.4543 - acc: 0.7846 - val_loss: 0.4775 - val_acc: 0.7657\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4474 - acc: 0.7897 - val_loss: 0.4729 - val_acc: 0.7664\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4433 - acc: 0.7930 - val_loss: 0.4676 - val_acc: 0.7725\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4412 - acc: 0.7941 - val_loss: 0.4650 - val_acc: 0.7722\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.4420 - acc: 0.7926 - val_loss: 0.4673 - val_acc: 0.7726\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4389 - acc: 0.7952 - val_loss: 0.4571 - val_acc: 0.7792\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4342 - acc: 0.7990 - val_loss: 0.4601 - val_acc: 0.7778\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4312 - acc: 0.8010 - val_loss: 0.4540 - val_acc: 0.7829\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4339 - acc: 0.7978 - val_loss: 0.4572 - val_acc: 0.7803\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4299 - acc: 0.7996 - val_loss: 0.4714 - val_acc: 0.7675\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4268 - acc: 0.8041 - val_loss: 0.4482 - val_acc: 0.7868\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4217 - acc: 0.8062 - val_loss: 0.4521 - val_acc: 0.7844\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4188 - acc: 0.8092 - val_loss: 0.4469 - val_acc: 0.7888\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4172 - acc: 0.8096 - val_loss: 0.4434 - val_acc: 0.7890\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4142 - acc: 0.8131 - val_loss: 0.4424 - val_acc: 0.7895\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4193 - acc: 0.8067 - val_loss: 0.4482 - val_acc: 0.7858\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4105 - acc: 0.8145 - val_loss: 0.4392 - val_acc: 0.7913\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4091 - acc: 0.8140 - val_loss: 0.4422 - val_acc: 0.7902\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4082 - acc: 0.8156 - val_loss: 0.4528 - val_acc: 0.7824\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4052 - acc: 0.8166 - val_loss: 0.4440 - val_acc: 0.7868\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4045 - acc: 0.8182 - val_loss: 0.4357 - val_acc: 0.7923\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.4115 - acc: 0.8123 - val_loss: 0.4401 - val_acc: 0.7885\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4004 - acc: 0.8212 - val_loss: 0.4332 - val_acc: 0.7977\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4001 - acc: 0.8194 - val_loss: 0.4305 - val_acc: 0.7974\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3976 - acc: 0.8213 - val_loss: 0.4481 - val_acc: 0.7818\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3982 - acc: 0.8217 - val_loss: 0.4282 - val_acc: 0.8016\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3959 - acc: 0.8228 - val_loss: 0.4336 - val_acc: 0.7983\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3998 - acc: 0.8191 - val_loss: 0.4272 - val_acc: 0.7974\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3944 - acc: 0.8242 - val_loss: 0.4236 - val_acc: 0.8042\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3934 - acc: 0.8236 - val_loss: 0.4281 - val_acc: 0.8003\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3933 - acc: 0.8235 - val_loss: 0.4247 - val_acc: 0.8031\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3902 - acc: 0.8264 - val_loss: 0.4230 - val_acc: 0.8023\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3859 - acc: 0.8275 - val_loss: 0.4389 - val_acc: 0.7899\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3883 - acc: 0.8259 - val_loss: 0.4208 - val_acc: 0.8060\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3864 - acc: 0.8281 - val_loss: 0.4356 - val_acc: 0.7941\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3821 - acc: 0.8309 - val_loss: 0.4275 - val_acc: 0.7994\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3830 - acc: 0.8313 - val_loss: 0.4328 - val_acc: 0.7944\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3797 - acc: 0.8310 - val_loss: 0.4259 - val_acc: 0.8024\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3837 - acc: 0.8290 - val_loss: 0.4191 - val_acc: 0.8059\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3787 - acc: 0.8331 - val_loss: 0.4217 - val_acc: 0.8038\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3768 - acc: 0.8349 - val_loss: 0.4200 - val_acc: 0.8057\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3769 - acc: 0.8340 - val_loss: 0.4187 - val_acc: 0.8058\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3742 - acc: 0.8365 - val_loss: 0.4145 - val_acc: 0.8077\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3716 - acc: 0.8384 - val_loss: 0.4150 - val_acc: 0.8061\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3721 - acc: 0.8366 - val_loss: 0.4126 - val_acc: 0.8088\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3700 - acc: 0.8379 - val_loss: 0.4153 - val_acc: 0.8091\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3737 - acc: 0.8348 - val_loss: 0.4192 - val_acc: 0.8041\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3741 - acc: 0.8341 - val_loss: 0.4317 - val_acc: 0.7959\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3711 - acc: 0.8370 - val_loss: 0.4164 - val_acc: 0.8060\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3733 - acc: 0.8349 - val_loss: 0.4200 - val_acc: 0.8045\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3666 - acc: 0.8396 - val_loss: 0.4128 - val_acc: 0.8074\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3629 - acc: 0.8426 - val_loss: 0.4105 - val_acc: 0.8096\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3635 - acc: 0.8413 - val_loss: 0.4124 - val_acc: 0.8095\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3645 - acc: 0.8412 - val_loss: 0.4081 - val_acc: 0.8113\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3625 - acc: 0.8422 - val_loss: 0.4098 - val_acc: 0.8090\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3615 - acc: 0.8435 - val_loss: 0.4184 - val_acc: 0.8042\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3631 - acc: 0.8410 - val_loss: 0.4127 - val_acc: 0.8067\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3597 - acc: 0.8436 - val_loss: 0.4086 - val_acc: 0.8087\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3574 - acc: 0.8460 - val_loss: 0.4062 - val_acc: 0.8107\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 1s 87us/step - loss: 0.3569 - acc: 0.8449 - val_loss: 0.4087 - val_acc: 0.8091\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 1s 86us/step - loss: 0.3575 - acc: 0.8457 - val_loss: 0.4075 - val_acc: 0.8115\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3561 - acc: 0.8456 - val_loss: 0.4140 - val_acc: 0.8063\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3596 - acc: 0.8428 - val_loss: 0.4047 - val_acc: 0.8125\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3541 - acc: 0.8475 - val_loss: 0.4054 - val_acc: 0.8125\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3551 - acc: 0.8454 - val_loss: 0.4114 - val_acc: 0.8082\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3557 - acc: 0.8460 - val_loss: 0.4282 - val_acc: 0.7953\n",
      "Epoch 98/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3523 - acc: 0.8475 - val_loss: 0.4084 - val_acc: 0.8095\n",
      "Epoch 99/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3548 - acc: 0.8457 - val_loss: 0.4033 - val_acc: 0.8117\n",
      "Epoch 100/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3512 - acc: 0.8490 - val_loss: 0.4070 - val_acc: 0.8122\n",
      "Epoch 101/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3550 - acc: 0.8451 - val_loss: 0.4053 - val_acc: 0.8117\n",
      "Epoch 102/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3517 - acc: 0.8463 - val_loss: 0.4036 - val_acc: 0.8116\n",
      "Epoch 103/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3487 - acc: 0.8489 - val_loss: 0.4031 - val_acc: 0.8142\n",
      "Epoch 104/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3528 - acc: 0.8461 - val_loss: 0.4051 - val_acc: 0.8122\n",
      "Epoch 105/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3471 - acc: 0.8497 - val_loss: 0.4043 - val_acc: 0.8119\n",
      "Epoch 106/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3463 - acc: 0.8504 - val_loss: 0.4031 - val_acc: 0.8136\n",
      "Epoch 107/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3459 - acc: 0.8515 - val_loss: 0.3990 - val_acc: 0.8154\n",
      "Epoch 108/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3462 - acc: 0.8517 - val_loss: 0.4092 - val_acc: 0.8099\n",
      "Epoch 109/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3454 - acc: 0.8504 - val_loss: 0.4087 - val_acc: 0.8093\n",
      "Epoch 110/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.3443 - acc: 0.8518 - val_loss: 0.4132 - val_acc: 0.8062\n",
      "Epoch 111/200\n",
      " 2500/17142 [===>..........................] - ETA: 0s - loss: 0.3499 - acc: 0.8451"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3502 - acc: 0.8505 - val_loss: 0.3986 - val_acc: 0.8154\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3520 - acc: 0.8482 - val_loss: 0.3991 - val_acc: 0.8154\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3548 - acc: 0.8473 - val_loss: 0.4008 - val_acc: 0.8143\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3535 - acc: 0.8471 - val_loss: 0.4087 - val_acc: 0.8096\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3523 - acc: 0.8463 - val_loss: 0.4097 - val_acc: 0.8095\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3537 - acc: 0.8462 - val_loss: 0.4145 - val_acc: 0.8064\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3490 - acc: 0.8486 - val_loss: 0.3954 - val_acc: 0.8173\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3487 - acc: 0.8502 - val_loss: 0.3954 - val_acc: 0.8174\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3457 - acc: 0.8512 - val_loss: 0.3930 - val_acc: 0.8196\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3446 - acc: 0.8519 - val_loss: 0.3944 - val_acc: 0.8175\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3473 - acc: 0.8484 - val_loss: 0.3997 - val_acc: 0.8133\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.3502 - acc: 0.8470 - val_loss: 0.3970 - val_acc: 0.8165\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3445 - acc: 0.8512 - val_loss: 0.3958 - val_acc: 0.8168\n",
      "Epoch 98/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.3412 - acc: 0.8542 - val_loss: 0.3947 - val_acc: 0.8174\n",
      "Epoch 99/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3409 - acc: 0.8548 - val_loss: 0.3918 - val_acc: 0.8196\n",
      "Epoch 100/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3414 - acc: 0.8532 - val_loss: 0.3913 - val_acc: 0.8192\n",
      "Epoch 101/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.3413 - acc: 0.8526 - val_loss: 0.4032 - val_acc: 0.8132\n",
      "Epoch 102/200\n",
      "17142/17142 [==============================] - 1s 78us/step - loss: 0.3381 - acc: 0.8557 - val_loss: 0.4039 - val_acc: 0.8126\n",
      "Epoch 103/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3391 - acc: 0.8548 - val_loss: 0.3933 - val_acc: 0.8173\n",
      "Epoch 104/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3410 - acc: 0.8530 - val_loss: 0.3968 - val_acc: 0.8164\n",
      "Epoch 105/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3379 - acc: 0.8549 - val_loss: 0.3939 - val_acc: 0.8191\n",
      "Epoch 106/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3375 - acc: 0.8556 - val_loss: 0.3961 - val_acc: 0.8173\n",
      "Epoch 107/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3360 - acc: 0.8551 - val_loss: 0.3949 - val_acc: 0.8164\n",
      "Epoch 108/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3351 - acc: 0.8566 - val_loss: 0.3918 - val_acc: 0.8193\n",
      "Epoch 109/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3394 - acc: 0.8528 - val_loss: 0.4083 - val_acc: 0.8111\n",
      "Epoch 110/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3384 - acc: 0.8526 - val_loss: 0.3930 - val_acc: 0.8193\n",
      "auroc: 0.8818815270255486\n",
      "auprc: 0.8091946328849456\n",
      "auroc: 0.8802962384282399\n",
      "auprc: 0.8063127465993093\n",
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 8s 480us/step - loss: 0.7189 - acc: 0.6130 - val_loss: 0.6658 - val_acc: 0.6286\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.6511 - acc: 0.6481 - val_loss: 0.6510 - val_acc: 0.6529\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.6416 - acc: 0.6518 - val_loss: 0.6490 - val_acc: 0.6558\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.6303 - acc: 0.6602 - val_loss: 0.6359 - val_acc: 0.6541\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.6175 - acc: 0.6693 - val_loss: 0.6234 - val_acc: 0.6663\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.6069 - acc: 0.6767 - val_loss: 0.6134 - val_acc: 0.6818\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.5940 - acc: 0.6858 - val_loss: 0.6070 - val_acc: 0.6901\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.5850 - acc: 0.6936 - val_loss: 0.6065 - val_acc: 0.6879\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.5723 - acc: 0.7040 - val_loss: 0.5852 - val_acc: 0.6875\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.5594 - acc: 0.7143 - val_loss: 0.5775 - val_acc: 0.7080\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.5479 - acc: 0.7229 - val_loss: 0.5548 - val_acc: 0.7166\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.5367 - acc: 0.7305 - val_loss: 0.5597 - val_acc: 0.7155\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.5254 - acc: 0.7391 - val_loss: 0.5373 - val_acc: 0.7321\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.5195 - acc: 0.7425 - val_loss: 0.5548 - val_acc: 0.7158\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.5123 - acc: 0.7476 - val_loss: 0.5217 - val_acc: 0.7409\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.5021 - acc: 0.7533 - val_loss: 0.5171 - val_acc: 0.7423\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4989 - acc: 0.7570 - val_loss: 0.5367 - val_acc: 0.7340\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4928 - acc: 0.7582 - val_loss: 0.5015 - val_acc: 0.7515\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4820 - acc: 0.7670 - val_loss: 0.4977 - val_acc: 0.7563\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.4830 - acc: 0.7675 - val_loss: 0.5104 - val_acc: 0.7512\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4790 - acc: 0.7679 - val_loss: 0.4902 - val_acc: 0.7607\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4670 - acc: 0.7783 - val_loss: 0.4913 - val_acc: 0.7576\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4656 - acc: 0.7778 - val_loss: 0.4800 - val_acc: 0.7688\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4590 - acc: 0.7820 - val_loss: 0.4817 - val_acc: 0.7660\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4550 - acc: 0.7875 - val_loss: 0.4759 - val_acc: 0.7723\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4528 - acc: 0.7873 - val_loss: 0.4696 - val_acc: 0.7728\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4473 - acc: 0.7914 - val_loss: 0.4664 - val_acc: 0.7772\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4417 - acc: 0.7960 - val_loss: 0.4615 - val_acc: 0.7818\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4396 - acc: 0.7963 - val_loss: 0.4596 - val_acc: 0.7837\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4351 - acc: 0.8020 - val_loss: 0.4618 - val_acc: 0.7826\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4342 - acc: 0.8011 - val_loss: 0.4683 - val_acc: 0.7730\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4348 - acc: 0.7990 - val_loss: 0.4653 - val_acc: 0.7772\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.4326 - acc: 0.8018 - val_loss: 0.4524 - val_acc: 0.7860\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4246 - acc: 0.8066 - val_loss: 0.4631 - val_acc: 0.7773\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4255 - acc: 0.8060 - val_loss: 0.4493 - val_acc: 0.7879\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4199 - acc: 0.8095 - val_loss: 0.4452 - val_acc: 0.7928\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4159 - acc: 0.8141 - val_loss: 0.4457 - val_acc: 0.7894\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4136 - acc: 0.8157 - val_loss: 0.4730 - val_acc: 0.7740\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4162 - acc: 0.8117 - val_loss: 0.4379 - val_acc: 0.7974\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4102 - acc: 0.8162 - val_loss: 0.4458 - val_acc: 0.7905\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4154 - acc: 0.8129 - val_loss: 0.4525 - val_acc: 0.7826\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4077 - acc: 0.8174 - val_loss: 0.4382 - val_acc: 0.7955\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4066 - acc: 0.8173 - val_loss: 0.4353 - val_acc: 0.7982\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4034 - acc: 0.8212 - val_loss: 0.4388 - val_acc: 0.7934\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4131 - acc: 0.8100 - val_loss: 0.4322 - val_acc: 0.7975\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.4006 - acc: 0.8215 - val_loss: 0.4494 - val_acc: 0.7850\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3971 - acc: 0.8237 - val_loss: 0.4302 - val_acc: 0.8008\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3983 - acc: 0.8229 - val_loss: 0.4328 - val_acc: 0.7996\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4007 - acc: 0.8199 - val_loss: 0.4320 - val_acc: 0.7991\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3947 - acc: 0.8243 - val_loss: 0.4306 - val_acc: 0.7991\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3907 - acc: 0.8285 - val_loss: 0.4248 - val_acc: 0.8049\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3901 - acc: 0.8284 - val_loss: 0.4426 - val_acc: 0.7893\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3938 - acc: 0.8240 - val_loss: 0.4284 - val_acc: 0.8009\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3868 - acc: 0.8301 - val_loss: 0.4301 - val_acc: 0.7989\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3896 - acc: 0.8280 - val_loss: 0.4213 - val_acc: 0.8062\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3881 - acc: 0.8266 - val_loss: 0.4251 - val_acc: 0.8028\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3868 - acc: 0.8289 - val_loss: 0.4189 - val_acc: 0.8081\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3822 - acc: 0.8321 - val_loss: 0.4186 - val_acc: 0.8066\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3821 - acc: 0.8323 - val_loss: 0.4179 - val_acc: 0.8080\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3793 - acc: 0.8342 - val_loss: 0.4223 - val_acc: 0.8045\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3835 - acc: 0.8302 - val_loss: 0.4211 - val_acc: 0.8046\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3790 - acc: 0.8340 - val_loss: 0.4184 - val_acc: 0.8060\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3761 - acc: 0.8365 - val_loss: 0.4211 - val_acc: 0.8043\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3770 - acc: 0.8345 - val_loss: 0.4298 - val_acc: 0.7984\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3762 - acc: 0.8354 - val_loss: 0.4139 - val_acc: 0.8095\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3741 - acc: 0.8358 - val_loss: 0.4156 - val_acc: 0.8084\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3734 - acc: 0.8361 - val_loss: 0.4117 - val_acc: 0.8113\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3768 - acc: 0.8338 - val_loss: 0.4211 - val_acc: 0.8042\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3691 - acc: 0.8398 - val_loss: 0.4157 - val_acc: 0.8083\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3693 - acc: 0.8389 - val_loss: 0.4223 - val_acc: 0.8028\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3700 - acc: 0.8379 - val_loss: 0.4094 - val_acc: 0.8121\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3668 - acc: 0.8414 - val_loss: 0.4117 - val_acc: 0.8115\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3677 - acc: 0.8391 - val_loss: 0.4353 - val_acc: 0.7948\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3700 - acc: 0.8375 - val_loss: 0.4077 - val_acc: 0.8125\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3650 - acc: 0.8419 - val_loss: 0.4284 - val_acc: 0.8000\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3629 - acc: 0.8423 - val_loss: 0.4090 - val_acc: 0.8109\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3694 - acc: 0.8389 - val_loss: 0.4362 - val_acc: 0.7936\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3640 - acc: 0.8413 - val_loss: 0.4108 - val_acc: 0.8098\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3623 - acc: 0.8429 - val_loss: 0.4077 - val_acc: 0.8117\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3601 - acc: 0.8446 - val_loss: 0.4089 - val_acc: 0.8104\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3599 - acc: 0.8442 - val_loss: 0.4084 - val_acc: 0.8114\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3594 - acc: 0.8439 - val_loss: 0.4045 - val_acc: 0.8145\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3567 - acc: 0.8464 - val_loss: 0.4136 - val_acc: 0.8079\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3623 - acc: 0.8417 - val_loss: 0.4175 - val_acc: 0.8055\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3605 - acc: 0.8426 - val_loss: 0.4052 - val_acc: 0.8129\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3543 - acc: 0.8480 - val_loss: 0.4041 - val_acc: 0.8143\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3522 - acc: 0.8482 - val_loss: 0.4025 - val_acc: 0.8154\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3547 - acc: 0.8462 - val_loss: 0.4098 - val_acc: 0.8094\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3539 - acc: 0.8462 - val_loss: 0.4009 - val_acc: 0.8158\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3509 - acc: 0.8480 - val_loss: 0.4032 - val_acc: 0.8138\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3497 - acc: 0.8503 - val_loss: 0.4046 - val_acc: 0.8127\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3512 - acc: 0.8469 - val_loss: 0.4122 - val_acc: 0.8082\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3548 - acc: 0.8448 - val_loss: 0.4178 - val_acc: 0.8074\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3533 - acc: 0.8456 - val_loss: 0.4108 - val_acc: 0.8088\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3489 - acc: 0.8489 - val_loss: 0.3996 - val_acc: 0.8167\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3463 - acc: 0.8517 - val_loss: 0.4148 - val_acc: 0.8060\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3472 - acc: 0.8501 - val_loss: 0.4139 - val_acc: 0.8070\n",
      "Epoch 98/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3462 - acc: 0.8497 - val_loss: 0.3983 - val_acc: 0.8171\n",
      "Epoch 99/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3452 - acc: 0.8520 - val_loss: 0.4050 - val_acc: 0.8136\n",
      "Epoch 100/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3491 - acc: 0.8482 - val_loss: 0.4078 - val_acc: 0.8111\n",
      "Epoch 101/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3447 - acc: 0.8526 - val_loss: 0.4073 - val_acc: 0.8111\n",
      "Epoch 102/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3486 - acc: 0.8478 - val_loss: 0.4019 - val_acc: 0.8152\n",
      "Epoch 103/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3435 - acc: 0.8505 - val_loss: 0.3957 - val_acc: 0.8180\n",
      "Epoch 104/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3447 - acc: 0.8503 - val_loss: 0.3961 - val_acc: 0.8182\n",
      "Epoch 105/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3467 - acc: 0.8483 - val_loss: 0.4005 - val_acc: 0.8153\n",
      "Epoch 106/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3409 - acc: 0.8525 - val_loss: 0.4098 - val_acc: 0.8104\n",
      "Epoch 107/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3393 - acc: 0.8538 - val_loss: 0.3969 - val_acc: 0.8177\n",
      "Epoch 108/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3387 - acc: 0.8554 - val_loss: 0.4070 - val_acc: 0.8112\n",
      "Epoch 109/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3383 - acc: 0.8548 - val_loss: 0.4010 - val_acc: 0.8138\n",
      "Epoch 110/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3385 - acc: 0.8546 - val_loss: 0.3943 - val_acc: 0.8190\n",
      "Epoch 111/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3371 - acc: 0.8558 - val_loss: 0.4036 - val_acc: 0.8141\n",
      "Epoch 112/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3374 - acc: 0.8549 - val_loss: 0.4101 - val_acc: 0.8093\n",
      "Epoch 113/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3411 - acc: 0.8506 - val_loss: 0.3950 - val_acc: 0.8193\n",
      "Epoch 114/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3332 - acc: 0.8579 - val_loss: 0.3994 - val_acc: 0.8162\n",
      "Epoch 115/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3358 - acc: 0.8562 - val_loss: 0.3928 - val_acc: 0.8202\n",
      "Epoch 116/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3337 - acc: 0.8578 - val_loss: 0.3975 - val_acc: 0.8175\n",
      "Epoch 117/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3333 - acc: 0.8565 - val_loss: 0.3966 - val_acc: 0.8184\n",
      "Epoch 118/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3328 - acc: 0.8575 - val_loss: 0.3924 - val_acc: 0.8206\n",
      "Epoch 119/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3361 - acc: 0.8542 - val_loss: 0.4120 - val_acc: 0.8075\n",
      "Epoch 120/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3352 - acc: 0.8543 - val_loss: 0.3919 - val_acc: 0.8214\n",
      "Epoch 121/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3330 - acc: 0.8580 - val_loss: 0.3936 - val_acc: 0.8190\n",
      "Epoch 122/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3322 - acc: 0.8567 - val_loss: 0.3918 - val_acc: 0.8207\n",
      "Epoch 123/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3368 - acc: 0.8525 - val_loss: 0.3933 - val_acc: 0.8192\n",
      "Epoch 124/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3279 - acc: 0.8601 - val_loss: 0.3908 - val_acc: 0.8208\n",
      "Epoch 125/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3321 - acc: 0.8581 - val_loss: 0.3920 - val_acc: 0.8198\n",
      "Epoch 126/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3281 - acc: 0.8604 - val_loss: 0.3916 - val_acc: 0.8197\n",
      "Epoch 127/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3305 - acc: 0.8571 - val_loss: 0.3973 - val_acc: 0.8171\n",
      "Epoch 128/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3281 - acc: 0.8601 - val_loss: 0.4014 - val_acc: 0.8157\n",
      "Epoch 129/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3293 - acc: 0.8584 - val_loss: 0.3941 - val_acc: 0.8193\n",
      "Epoch 130/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3262 - acc: 0.8598 - val_loss: 0.3930 - val_acc: 0.8193\n",
      "Epoch 131/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3254 - acc: 0.8626 - val_loss: 0.3944 - val_acc: 0.8185\n",
      "Epoch 132/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.3284 - acc: 0.8599 - val_loss: 0.3968 - val_acc: 0.8155\n",
      "Epoch 133/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.3299 - acc: 0.8575 - val_loss: 0.4158 - val_acc: 0.8070\n",
      "Epoch 134/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3295 - acc: 0.8568 - val_loss: 0.4067 - val_acc: 0.8097\n",
      "auroc: 0.8813852906119779\n",
      "auprc: 0.8047043004644281\n",
      "auroc: 0.880426772246035\n",
      "auprc: 0.8031722821864795\n",
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 8s 490us/step - loss: 0.7482 - acc: 0.6003 - val_loss: 0.6724 - val_acc: 0.6418\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.6580 - acc: 0.6435 - val_loss: 0.6625 - val_acc: 0.6345\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.6472 - acc: 0.6493 - val_loss: 0.6565 - val_acc: 0.6434\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.6397 - acc: 0.6557 - val_loss: 0.6543 - val_acc: 0.6346\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 1s 80us/step - loss: 0.6331 - acc: 0.6595 - val_loss: 0.6424 - val_acc: 0.6528\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 1s 79us/step - loss: 0.6221 - acc: 0.6659 - val_loss: 0.6363 - val_acc: 0.6491\n",
      "Epoch 7/200\n",
      "14500/17142 [========================>.....] - ETA: 0s - loss: 0.6148 - acc: 0.6706"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3248 - acc: 0.8613 - val_loss: 0.4163 - val_acc: 0.8055\n",
      "Epoch 137/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3291 - acc: 0.8573 - val_loss: 0.4207 - val_acc: 0.8059\n",
      "Epoch 138/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3250 - acc: 0.8607 - val_loss: 0.3932 - val_acc: 0.8186\n",
      "Epoch 139/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3212 - acc: 0.8632 - val_loss: 0.3961 - val_acc: 0.8178\n",
      "auroc: 0.8832150869028016\n",
      "auprc: 0.8101884025260963\n",
      "auroc: 0.8819873501594798\n",
      "auprc: 0.8084458521069542\n",
      "Train on 17142 samples, validate on 12858 samples\n",
      "Epoch 1/200\n",
      "17142/17142 [==============================] - 9s 514us/step - loss: 0.6846 - acc: 0.6226 - val_loss: 0.6677 - val_acc: 0.6441\n",
      "Epoch 2/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.6529 - acc: 0.6480 - val_loss: 0.6559 - val_acc: 0.6464\n",
      "Epoch 3/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.6451 - acc: 0.6525 - val_loss: 0.6571 - val_acc: 0.6446\n",
      "Epoch 4/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.6332 - acc: 0.6584 - val_loss: 0.6475 - val_acc: 0.6503\n",
      "Epoch 5/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.6245 - acc: 0.6644 - val_loss: 0.6366 - val_acc: 0.6572\n",
      "Epoch 6/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.6216 - acc: 0.6666 - val_loss: 0.6356 - val_acc: 0.6479\n",
      "Epoch 7/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.6106 - acc: 0.6732 - val_loss: 0.6254 - val_acc: 0.6749\n",
      "Epoch 8/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.5984 - acc: 0.6828 - val_loss: 0.6149 - val_acc: 0.6604\n",
      "Epoch 9/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.5928 - acc: 0.6842 - val_loss: 0.6266 - val_acc: 0.6823\n",
      "Epoch 10/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.5789 - acc: 0.6985 - val_loss: 0.5930 - val_acc: 0.6952\n",
      "Epoch 11/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.5683 - acc: 0.7058 - val_loss: 0.5952 - val_acc: 0.6948\n",
      "Epoch 12/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.5572 - acc: 0.7141 - val_loss: 0.5748 - val_acc: 0.6999\n",
      "Epoch 13/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.5474 - acc: 0.7200 - val_loss: 0.5760 - val_acc: 0.7105\n",
      "Epoch 14/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.5380 - acc: 0.7290 - val_loss: 0.5673 - val_acc: 0.7104\n",
      "Epoch 15/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.5332 - acc: 0.7302 - val_loss: 0.5448 - val_acc: 0.7245\n",
      "Epoch 16/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.5195 - acc: 0.7404 - val_loss: 0.5379 - val_acc: 0.7274\n",
      "Epoch 17/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.5131 - acc: 0.7446 - val_loss: 0.5286 - val_acc: 0.7280\n",
      "Epoch 18/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.5078 - acc: 0.7467 - val_loss: 0.5268 - val_acc: 0.7301\n",
      "Epoch 19/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.5020 - acc: 0.7528 - val_loss: 0.5185 - val_acc: 0.7374\n",
      "Epoch 20/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4948 - acc: 0.7595 - val_loss: 0.5151 - val_acc: 0.7421\n",
      "Epoch 21/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4885 - acc: 0.7608 - val_loss: 0.5083 - val_acc: 0.7434\n",
      "Epoch 22/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4844 - acc: 0.7631 - val_loss: 0.4994 - val_acc: 0.7529\n",
      "Epoch 23/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4786 - acc: 0.7688 - val_loss: 0.5018 - val_acc: 0.7520\n",
      "Epoch 24/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4781 - acc: 0.7680 - val_loss: 0.4987 - val_acc: 0.7539\n",
      "Epoch 25/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4752 - acc: 0.7707 - val_loss: 0.5180 - val_acc: 0.7440\n",
      "Epoch 26/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4661 - acc: 0.7780 - val_loss: 0.4832 - val_acc: 0.7609\n",
      "Epoch 27/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4628 - acc: 0.7812 - val_loss: 0.4877 - val_acc: 0.7604\n",
      "Epoch 28/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4568 - acc: 0.7851 - val_loss: 0.4768 - val_acc: 0.7683\n",
      "Epoch 29/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4520 - acc: 0.7887 - val_loss: 0.4896 - val_acc: 0.7618\n",
      "Epoch 30/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4625 - acc: 0.7787 - val_loss: 0.4766 - val_acc: 0.7701\n",
      "Epoch 31/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4484 - acc: 0.7906 - val_loss: 0.4696 - val_acc: 0.7731\n",
      "Epoch 32/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4455 - acc: 0.7918 - val_loss: 0.4663 - val_acc: 0.7735\n",
      "Epoch 33/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4406 - acc: 0.7946 - val_loss: 0.4698 - val_acc: 0.7721\n",
      "Epoch 34/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4389 - acc: 0.7977 - val_loss: 0.4603 - val_acc: 0.7785\n",
      "Epoch 35/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4346 - acc: 0.7985 - val_loss: 0.4616 - val_acc: 0.7751\n",
      "Epoch 36/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4368 - acc: 0.7968 - val_loss: 0.4567 - val_acc: 0.7825\n",
      "Epoch 37/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4303 - acc: 0.8036 - val_loss: 0.4585 - val_acc: 0.7769\n",
      "Epoch 38/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.4312 - acc: 0.8009 - val_loss: 0.4609 - val_acc: 0.7772\n",
      "Epoch 39/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4266 - acc: 0.8036 - val_loss: 0.4591 - val_acc: 0.7805\n",
      "Epoch 40/200\n",
      "17142/17142 [==============================] - 1s 84us/step - loss: 0.4290 - acc: 0.8009 - val_loss: 0.4543 - val_acc: 0.7797\n",
      "Epoch 41/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4201 - acc: 0.8083 - val_loss: 0.4607 - val_acc: 0.7760\n",
      "Epoch 42/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4222 - acc: 0.8068 - val_loss: 0.4491 - val_acc: 0.7844\n",
      "Epoch 43/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4169 - acc: 0.8105 - val_loss: 0.4536 - val_acc: 0.7833\n",
      "Epoch 44/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4181 - acc: 0.8105 - val_loss: 0.4495 - val_acc: 0.7855\n",
      "Epoch 45/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4123 - acc: 0.8140 - val_loss: 0.4488 - val_acc: 0.7843\n",
      "Epoch 46/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4109 - acc: 0.8148 - val_loss: 0.4408 - val_acc: 0.7914\n",
      "Epoch 47/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.4081 - acc: 0.8175 - val_loss: 0.4427 - val_acc: 0.7871\n",
      "Epoch 48/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4123 - acc: 0.8137 - val_loss: 0.4672 - val_acc: 0.7752\n",
      "Epoch 49/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4104 - acc: 0.8131 - val_loss: 0.4515 - val_acc: 0.7817\n",
      "Epoch 50/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4106 - acc: 0.8120 - val_loss: 0.4387 - val_acc: 0.7899\n",
      "Epoch 51/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4047 - acc: 0.8183 - val_loss: 0.4353 - val_acc: 0.7936\n",
      "Epoch 52/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4032 - acc: 0.8193 - val_loss: 0.4348 - val_acc: 0.7966\n",
      "Epoch 53/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.4008 - acc: 0.8204 - val_loss: 0.4399 - val_acc: 0.7900\n",
      "Epoch 54/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3980 - acc: 0.8234 - val_loss: 0.4360 - val_acc: 0.7924\n",
      "Epoch 55/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3968 - acc: 0.8232 - val_loss: 0.4374 - val_acc: 0.7913\n",
      "Epoch 56/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3948 - acc: 0.8253 - val_loss: 0.4360 - val_acc: 0.7926\n",
      "Epoch 57/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3947 - acc: 0.8230 - val_loss: 0.4275 - val_acc: 0.7984\n",
      "Epoch 58/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3933 - acc: 0.8247 - val_loss: 0.4402 - val_acc: 0.7880\n",
      "Epoch 59/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3902 - acc: 0.8283 - val_loss: 0.4282 - val_acc: 0.8004\n",
      "Epoch 60/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3878 - acc: 0.8282 - val_loss: 0.4277 - val_acc: 0.7988\n",
      "Epoch 61/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3881 - acc: 0.8284 - val_loss: 0.4420 - val_acc: 0.7879\n",
      "Epoch 62/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3866 - acc: 0.8296 - val_loss: 0.4239 - val_acc: 0.8007\n",
      "Epoch 63/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3840 - acc: 0.8299 - val_loss: 0.4213 - val_acc: 0.8029\n",
      "Epoch 64/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3881 - acc: 0.8258 - val_loss: 0.4223 - val_acc: 0.8017\n",
      "Epoch 65/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3822 - acc: 0.8320 - val_loss: 0.4207 - val_acc: 0.8017\n",
      "Epoch 66/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3827 - acc: 0.8309 - val_loss: 0.4218 - val_acc: 0.8041\n",
      "Epoch 67/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3810 - acc: 0.8327 - val_loss: 0.4290 - val_acc: 0.7971\n",
      "Epoch 68/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3787 - acc: 0.8332 - val_loss: 0.4183 - val_acc: 0.8048\n",
      "Epoch 69/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3777 - acc: 0.8332 - val_loss: 0.4227 - val_acc: 0.8028\n",
      "Epoch 70/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3779 - acc: 0.8340 - val_loss: 0.4217 - val_acc: 0.8031\n",
      "Epoch 71/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3750 - acc: 0.8348 - val_loss: 0.4151 - val_acc: 0.8064\n",
      "Epoch 72/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3731 - acc: 0.8357 - val_loss: 0.4195 - val_acc: 0.8042\n",
      "Epoch 73/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3725 - acc: 0.8373 - val_loss: 0.4237 - val_acc: 0.8012\n",
      "Epoch 74/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3707 - acc: 0.8375 - val_loss: 0.4245 - val_acc: 0.8011\n",
      "Epoch 75/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3724 - acc: 0.8360 - val_loss: 0.4166 - val_acc: 0.8052\n",
      "Epoch 76/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3731 - acc: 0.8350 - val_loss: 0.4227 - val_acc: 0.8013\n",
      "Epoch 77/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3733 - acc: 0.8358 - val_loss: 0.4184 - val_acc: 0.8040\n",
      "Epoch 78/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3653 - acc: 0.8423 - val_loss: 0.4182 - val_acc: 0.8036\n",
      "Epoch 79/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3663 - acc: 0.8410 - val_loss: 0.4140 - val_acc: 0.8061\n",
      "Epoch 80/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3651 - acc: 0.8410 - val_loss: 0.4124 - val_acc: 0.8071\n",
      "Epoch 81/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3681 - acc: 0.8383 - val_loss: 0.4159 - val_acc: 0.8044\n",
      "Epoch 82/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3641 - acc: 0.8405 - val_loss: 0.4250 - val_acc: 0.7997\n",
      "Epoch 83/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3639 - acc: 0.8409 - val_loss: 0.4095 - val_acc: 0.8089\n",
      "Epoch 84/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3623 - acc: 0.8419 - val_loss: 0.4061 - val_acc: 0.8103\n",
      "Epoch 85/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3613 - acc: 0.8419 - val_loss: 0.4104 - val_acc: 0.8077\n",
      "Epoch 86/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3581 - acc: 0.8452 - val_loss: 0.4070 - val_acc: 0.8098\n",
      "Epoch 87/200\n",
      "17142/17142 [==============================] - 1s 87us/step - loss: 0.3580 - acc: 0.8450 - val_loss: 0.4273 - val_acc: 0.7983\n",
      "Epoch 88/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3594 - acc: 0.8438 - val_loss: 0.4037 - val_acc: 0.8112\n",
      "Epoch 89/200\n",
      "17142/17142 [==============================] - 1s 85us/step - loss: 0.3547 - acc: 0.8458 - val_loss: 0.4096 - val_acc: 0.8089\n",
      "Epoch 90/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3594 - acc: 0.8427 - val_loss: 0.4070 - val_acc: 0.8110\n",
      "Epoch 91/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3539 - acc: 0.8473 - val_loss: 0.4099 - val_acc: 0.8094\n",
      "Epoch 92/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3530 - acc: 0.8475 - val_loss: 0.4138 - val_acc: 0.8059\n",
      "Epoch 93/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3545 - acc: 0.8465 - val_loss: 0.4127 - val_acc: 0.8061\n",
      "Epoch 94/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3546 - acc: 0.8462 - val_loss: 0.4031 - val_acc: 0.8122\n",
      "Epoch 95/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3484 - acc: 0.8506 - val_loss: 0.4224 - val_acc: 0.8026\n",
      "Epoch 96/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3524 - acc: 0.8460 - val_loss: 0.4150 - val_acc: 0.8049\n",
      "Epoch 97/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3524 - acc: 0.8462 - val_loss: 0.3998 - val_acc: 0.8141\n",
      "Epoch 98/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3472 - acc: 0.8503 - val_loss: 0.4016 - val_acc: 0.8136\n",
      "Epoch 99/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3512 - acc: 0.8477 - val_loss: 0.4109 - val_acc: 0.8080\n",
      "Epoch 100/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3478 - acc: 0.8492 - val_loss: 0.4035 - val_acc: 0.8137\n",
      "Epoch 101/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3480 - acc: 0.8485 - val_loss: 0.4060 - val_acc: 0.8111\n",
      "Epoch 102/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3452 - acc: 0.8512 - val_loss: 0.4010 - val_acc: 0.8138\n",
      "Epoch 103/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3446 - acc: 0.8517 - val_loss: 0.4088 - val_acc: 0.8092\n",
      "Epoch 104/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3448 - acc: 0.8498 - val_loss: 0.3979 - val_acc: 0.8154\n",
      "Epoch 105/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3454 - acc: 0.8507 - val_loss: 0.4436 - val_acc: 0.7920\n",
      "Epoch 106/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3526 - acc: 0.8435 - val_loss: 0.4129 - val_acc: 0.8058\n",
      "Epoch 107/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3443 - acc: 0.8498 - val_loss: 0.4002 - val_acc: 0.8148\n",
      "Epoch 108/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3455 - acc: 0.8500 - val_loss: 0.4038 - val_acc: 0.8113\n",
      "Epoch 109/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3418 - acc: 0.8522 - val_loss: 0.4017 - val_acc: 0.8126\n",
      "Epoch 110/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3393 - acc: 0.8531 - val_loss: 0.3990 - val_acc: 0.8136\n",
      "Epoch 111/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3461 - acc: 0.8478 - val_loss: 0.4011 - val_acc: 0.8140\n",
      "Epoch 112/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3407 - acc: 0.8529 - val_loss: 0.3963 - val_acc: 0.8150\n",
      "Epoch 113/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3373 - acc: 0.8546 - val_loss: 0.3945 - val_acc: 0.8172\n",
      "Epoch 114/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3379 - acc: 0.8548 - val_loss: 0.3959 - val_acc: 0.8164\n",
      "Epoch 115/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3382 - acc: 0.8537 - val_loss: 0.3986 - val_acc: 0.8138\n",
      "Epoch 116/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3404 - acc: 0.8516 - val_loss: 0.4160 - val_acc: 0.8071\n",
      "Epoch 117/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3397 - acc: 0.8526 - val_loss: 0.3943 - val_acc: 0.8160\n",
      "Epoch 118/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3352 - acc: 0.8558 - val_loss: 0.4008 - val_acc: 0.8128\n",
      "Epoch 119/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3343 - acc: 0.8572 - val_loss: 0.3971 - val_acc: 0.8166\n",
      "Epoch 120/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3348 - acc: 0.8550 - val_loss: 0.4011 - val_acc: 0.8116\n",
      "Epoch 121/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3359 - acc: 0.8551 - val_loss: 0.3969 - val_acc: 0.8151\n",
      "Epoch 122/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3377 - acc: 0.8520 - val_loss: 0.4058 - val_acc: 0.8122\n",
      "Epoch 123/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3326 - acc: 0.8569 - val_loss: 0.3944 - val_acc: 0.8169\n",
      "Epoch 124/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3304 - acc: 0.8587 - val_loss: 0.3968 - val_acc: 0.8155\n",
      "Epoch 125/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3292 - acc: 0.8595 - val_loss: 0.3956 - val_acc: 0.8162\n",
      "Epoch 126/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3303 - acc: 0.8581 - val_loss: 0.3916 - val_acc: 0.8179\n",
      "Epoch 127/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3322 - acc: 0.8570 - val_loss: 0.3960 - val_acc: 0.8150\n",
      "Epoch 128/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3307 - acc: 0.8567 - val_loss: 0.4107 - val_acc: 0.8074\n",
      "Epoch 129/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3293 - acc: 0.8577 - val_loss: 0.3919 - val_acc: 0.8175\n",
      "Epoch 130/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3285 - acc: 0.8584 - val_loss: 0.3971 - val_acc: 0.8162\n",
      "Epoch 131/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3272 - acc: 0.8585 - val_loss: 0.3912 - val_acc: 0.8175\n",
      "Epoch 132/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3288 - acc: 0.8584 - val_loss: 0.4199 - val_acc: 0.8006\n",
      "Epoch 133/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3281 - acc: 0.8588 - val_loss: 0.4015 - val_acc: 0.8137\n",
      "Epoch 134/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3249 - acc: 0.8613 - val_loss: 0.3971 - val_acc: 0.8152\n",
      "Epoch 135/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3269 - acc: 0.8594 - val_loss: 0.4117 - val_acc: 0.8091\n",
      "Epoch 136/200\n",
      "17142/17142 [==============================] - 1s 83us/step - loss: 0.3338 - acc: 0.8534 - val_loss: 0.3953 - val_acc: 0.8164\n",
      "Epoch 137/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3275 - acc: 0.8590 - val_loss: 0.4026 - val_acc: 0.8130\n",
      "Epoch 138/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3283 - acc: 0.8570 - val_loss: 0.3942 - val_acc: 0.8169\n",
      "Epoch 139/200\n",
      "17142/17142 [==============================] - 1s 81us/step - loss: 0.3252 - acc: 0.8597 - val_loss: 0.3991 - val_acc: 0.8150\n",
      "Epoch 140/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3233 - acc: 0.8614 - val_loss: 0.3963 - val_acc: 0.8154\n",
      "Epoch 141/200\n",
      "17142/17142 [==============================] - 1s 82us/step - loss: 0.3232 - acc: 0.8613 - val_loss: 0.3937 - val_acc: 0.8162\n",
      "auroc: 0.879913393917247\n",
      "auprc: 0.8040968406053789\n",
      "auroc: 0.8791981358885513\n",
      "auprc: 0.8031771141395989\n"
     ]
    }
   ],
   "source": [
    "for curr_seed in all_seeds: \n",
    "    model, early_stopping_callback, auroc_callback = train_model(model_wrapper = SIAMESE_WRAPPER, \n",
    "                                                                 aug = None, \n",
    "                                                                 curr_seed = curr_seed, \n",
    "                                                                 batch_size = 500,\n",
    "                                                                 x = x_train_1000, \n",
    "                                                                 y = y_train_1000, \n",
    "                                                                 val_data = (x_val_1000, y_val_1000))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"Siamese_1000_auROC\", curr_seed = curr_seed,\n",
    "             callback = auroc_callback, model = model, val_data = (x_val_1000, y_val_1000))\n",
    "    save_all(filepath = \"general_results\", model_arch = \"Siamese_1000_loss\", curr_seed = curr_seed,\n",
    "             callback = early_stopping_callback, model = model, val_data = (x_val_1000, y_val_1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf1.15.2]",
   "language": "python",
   "name": "conda-env-tf1.15.2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
